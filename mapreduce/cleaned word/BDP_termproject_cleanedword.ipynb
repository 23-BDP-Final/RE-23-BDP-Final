{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "t_GIC0GTALOQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def WordStop(data):\n",
        "    # cleaned_content = re.sub(r'[^\\.\\?\\!\\w\\d\\s]', '', data)  # 문장단위로 끊기\n",
        "    cleaned_content = data.lower()\n",
        "    # 단어로 쪼개기 토큰화\n",
        "    word_tokens = nltk.word_tokenize(cleaned_content)\n",
        "    # 품사\n",
        "    tokens_pos = nltk.pos_tag(word_tokens)\n",
        "    # 명사만 추출\n",
        "    NN_words = []\n",
        "    for word, pos in tokens_pos:\n",
        "        if 'NN' in pos:\n",
        "            NN_words.append(word)\n",
        "    # 원형\n",
        "    wlem = nltk.WordNetLemmatizer()\n",
        "    lemmatized_words = []\n",
        "    for word in NN_words:\n",
        "        new_word = wlem.lemmatize(word)\n",
        "        lemmatized_words.append(new_word)\n",
        "    # 불용어\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    # nltk에서 제공하는 불용어사전 이용\n",
        "    unique_NN_words = set(lemmatized_words)\n",
        "    final_NN_words = lemmatized_words\n",
        "    # 불용어 제거\n",
        "    for word in unique_NN_words:\n",
        "        if word in stopwords_list:\n",
        "            while word in final_NN_words: final_NN_words.remove(word)\n",
        "    # 직접 만든 불용어 사전\n",
        "    customized_stopwords = ['be', 'today', 'yesterday', \"it’s\", \"don’t\"]\n",
        "    unique_NN_words1 = set(final_NN_words)\n",
        "    for word in unique_NN_words1:\n",
        "        if word in customized_stopwords:\n",
        "            while word in final_NN_words: final_NN_words.remove(word)\n",
        "    #특정 문자가 포함된 단어 없애기\n",
        "    search = \"\\\\u\"\n",
        "    for word in final_NN_words:\n",
        "      if search in word:\n",
        "          final_NN_words.remove(word)\n",
        "\n",
        "    return final_NN_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/23-BDP-Final/RE-23-BDP-Final/main/mapreduce/wordcount_description_en', on_bad_lines='skip', sep='\\t', header=None, names=['name','num'])\n",
        "data['cleaned_words'] = data['name'].apply(lambda x: ' '.join(WordStop(x)))\n",
        "result = data[data['cleaned_words'].str.strip() != ''][['cleaned_words', 'num']]\n",
        "result.to_csv('wordcount_clean_description_en.csv')\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/23-BDP-Final/RE-23-BDP-Final/main/mapreduce/wordcount_requirement_en', on_bad_lines='skip', sep='\\t', header=None, names=['name','num'])\n",
        "data['cleaned_words'] = data['name'].apply(lambda x: ' '.join(WordStop(x)))\n",
        "result = data[data['cleaned_words'].str.strip() != ''][['cleaned_words', 'num']]\n",
        "result.to_csv('wordcount_clean_requirement_en.csv')\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/23-BDP-Final/RE-23-BDP-Final/main/mapreduce/wordcount_preferred_en', on_bad_lines='skip', sep='\\t', header=None, names=['name','num'])\n",
        "data['cleaned_words'] = data['name'].apply(lambda x: ' '.join(WordStop(x)))\n",
        "result = data[data['cleaned_words'].str.strip() != ''][['cleaned_words', 'num']]\n",
        "result.to_csv('wordcount_clean_preferred_en.csv')"
      ],
      "metadata": {
        "id": "EZ4oczjEAQ_8"
      },
      "execution_count": 102,
      "outputs": []
    }
  ]
}