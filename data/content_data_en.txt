description• Develop backend/data engineers for ‘Webtoon Metrics’ and new services. • Develop modeling of new services using time series analysis.• Develop a back-end system that can handle large amounts of traffic and reliably transmit large amounts of data to Kyobo Bookstore’s internal and external systems • Implement and operate back-end microservice design based on the AWS cloud • Review and introduce new technologies for continuous improvement of back-end architecture - Development of API for Kyobo Bookstore data app based on cloud environment - Development of data supply API linked to data analysis pipeline system (data engineer collaboration) - Design and continuous improvement of back-end architecture based on cloud environment - Back-end server operation and monitoring - New technology research and reflection for back-end system operation and improvement• Backend service development • Large data processing using big data platformㆍWeb application backend development• Server backend development• Development and operation of platform service API • Design and improvement of system architecture • Maintenance and management of operating servers Develop additional functions• Web backend REST API development and optimization • DB design and operation • File server and data management• Crawling development (Python) • RESTFUL API design and development (Elastic Search Node.js, etc.) • Data pipeline (ETL) development and operation leading to data collection/processing/visualization[Required skills] • Big data engineering training course instructor - Python programming - Java programming (full stack implementation) - Linux - Database (SQL) - Machine learning / deep learning / image deep learning - Engineering (Hadoop Spark MapReduce, etc.) - MLOps ( Docker Kubernetes Kubeflow, etc.) • Artificial intelligence (natural language processing) training course - Python programming (full stack implementation) - Database (SQL) - Machine learning / deep learning / image deep learning / natural language deep learning - OpenCV • MSA-based backend training course - Java programming (full stack implementation) - Database (SQL) - Linux - Git - MSA - DevOps / Docker / Kubernetes[Required skills] • Big data engineering training course instructor - Python programming - Java programming (full stack implementation) - Linux - Database (SQL) - Machine learning / deep learning / image deep learning - Engineering (Hadoop Spark MapReduce, etc.) - MLOps ( Docker Kubernetes Kubeflow, etc.) • Artificial intelligence (natural language processing) training course - Python programming (full stack implementation) - Database (SQL) - Machine learning / deep learning / image deep learning / natural language deep learning - OpenCV • MSA-based backend training course - Java programming (full stack implementation) - Database (SQL) - Linux - Git - MSA - DevOps / Docker / Kubernetes1. Main tasks • Development and operation of Partridge Systems Back-End • Design and implementation of Partridge Systems Back-End service architecture 2. Recruitment procedures and guidance Step1_ Document screening (response within 3 days after viewing) Step2_ Technical interview screening (face-to-face) Step3_ Final acceptance and treatment consultation ㆍ If the information provided during the recruitment process is different from the facts, employment may be cancelled. ㆍ If you have any other questions, please contact the hiring manager.What does the Business Operations team do? Aloha Factory's Business Operations team manages the company's business strategies and issues in all business areas, from mid- to long-term strategies to optimization of organizational operations. We are developing Flamingo, a data analysis and management tool that allows anyone to easily check game data and create games. Flamingo started with the perspective of ‘creating games together.’ Flamingo service starts from the ‘game production’ stage, which is located at the very first stage of the game service value chain (game production - marketing - monetization - operation stage). We focus on making the game solid step by step through continuous discussion from the early stages of game production and data analysis and sharing through Flamingo. In addition to the data analysis function, game creators can create games in a better environment. We are preparing various operational functions required during the actual development process.Game abnormality detection - Develop and operate a model to detect abnormal users (nuclear macro workshops, etc.) in the game - Analyze abnormal user behavior patterns through game data and determine whether users behave abnormally - In the blockchain environment of P2E games Development of a detection model for abusing (coin trading/mining, etc.) users and analysis of abusing patterns Detection of advertising fraud - Development of a detection model for users of advertising fraud occurring in marketing media - Research on new advertising fraud techniques and improvement of detection models Social media analysis - Provides insight into reputation and community issues for games in service through analysis of user trends on social media User data analysis - Analyzes user behavior within Netmarble games and data analysis/prediction work to maximize game PLC/sales - Enhancement of game services User segmentation and indicator development for marketing data analysis - Data analysis work to optimize Netmarble marketing performance - Advertising value analysis using ML model - Development of digital marketing simulation indicators - Development of visualization dashboard based on Tableau Looker Studio- Deriving business insight through planning/discovering company-wide data analysis tasks - Building and upgrading models using machine learning/deep learning, etc. - Improving existing work processes and developing new indicatorsResponsibilities Design and load various marts and create and provide products such as dashboards and alarms. Discover business problems based on accumulated data and propose solutions through in-depth data analysis. In addition to commerce represented by member product orders, order entry/processing. Analyze overall logistics data, including shipment and delivery• Data modeling based on company-owned DB and defining and solving problems through it • Development of product recommendation/personalization model and product/person ranking model • Development of text mining and models such as product metadata and user review data - Sales of customer products held by Kyobo Bookstore, etc. Develop new recommendation/personalization models based on data - Develop models such as classification topic extraction and sentiment analysis using structured/unstructured text - Research analysis models related to book-adjacent products ▶ Document screening ▶ Interview screening ▶ Health checkup ▶ Final acceptance- Development of visualization (Tableau) dashboard using internal/external data - Data definition collection/processing modeling for development of visualization screen, etc. - Visualization dashboard consulting for business usersBased on expertise in data analysis and communication, 1. Data-based problem solving - Communicate with field managers in each area to derive tasks and present data-based solutions to decision makers. 2. Establishment of data governance - Based on close communication with field managers in each area, we derive data strategies and proceed with data planning, dashboard development, and data product development to create sustainable insights. 3. Capitalize data strategy - Plan new data collection to create new value and conduct data analysis to establish strategy. 4. Quantification, analysis, and monitoring of key indicators - Analyze key indicators to achieve company goals, monitor the status, and create a dashboard to easily check these various indicators. Example of work) Case 1) Derive the needs of the person in charge who wants to view the flow of logistics organized in a separate system through one screen, analyze the requirements, collect data, analyze/process it, form a data mart, and create a dashboard. Case 2) In order to establish a data governance system that can be directly utilized by field managers, principles and policy processes were established and products were selected together with team members to build a data warehouse. and developing a data mart and dashboard to suit the needs of the people in charge, thus creating a data-based work culture.) • In the Data Intelligence Team, you can work with project ownership regardless of seniority. • There is an opportunity to directly analyze and create value from Kyobo Bookstore’s large and diverse data. • You can experience a variety of cloud-based technology stacks. • You can initiate and carry out interesting projects directly by collaborating with people in charge of various departments.- Perform data analysis tasks necessary for establishing and managing delivery business plans - Derive measures to advance services and improve performance through data analysis - Extract data and quantify quantitative data - Define new indicators, manage and visualize indicators - Analyze indicators for process improvement Establishment of action plan and communication with relevant departments• Data pipeline (ETL) development and operation • Data mart design modeling operation • Support for data analysis environment for organization members▷ If you join, you will do these things together. • Provide customers with high-level cyber threat response insight through cyber crime-related tracking and data analysis. • Use threat intelligence to predict the technical characteristics and behavior of unknown threat actors and analyze threat information that may result from attacks. • Data analysis uses technologies such as machine learning and natural language processing (NLP) to derive more meaningful intelligence from data collected from various sources. ▷ We are using these technologies and tools. • Python R ELK • MongoDB Git Docker Kubernetes • Collaboration tools) Notion Slack Jira Google Workspace- Establish mid- to long-term data strategy and business plan - Establish plan to internalize data utilization capabilities - Discover/implement new big data business【PayTaLab Growth Hacking Team】 The Growth Hacking Team is conducting CRM (Customer Relationship Management) to help users and business owners who use Pass Order experience the value of Pass Order and use it better. We drive product growth through experimentation and constant verification regardless of means and methods. The goal is to create an organization where data flows so that decisions can be made based on data rather than intuition. The Growth Hacking team is comprised of experts who always think from the user's perspective for business growth and know PassOrder better than anyone else. 【It would be great if you could share this information with us!】 • Please submit a portfolio or career description that includes a detailed explanation and specific materials about your experience extracting and analyzing data. • If you have experience presenting an action or carrying out a project through data analysis, please tell us the process of problem definition - hypothesis setting - experimental design - results. • I’m curious about what thoughts and values ​​you have as a data engineer. • Please submit your answers to the preliminary questions below along with your resume. - Why do you apply to Payta Lab? - When you join Payta Lab, what method and plan do you want to proceed with your work? 【If you come, you will do this kind of work!】 • We collect and manage data so that Passorder can make decisions based on data. • We provide a data-driven environment to help members make data-based decisions and derive insights. • Build a data pipeline by modeling and visualizing data analysis results. • Establish a data utilization strategy and define KPIs and key service-related indicators. • Predict the expected effects of solutions derived through data analysis and measure post-performance through testing.• Responsible for identifying problems and planning and analyzing them with POs/engineers/designers to improve app services. • Improve mobile app service indicators and provide data-based insights for problem solving. • Design experiments to improve services and analyze the results.• Preparing mid- to long-term business planning documents • Preparing weekly and monthly performance reports • Conducting service index analysis • Participating in other business-related projects• Extract and process data required for business division business tasks • Collect, purify and analyze farm information, user behavior data and external data to derive business insights • Present and implement service improvement plans based on data analysis results • Data visualization tool (BI tool Google Sheet etc.) to clearly communicate and report analysis results to stakeholders • Analyze farmers’ behavior and the effectiveness of services • Manage the progress of data analysis projects and report results• Establish and analyze data policies created/managed in the Wavve service.• ETL • Data analysis • Statistical system maintenance • Data service API development and maintenanceMain tasks • Commercial real estate beneficiary certificate information service planning • Global REITs service improvement and advancement planning • Data sourcing and collection plan design • Database table structure design • Creation of documents required for web/app development (function definition, screen design, data structure diagram, etc., recruitment process) • It is conducted in the following order: [Document Screening > Working-level Coffee-Chat > ​​Management Interview] • Coffee-Chat and management interview are held at SPI's Yeouido office. Working Conditions • Work Type: Full-time employee (Decision on conversion after 3-month probationary evaluation) ) • Workplace: SPI Yeouido Office (6th floor, 83 Uisadong-daero, Yeongdeungpo-gu, Seoul / Seoul Fintech Lab, Yeouido-dong)• Problem definition, hypothesis setting, experimental design • Analysis modeling and indicator analysis EDA • Insight derivation and reporting for decision making • Dashboard planning/production using visualization tools (Tableau, etc.)We are looking for a Business Analyst who can create impactful changes based on high-level quantitative and logical thinking capabilities. Specific tasks are as follows: • Monitor Channel Talk business status and analyze issues. We find the essence of the problem, analyze it, and solve it in the most efficient way. We also compare with global SaaS benchmarks to identify areas for improvement. • Forecast key business indicators and reflect them in establishing goals. • Plan the optimal data visualization method and provide a dashboard that will become the team's weapon. We mainly use Tableau Redash Google Spreadsheets and collaborate with the development team to integrate them into a more accessible internal admin tool. • Data required for analysis is loaded in the most efficient manner. We plan a Big Query-based analytical data mart and implement it in collaboration with the data engineering team. • If there is key data that is not being collected, collect the data through collaboration with the development team and introduction of external tools. We introduce user behavior analysis tools such as Amplitude to plan events to be tracked and create dashboards. • Collaborate organically with multiple teams that require optimal decision-making based on data. Among them, there is a lot of collaboration with the sales team, marketing team, product team, and finance team.• Analyze service/business status and derive insights • Discover new and important indicators • Create and operate a data dashboard • Contribute to defining problems for service/business improvement or experimentation based on the analyzed data and the resulting insights • Hypothesis ·Contribute to the design of experiment data and experimental methods • Interpret indicators and obtain insights in the post-experiment analysis stage, report and shareLooking for a data expert to create a new service that will revolutionize the music industry with data • Product strategy design and project leading for K-pop industry workers to provide new data experiences • Analysis of artist fandom-related data collected on K-pop radar and Blip • Data Analysis of K-pop industry and artist growth factors through • Planning of K-pop Radar 2.0, a premium version of K-pop Radar 1.0- Define important data analysis tasks that must be solved through close communication with multiple teams - Data consideration/design to exploratively analyze numerous product-related data - Define various hypotheses based on gaining insights through broad and deep data analysis - Test the hypotheses Design experiments and analyze results - set important key indicators and create visualizations to track those indicators• We study ML models that predict user behavior using large log data from games served by Nexon. • We analyze the process starting from marketing activities to in-game services and measure advertising performance based on the loaded data. • From a marketing perspective, cluster users with various characteristics and find similar customers that fit marketing purposes. • Optimize costs by measuring advertising effectiveness and create better campaign plans through forecasting.• Development of recommendation algorithm and recommendation engine based on big data • Development of large data processing statistics and user analysis system • Development of Python-based frontend and backend[I would like to work on these tasks together] • Data modeling and processing to efficiently use large amounts of data required for game and business analysis • Design and implementation of the overall ETL process from data collection/processing/visualization • Various data from game services Extraction and provision • Report development and operation [Ideal talent for the analysis support team] • A person who can proactively find problems and understand the essence of the problem • A person who can communicate smoothly with colleagues in other job categories • A person who is humble and has a learning attitude with colleagues Someone who can enjoy workingMainly responsible for these tasks: • Developing and upgrading new client and server functions for clinical research solutions • Identifying and improving vulnerabilities in existing systems • Collaborating with planners and designers to refine product plans• Identify data suitable for analysis purposes, collect and process data (perform feature engineering) • Develop and verify analysis models using statistics and machine learning techniques for large amounts of data - Process/process data using statistical tools and programming languages - Development and verification of optimized analysis models for each project using statistics and machine learning techniques • Operation and monitoring of data analysis models • Improvements made and continuous updates (advancement) through analysis monitoring • Business areas based on data analysis results Derive applicable solutions• Develop models to apply to data products using ML technologies such as Computer Vision / NLP • Data processing and serving/operation engineering required for ML models• Define important company-wide questions from a data perspective and show the direction of decision-making • Allow everyone to understand how decisions and product improvements affect users • Different teams across the company can look in the same direction: users Contribute with data so that • We create indicator analysis experiments in a stackable form so that anyone in the company can view them and use them for their own work or decision-making. • We systematically remove bias that occurs in the decision-making process through methods such as A/B testing. • We allow anyone to see the data they need to make decisions.• Create hypotheses and derive insights from data to improve business models across the company and division. • Collaborate closely with the Product Owner to provide data and insights needed to validate problem definition hypotheses throughout the product improvement process. • Lead the development of a company-wide voluntary/efficient data utilization culture.• All log data generated in and out of the game is used and analyzed. ＞ Game goods trading, quest items, action logs, images, text gacha, etc. • Using Spark, large amounts of log data are extracted, processed and analyzed. • Develop time series models and indicators > Detect and interpret unusual signals occurring in more than 10 million time series every day. ＞ Our goal is for all abnormalities that occur within the game to be detected in a time series somewhere and interpreted within the data we have collected. • Research and develop abuse and workplace detection models. ＞ Artificial neural network, machine learning, statistical inference, computer algorithm, etc. We can use all possible methods and think and research together. > Feature engineering process to develop meaningful variables from massive logs is also included. • Detects probability abnormalities in game gacha. ＞ We monitor the actual probabilities of the game’s gacha items and study probability detection models.• Indicator management through data extraction and preprocessing using SQL • Production and operation of data visualization and dashboards (Tableau, etc.) • Analysis requirements planning and data collection management • Data-based hypothesis establishment and verification • Various indicator definitions and tracking automation [Typical Procedure] Document screening -> 1st interview -> Treatment consultation and final acceptance ※ There may be a coding test for each position applied. (Separate guidance will be provided when arranging the interview schedule) ※ Reference checks can be performed with consent during the selection process.• Design indicators and configure dashboards/reports for the growth of Wolbu.com and data-driven decision-making • Identify issues through monitoring and analysis of key indicators • Derive insights and service directions through data analysis resultsSpoqa's data analysts play an important role in using data to create service growth. Communicates with the PM designer and developer who directly create the service and plays a leading role in data-driven decision-making. We are looking for someone to grow our kitchen board service together. • We use data to solve various service problems. • Define, track, visualize, and share key service indicators with team members. • Design and manage data logs to analyze service usage patterns. • Through user analysis, we derive and share insights that can directly affect the service. • Define and manage company-wide and department-specific KPIs.Data analysis involves managing and analyzing large amounts of data to design models that provide information or make predictions to help make decisions. Data analysis derives insights through data analysis, a job that was created in the 2010s through the convergence of jobs such as database managers, statistics, and business analysts that previously existed in the IT and management fields. Through the insights derived in this way, it creates dashboards that help each business or organization and plays a role in helping to understand the data. • Identify the service status and derive insights after analyzing data for the service • Create a report that clearly explains the data visualization analysis through the dashboard in business language • Propose action items and solutions to grow business KPI based on the derived insights • Define key indicators appropriate for the current situation, plan and verify hypotheses for them, and create and operate ML models for advertising and commerce user targeting.• Building and operating an Apache Airflow environment running on EKS • Supporting Apache Airflow DAG development and operation • Developing a data pipeline that collects/processes/loads advertising data • Enhancing Data Warehouse Data Lake• Extract and analyze internal service data • Derive improvement plans and write reportsPulsong's data scientist is responsible for researching and developing an algorithm that detects signs of health problems by analyzing the cat's daily life data, and analyzing various data related to the cat as well as the cat's health to build our service to be integrated into the customers. [As a data scientist in Pearlsong's project team, you will perform the following tasks.] - Analysis of cat biometric data obtained from Pearlsong's IoT devices - Processing of data collected through smart devices and deriving insights helpful in service planning - Cats' faces Development of photo-based recognition model - Establishment of statistical model through communication with in-house veterinarian - Collection/analysis of based data• Build and operate a data pipeline for large-scale data collection and processing data analysis in a distributed environment • Build infrastructure for distribution of AI models • Build and operate API services for services • Collaboration and process automation within and outside the data team- Analyze behavioral data of APR's mobile users generated domestically and overseas and derive insights - Derive improvements to services based on insights derived through data analysis - Organize and report data generated from APR's various brands and visualization - Modeling to be used in AI services using cloud technology ※ This is not a task of verifying data consistency, but of deriving insights and improvements through data analysis.- Data platform architecture design - Data platform construction and operation• Consider and design a data structure to explorably analyze numerous product-related data • Gain insights through broad and deep data analysis and create various hypotheses based on them • Design an experiment to verify the hypothesis and report the experiment results Analyze • Set key indicators important for local store growth and support visualization to track those indicators.• Game recommendation modeling using gamer logs and game metadata • Churn prediction modeling using user behavior data • Feature engineering to extract data from log data- Create intuitive dashboards and reports needed for data-based decision making. - We collect and analyze game data to derive insights that can refine the game journey and enhance the product. - Identify/collect/analyze key data throughout the life-time of a fast-paced game and compare it to the actual situation to derive insights that can enhance the product. - Set the indicators and hypotheses necessary to achieve the goal and conduct A/B tests to verify them. - Through collaboration with various fields such as marketing planning business, we define problem points toward KPI and provide solutions. - We provide users with the best experience by enhancing the game journey through continuous iteration and more accurate and agile attempts.• Perform data collection refinement visualization modeling optimization tasks for SaaS service development. • Perform algorithms and modeling tasks for service application. • We carry out a visualization dashboard development project using BI tool Tableau. • Conduct service data analysis and derive insights • Analyze data and derive insights on company-wide services and various issues• Provides business insights and direction through in-depth statistical analysis of a wide range of medical data. • Design and develop modern data analysis and visualization tools for various stakeholders within the organization. • Work closely with ML engineers and medical experts to develop and validate AI predictive models and algorithms. • Work closely with data engineers to ensure the quality and usability of data for continuous improvement of AI solutions.• Define and develop company-wide key indicators that can track business growth. • Build and manage a dashboard to monitor company-wide key indicators. • Define, analyze, and solve cross-functional business problems with colleagues in various functions such as product design, engineering, marketing, and sales.• Provides higher value to learners based on data from 1.2 million members and 47 million cumulative learnings. • Develop a personalized recommendation model based on user behavior data and apply it to actual services. • Design and run A/B tests and analyze results to uncover meaningful information to improve learner experience. • Extract information from data and derive insights through data management, analysis, visualization and modeling. • Improve data quality by identifying and sharing data problems. • Contributes to the performance of the infrastructure lab by supporting data-based decision making. [Technology Stack] Required skills: Clear understanding and excellent application skills are required. SQL Python TensorFlow Keras Preferred skills: Practical understanding and relevant experience are required. GCP AWS Spark Scala Java Ubuntu CentOS LinuxPulsong's data scientist is responsible for researching and developing an algorithm that detects signs of health problems by analyzing the cat's daily life data, and analyzing various data related to the cat as well as the cat's health to build our service to be integrated into the customers. [As a data scientist in Pearlsong's project team, you will perform the following tasks.] - Analysis of cat biometric data obtained from Pearlsong's IoT devices - Processing of data collected through smart devices and deriving insights helpful in service planning - Cats' faces Development of photo-based recognition model - Establishment of statistical model through communication with in-house veterinarian - Collection/analysis of based data- Conducting data-based research to predict trends in the e-commerce market - Analysis of trends in domestic and international e-commerce markets - Consumer analysis of various product categories in the e-commerce industry - Case studies on mega-hit products - Large-scale e-commerce Derive meaningful insights by analyzing and utilizing MUS dataWe build a Data Pipeline Data Lake Data Warehouse Data Visualization platform to provide quick and easy data analysis and service environment. • We develop and operate a visualization platform by purifying and profiling data to improve the quality of Finda’s services and marketing activities. We perform data governance activities to ensure the quality and usability of Finda data. • Plan a process to systematically manage data. • Contributes to increasing consistency and reliability through data standardization. • Manage data marts to improve data efficiency.• Process log data ETL data. • Design/build/manage/optimize performance of data pipeline system. • Automate infrastructure construction and data processing using Kafka Composer, etc. • Provides a base environment for configuring Data Mart.[Introducing the AI ​​Recommendation Engine Team] - The Olive Young AI Recommendation Engine Team aims to realize the hyper-personalized recommendation service of Olive Young, a leader in the domestic H&B market. · Create a recommendation algorithm based on big data · Recommend data through the recommendation system. It learns and performs recommendation modeling. · We plan an integrated on-off recommendation service from an omni-channel perspective for hyper-personalized recommendations. [Responsibilities] Big data-based customer behavior patterns and product purchase factors analysis and insight derivation Data analysis design modeling Data mining Big data-based product recommendation model User segment extraction Score model Development of prediction model Efficient purification of raw data and data structure definition Data pipeline Development• Enhancement of corporate credit analysis model • Analysis of data utilizing various financial and economic variables • Structure of analysis data through data pre-processing and processingIntroducing the OpenUp PT (Product Team). Finda’s Open Up PT is working to solve the financial concerns of small business owners and the self-employed! • OpenUp PT is a big data-based commercial analysis startup that has been running since 2018 and will be working with Finda starting in 2022. • We provide commercial analysis services for small business owners and self-employed people based on approximately 9,000 region-based self-employment sales data and approximately 700,000 pieces of data that are newly generated every month. • In addition, we are working to help business owners have a better financial life by launching a ‘restaurant business startup calculator’ that allows you to easily measure the costs needed to start a business! Main tasks • Manage various data used throughout the service to improve the quality of the open-up service. • Responsible for managing pipelines and processing data to efficiently utilize unrefined commercial and sales data. • Operate and improve related systems to maximize data usability.【PayTaLab Growth Hacking Team】 The Growth Hacking Team is conducting CRM (Customer Relationship Management) to help users and business owners who use Pass Order experience the value of Pass Order and use it better. We drive product growth through experimentation and constant verification regardless of means and methods. The goal is to create an organization where data flows so that decisions can be made based on data rather than intuition. The Growth Hacking team is comprised of experts who always think from the user's perspective for business growth and know PassOrder better than anyone else. 【It would be great if you could share this information with us!】 • Please submit a portfolio or career description that includes a detailed explanation and specific materials about your experience extracting and analyzing data. • If you have experience presenting an action or carrying out a project through data analysis, please tell us the process of problem definition - hypothesis setting - experimental design - results. • I’m curious about what thoughts and values ​​you have as a data engineer. • Please submit your answers to the preliminary questions below along with your resume. - Why do you apply to Payta Lab? - When you join Payta Lab, what method and plan do you want to proceed with your work? 【If you come, you will do this kind of work!】 • We collect and manage data so that Passorder can make decisions based on data. • We provide a data-driven environment to help members make data-based decisions and derive insights. • Build a data pipeline by modeling and visualizing data analysis results. • Establish a data utilization strategy and define KPIs and key service-related indicators. • Predict the expected effects of solutions derived through data analysis and measure post-performance through testing.- Create dashboards and reports by refining/processing/loading behavioral data of connecting service users - Provide quantitative figures and analysis results so that the product team can make data-driven decisions - A/B testing to verify hypotheses Design and propose data logging methods - Derive insights from A/B testing and sprint results through data analysis and apply services - Collaborate with various related departments such as product team/marketing team/service operation team to establish hypotheses to define and solve business problems• Automate and advance Amplitude event & property design work request testing QA process • Design logs by communicating with planners and developers from multiple development organizations • Create test cases and monitor whether event logs are obtained according to definition • Monitor event log consistency and detect defects Proactive problem solving • Collaborate with Data Governance Manager Data Engineer to improve event log quality and automate management• Present recommendation/search model direction based on recommendation/search product analysis • Provide insights by setting various hypotheses about users and analyzing them • Setting service indicators and visualizing reports[Hospital Information System Development Recruitment] **This work mainly involves designing and modeling DW data for each work area (medical care, nursing, examination, surgery, etc.) for hospital data analysis. - Establishment of DW and big data based on hospital computer system - Development of statistical indicators related to hospital work and performance of hospital indicator system operation - Performance of Seoul National University operation work [Development environment] - SQL: Oracle MS-SQL Vertica - ETL: Pentaho Data Integration - Collaboration tool : github Teams• Classify and define user tendencies or situations based on the behavior logs left by the user. • Convert each user’s gaming experience into personalized content based on user behavior logs. • Plan actions to help improve retention for each user situation and verify their effectiveness through experiments. • Analyze the impact of personalized treatment on users to uncover insights that help improve retention.• Data analysis ＞ Problem definition, hypothesis setting, experimental design ＞ Data preprocessing and EDA ＞ Analysis modeling and indicator analysis ＞ Development of new indicators for value measurement • Communication ＞ Quantitative analysis/HR for effective consensus building and persuasion during the decision-making process INT visualization/reporting ＞ Dashboard development using BI and visualization tools (Tableau) ＞ Consulting and other collaboration with related departments such as business development, investment and marketing• Identify, collect, and utilize data suitable for analysis purposes through understanding tasks • Establish/operate large data pipelines (ETL Airflow, etc.) • Diagnose and upgrade performance issues during collection or processing • Establish and operate DW DM• Development of Hana Tour’s search service and search platform • Hana Tour’s package/hotel/flight/tour ticket/travel itinerary search and integrated search Auto-completion of popular search terms Typo correction Development and improvement of related search terms • Customers leaving by using Hana Tour service Provide customized travel information and travel products to customers quickly and easily • Create customer-centered value through customer experience data based on various travel information/travel products• OK Savings Bank reception service development• DB log design to define and collect/analyze indicators of the games we service • Dashboard planning and design to intuitively understand the data • Tracking and documentation of abnormal data within the game • Extracting and processing necessary data for business purposes Indicator visualization • Reporting to effectively deliver insights to relevant departments• Data collection modeling • Real-time data processing system design and construction • Data ETL work • Big data platform construction and operation work • Data pipeline construction and maintenance using Apache Airflow• Develop integrated control solution • Develop data platform solution • Carry out solution construction project • Carry out research project• Hana Tour integrated search recommendation service UI/UX planning and operation (APP WEB) • Search recommendation service project leading (consultation and communication with related departments) • Search ranking admin dictionary management planning and operation• Produce educational content in the field of data science. • Research and study for content creation • Curriculum planning • Video note practice quiz production • Feedback and reflection on content To put it simply, a content producer (content PD) is the ‘teacher’ who creates online lecture content. Codeit is a subscription service that lets you learn coding interactively. Codeit's differentiation strategy is to directly produce not only the platform but also the content. And the people who create this content are ‘content PD’. It's okay if you don't have any experience creating online courses. Our mission is to discover people with the potential to become content PDs and help them produce good content within the infrastructure provided by Codeit. And because the value they will create is enormous, we plan to provide certain treatment to those who meet the conditions. 【Work in a team like this】 The content team is the team at the forefront of facing and solving the problems of today's education. We are a team of excellent team members with both practical and content capabilities, and we always work hard to make it easier for students to understand and learn more fun. After careful consideration, we select the most necessary topics and plan the curriculum and create content so that all lessons can be organically connected. Rather than simply learning, we help people feel the joy of learning and fall in love with it.• Design/propose experiments and analyze results to improve services and achieve goals • Analyze performance of key indicators • Derive insights to improve services • Analyze data for customer growth strategiesSystematic management and analysis of sales and advertising operation costs • Manage cost efficiency by collecting, refining and analyzing sales and industry data • Monitor sales activity trends and suggest improvements Build an advertising cost and effectiveness dashboard • Utilize various data sources to improve advertising performance Visualize and construct effective dashboards • Build and operate data visualization reports customized for advertisers • Monitor advertising performance in real time and make decisions based on data Establish advertiser sales strategies based on data • Analyze customer data and industry trends to create customized sales strategies for individual advertisers Proposal • Contribute to strategy formulation through analysis of market trends and competitors- Development of a back-end system that can stably transmit data modeling and processing to efficiently use large amounts of data required for e-commerce and business analysis - Design and implementation of the overall ETL process from data collection/processing/visualization - Data analysis pipeline system Development and operation - Extraction and provision of various data from our company's services - Development and operation of a search service capable of semantic, personalized, auto-correction search using Elasticsearch - Development and operation of reports* Development and operation of a Python-based e-commerce data scraping engine * Analysis/design of overall data processing, including collection, purification, standardization, optimization, and DB loading * Python-based backend development- Data-based problem definition & derivation of insights for product growth - Statistical experiment design & hypothesis verification for product growth and user experience improvement - Data-based design and monitoring of key indicators - Data log design and management- Data analysis and experimental design for the core engine of the digital logistics platform - Defining key indicators based on data, conducting monitoring analysis - Demand/supply prediction modeling - Core engine for platform advancement and optimization modeling for operational efficiency - Production of data pipeline - Data Manage log and mart table design for analysis• Construction and operation of data-related infrastructure • Construction and operation of batch data pipeline • Design and configuration of new DB system data architecture • DBMS monitoring and troubleshooting• Establish a data analysis pipeline and develop data products to serve inside and outside of Kyobo Bookstore • Construct an environment such as cloud-based analysis infrastructure according to data governance policy • Review/research/research/research cloud products and solutions suitable for problem solving Deriving solutions through experiments - Building a cloud-based Data Lake Data Warehouse-based pipeline and developing API products - Technical implementation to build data governance - Development of data analysis dashboard development-related APIs and collection of necessary data (crawling) - Data-related cloud Product solution research and experiment ▶ Document screening ▶ Interview screening ▶ Health checkup ▶ Final acceptance• We develop and operate a cloud data platform that allows people from various occupations to easily access and analyze data. • Responsible for collecting various data generated by Bank Salad users, configuring a data lake, and designing, building, and operating a stable data pipeline and mart. We mainly use the technology stack (tools) below. • Data cloud platform: Snowflake • Data processing and analysis: Apache Spark AWS Glue Amazon Athena Apache Kafka Amazon • DMS • Language: Python Scala Go • Infrastructure: AWS Kubernetes Terraform • Storage: Amazon S3 Amazon DocumentDB• Design, develop and operate a data pipeline for stable processing of data collected and processed in the Hackle SDK. • Contribute to Hackle platform services by developing and operating a data pipeline for real-time distributed processing of large amounts of data. • Research and introduce new technologies to advance the data platform.[Problems we will solve together] • Data pipeline management - As products and businesses grow, the amount of data that needs to be collected and purified and the need for various analyzes based on it continue to increase. - Continuous management is required, including maintenance/repair of existing data pipelines as well as adding new pipelines to meet needs. - Additionally, as some pipelines are used in connection with products, operational issues such as monitoring alarms and quick action must be resolved. • Establishment of modeling/mart to improve data usability - At Lemon Base, thanks to in-house query training and passion for the growth of each Lemon Base member, most members can handle SQL, but each handles the same information with their own standards and methods. The number of ad-hoc queries is also increasing. To prevent this, I think it's right to build a data mart or data warehouse consisting of abstractions of appropriate resolution so that cleaned up and checked logic can be reused. - New features and products are being added quickly, and data modeling needs are also rapidly increasing, so we are exploring various ways to solve this. We are independently studying Data Mesh, which has recently been in the spotlight, and in the long term, we want to be able to manage modeling mart pipelines directly in each domain, and we are slowly taking steps toward this end. • Improved Data Discovery - Various tools, such as the production RDBMS Amplitude Pipedrive, and their data are accumulated and gathered into a data lake. A variety of data is already being collected, but logs to measure the impact of new features are also created every day, so the need for a data exploration system that can determine what data is collected, what it means, and how it is used is growing. there is. - Accordingly, we have introduced and are operating DataHub, a data discovery platform. When accumulating new logs and data along with operating a data discovery platform, we plan to establish a system to record and manage the necessary information. [Stacks currently being used or planned to be introduced] • Infrastructure management: AWS (IAM VPC EC2) Terraform Kafka Docker DB (MariaDB PostgreSQL) BigQuery • Pipeline management: Databricks Serverless Framework (AWS Lambda) Spark (pyspark) • Orchestration: Databricks Airflow or Prefect • Monitoring: AWS CloudWatch + AWS SNS + Slack • Data discovery: Datahub• Analyze and provide indicators for each service and company for data-based decision making. • Create the processes necessary for data-driven decision-making and build a data-driven culture. • Design the indicators needed for each service and related teams and improve the process for creating indicators. • Extract/process data to uncover new business insights. • Report the analyzed key statistical indicators to company members through dashboard data visualization, etc.• Extract, analyze and effectively communicate various data required for key business decisions. • Communicate closely with multiple teams to define important data analysis challenges that need to be solved and uncover insights that benefit customers. • Build and provide dashboards by establishing and efficiently visualizing important key indicators that need to be conveyed as data. • Analyze data across commerce/logistics and implement data modeling and processing processes to be loaded into the Data Warehouse and mart. • Build a culture and environment of data-based thinking, decision-making, and finding solutions.• Construction and operation of data-related cloud infrastructure/platform • Construction and operation of near-real-time batch data pipeline • Construction and operation of DataOps MLOps platform • Enhancement of data/machine learning products (prediction recommendation optimization, etc.)The Job Planet data analysis team performs optimization to improve product design and user experience, and contributes to product development by proactively conducting detailed and in-depth analysis. [Main tasks] • Set major KPIs, create hypotheses to achieve goals, and improve services based on data through experiments. • Set the direction of service and persuade related departments through data analysis. • Work closely with developer product teams to track, visualize and share KPIs with team members. • Design and manage data logs to understand service usage patterns.- Configure and operate the DB and Data Pipeline (ETL) that collects/manages Definery Tradingworks' data - Apply tools to provide a data analysis environment and develop and operate related infrastructure design - Establish DB operation backup/recovery policy Establish access control policy Performance Management monitoring engine tuning - SQL tuning and inspection work - Data transfer and migration- Digital marketing planning and execution from a data perspective (discovering insights for data-driven decision-making and marketing communication through them) - Establishing a customer data management strategy for marketing purposes (utilizing data collection and integrated analysis) - Data management according to the customer journey Establishment of system strategy - Data analysis and insight generation Communication - Digital campaign activation design tailored to customer retention and lifetime value - Setting hypotheses and indicators to achieve campaign goals Quantitative numerical analysis and campaign performance analysis/reporting and evaluation to verify this - Digital and utilizing platforms for data-driven marketing (Salesforce Adobe Eloqua D365 GA MMP CDP, etc.) - Data analysis and segmentation modeling, etc. (Campaign/Digital/Media/Retail/Machine areas) - 1st party data extraction/refinement/processing using SQL and analytics – understanding privacy laws and data change management.• Support business decision making through data • Manage improvement of existing processes and development of new indicators• Define business issues from a data perspective and suggest direction for decision-making • Analyze large amounts of data based on various user behaviors and derive insights that contribute to business • Data that eliminates bias that may occur in the decision-making process using A/B testing, etc. Building a system• MySQL database operation and automation - HA/DR configuration and operation using MHA - MySQL error handling and troubleshooting - MySQL monitoring and analysis • Service project support and operation - Data modeling and query inspection - Query tuning and index strategy establishment - Service monitoring and problem solving • MySQL internal research and technology sharing - MySQL database code analysis & bugs and performance patches - MySQL New Feature review and service application - MySQL-related open source solution review and application method review• Development and operation of text data AI analysis service - Leading AI-related project technology development - Current TA engine utilization service and server management • Text data analysis quality management - Learning model learning data management and advancement management (product review positive or negative judgment, etc.) - User TA advance management and advanced management• Define the data analysis tasks needed to improve the product and user experience, and based on this, derive improvements to solve the problem situation • Create various hypotheses that can solve user problems and conduct A/B testing to verify them Design and interpret • Structure and clearly communicate the insights and discussions discovered through data so that members can easily and accurately understand them • Analyze problem situations three-dimensionally and support members to discuss them from various perspectives • Carrot We systematically structure the market and team's target indicators and verify them logically and statistically • Consider and design an efficient data structure needed for product analysis and discuss with related departments to lead improvements.* Build and operate data infrastructure and platform based on AWS cloud * Provide data analysis environment for data-based decision-making * Design scalable data architecture- Interpreting data analysis results and writing reports (operating and producing data dashboards (Tableau/Redash, etc.)) - Refining/processing/analyzing behavioral data of application users served by Bemosoft and deriving insights - Product Marketing Development HR Finance, etc. Define business problems through collaboration with the team and establish hypotheses to solve them - Propose ways to improve business logic through hypothesis establishment and verification - Collect, analyze and visualize data to understand user behavior and ensure that the service meets user needs Check whether data analysis results are periodically reported and communicated to collaborating departments• Online channel development (homepage, mobile web/app) • Mobile web/app screen development • iOS professional development work1. Main tasks - Design and develop DBT Snowflake and Airflow pipelines from Data Ingestion to building Data Lake Mart. - Design and develop Kafka Real-Time pipeline architecture for processing real-time data streaming. - Design and manage a data platform infrastructure that can process large amounts of real-time and batch data. - We manage a pipeline that connects approximately 50 million NFT prediction price data with more than 8,000 ML Models to customer portfolio data every day. 2. Challenge - Build a pipeline to cleanse and analyze over 10TB of Ethereum and multiple blockchain data on a daily basis. - Establish an environment that can process more than 10,000 pieces of blockchain data per second in real time. - Build a serving environment so that the processed large data can be effectively used in an actual production environment. - Create a structure that achieves all of the above challenges while optimizing costs. 3. Customer value - We focus on creating data products that can provide the necessary data according to customer requirements, rather than simply being engineers who prepare data. - Currently, the data product we created is being used in our Desktop & Mobile Product API Service NFT Estimated Model. In addition, we plan to provide large amounts of refined and interpreted blockchain data directly to customers.- Development and operation of marketing data pipeline - Development and operation of data pipeline in Google Cloud environment - DW design and development• Develop and operate Data Garage that collects data, Data ETL that converts/extracts/loads data, and Data Farm that manages it. • Convert and manage collected data into a form suitable for AI learning.- Discover potential customers and message insights through customer site user analysis (GA4 MMP CRM etc..) - Elaborate digital media operation elements based on data analysis (Frequency Reach Creative etc..) - Data collection and analysis for online advertising performance analysis Methodology development - Marketing performance data analysis-based conversion increase and operational structure advancement methodology research - Design and creation of data visualization for marketing performance dataMain tasks include DB server operation for platform services (DBA work such as introduction of high availability solutions and DBMS optimization), DB design SP inspection and data migration for new projects or additional development. Platform services are the foundational services for the online game business and require technology and experience in processing large amounts of traffic and large amounts of data, as well as service stability and high availability. We expect that this will be an opportunity to accumulate professional capabilities and experience as a DBA through work in this position. In the case of this recruitment, we will participate in the development and operation of 'MapleStory Universe', an NFT-centered ecosystem among various blockchain projects to be introduced in the global market based on Nexon's representative IP 'MapleStory'. • Responsible services - Global platform (global PC platform Maple Story Universe, etc.) - PC platform (Nexon member Nexon Cash cash shop/coupon PC room billing, game coin Nexon.com, etc.) - Mobile platform (authentication billing, Push, community session, coupon mailbox, etc.) • Job description - Establishment and operation of database server - Optimization of database server performance - Database development support (system/DB design and query inspection)• Patient data-based artificial intelligence interpretation platform ‘Ontol’ service planning • User demand analysis service improvement schedule management • Overall service planning including storyboard function definition[Wort Intelligence Development Team - Introduction to the Data Part] - The Data Part belongs to the development team and collaborates with the back-end and front parts - The Data Part is responsible for the overall supply/demand/conversion/loading of patent data - Data-related tasks We are working on automation and advancement - Based on a deep understanding of data, we are carrying out various tasks related to it - We are discussing through code reviews for better development [Work to be done together] - Various types of data We are supplying and supplying the process and creating a pipeline - We are carrying out ETL work on various related data centered on patent data - We are dealing with the work of purifying and creating data to be entered into the search server - We are building the infrastructure to contain the generated data and files. I'm managing it• Perform data analysis project • Design and build data analysis system • Automate data collection preprocessing analysis visualization, etc.1. Advertise • Instagram, Facebook, TikTok DSP Apple Search Ads advertisement operation • Planning and production of performance advertising materials • Optimization of Naver Google SEO and organic app downloads • PPL collaboration with influencer celebrities active on Instagram YouTubers, etc. 2. Analysis • Data analysis using internal analysis tools and Excel • Reporting and optimization suggestions based on activity results • Establishment of service strategy based on understanding of brand concept and target • Strategy improvement and advancement through planning and execution analysis• Understanding search terms and developing search word-product relationship model • Data processing and serving/operation engineering required for ML model• Provides higher value to learners based on data from 1.2 million members and 47 million cumulative learnings. • Develop a personalized recommendation model based on user behavior data and apply it to actual services. • Design and run A/B tests and analyze results to uncover meaningful information to improve learner experience. • Extract information from data and derive insights through data management, analysis, visualization and modeling. • Improve data quality by identifying and sharing data problems. • Contributes to the performance of the infrastructure lab by supporting data-based decision making. [Technology Stack] Required skills: Clear understanding and excellent application skills are required. SQL Python TensorFlow Keras Preferred skills: Practical understanding and relevant experience are required. GCP AWS Spark Scala Java Ubuntu CentOS Linux• Analyze web/app service data and derive insights • Design and manage logs through collaboration with development organizations and planners • Build an analysis dashboard using tools • Derive business insights using Google Analytics[This is the job you will take on if you join us!] - Research and development of time series deep learning algorithm technology - Research and development of LLM model fine tuning techniques - Development of algorithms applicable to manufacturing defect prediction and Anomaly detection - Algorithm advancementThe main tasks are as follows. • Pipeline architecture configuration/development/operation • Introduce new methods or tools to analyze data more efficiently. • Extract the data needed by related departments and provide analysis results as dashboards or reports. • Design logs for areas that need improvement and conduct A/B tests to analyze the results. • Manage key KPIs and monitor abnormal indicators.• Integrate existing financial models and AI to complete complementary structures and develop new models • Develop financial services at customer contact pointsThe data team collects and analyzes all data required for S2W product development and operation and improves the efficiency and availability of related software. The collection part performs the following tasks in more detail. • Container environment management for collection/analysis/data pipeline • Development/management of core library for collection/analysis/data pipeline • Securing and analyzing confidential leaked data • Surface Web Deep/Dark Web SNS collection and collector operation • Bot detection /Development and application of blocking bypass technology • NoSQL self-operation, storage and management of collected data using Datalake • Development and operation of API for collection module• Table design • Query performance optimization through database monitoring • Database and Agent Job operation management • Permission management• Data analysis and verification work for small and medium-sized companies • Analyze data verification results and create monitoring design and automation scripts• Big data engineer• Recommendation model development and service advancement (hypothesis setting - modeling - post-analysis - reporting) • Musinsa recommendation order - sorting model advancement • User analysis and modeling using large-capacity log data• Apply statistical analysis and modeling techniques to large and small datasets, evolve existing strategies, and explore new directions not previously explored across a variety of industries and domains. • Develop deep domain expertise in managing your own end-to-end data workflows and the behaviors that emerge from your data. • Visualize and explore data sets to envision new predictive capabilities.• Automate the development and operation of data flow and pipeline designs • Develop and operate the appropriate infrastructure design for business requirements • Collaborate within and across data teams and automate test deployment processes • Integrate data using multiple third-party APIs • Integrate other domain AWS cross-account data • Event processing platform development consistency abnormality detection• Apply statistical analysis and modeling techniques to large and small datasets, evolve existing strategies, and explore new directions not previously explored across a variety of industries and domains. • Develop deep domain expertise in managing your own end-to-end data workflows and the behaviors that emerge from your data. • Visualize and explore data sets to envision new predictive capabilities.ㆍIncludes data analytics product pre-sales ㆍAnalyzes data analytics market trends ㆍPerforms extensive data mining and comprehensive data analysis ㆍPerforms PoC and BMT in collaboration with large-scale project teams ㆍCooperates with various business teams to implement models and monitor results ㆍNeed to communicate project status and results with various levels of customers• Data security product development (C/C++)We develop algorithm models to be used in various services for automation, from power generation forecasting, which is the most important use in the power brokerage business (VPP). We often deal with weather information and mainly analyze solar/wind power. [Work content] - Advancement of solar power generation prediction algorithm - Wind power generation prediction - Development of power brokerage business optimization algorithm - Development of new and renewable energy control algorithm• Harubang service data management- Development of crop image/video analysis technology for mobile robots for greenhouse surveillance and agricultural work (Object detection segmentation tracking depth sensing etc.) - Development of analysis and prediction algorithms required for greenhouse operation automation ※ Technology stack: Python TensorFlow PyTorch ROS C++[This is what we are doing!] • Processing large amounts of data using Apache Spark in a public cloud (AWS) environment • Developing large data analysis platforms and data pipelines • Developing detection solutions in collaboration with various fields, including developing data analysis services • Operate large amounts of data produced by detection solutions in the form of a data lake mart, etc. [We want to do this in the future!] • Establish data governance to manage and verify large amounts of data • More stable and testable data pipes Line development • Establishing an environment that can be operated efficiently by systematizing various workflows of detection solutions• Design logs for areas that need improvement and conduct A/B tests to analyze the results. • Define and manage key KPIs and analyze the causes of abnormal indicators. • Introduce new methods or tools to analyze data more efficiently. Technologies/Tools used in the data part • Analysis language: SQL Python • Analysis DB: AWS Redshift • Dashboard: Tableau Datastudio • MMP: Airbridge AppsFlyer • AB Test: Firebase Hackle• Indicator management through data extraction and preprocessing using SQL • Operation and production of data visualization and dashboards (Tableau/Redash, etc.) • Analysis requirement planning and data collection management • Project data analysis work in various fields • Through SQL R Python, etc. Data analysis automation work [Selection procedure] Document screening -> 1st interview -> Treatment consultation and final acceptance ※ There may be a coding test for each position applied. (Separate guidance will be provided when arranging the interview schedule) ※ Reference checks can be performed with consent during the selection process.We carry out work to transform AI technology into a service. • Develop in-service ETL/learning/prediction pipelines• Oracle EBS operation and Cloning Data Migration Database Patch application • In-house database commercial and open source operation and error handling/server management • Database backup recovery performance monitoring/diagnosis/tuning of Oracle/MSsql/MariaDB, etc. • Testing and application of database new features • Deployment of database physical schema design changes • Establishment and operation of database HA solution- Web analysis using web log-based tools such as AA/GA - Advertiser communication• Development of data collection/analysis solution based on semiconductor communication standards: Java server program development, analysis system design, 0 people· Development of safe driving score and fuel efficiency driving score algorithm · Development of accident classification and accident detection algorithm through IMU sensor analysis · Analysis of factors affecting fuel efficiency of traffic accidents using ML./DL algorithm (clustering regression analysis) · Analysis of driving and sensor data accumulated in AWS Deriving business insights through[Modeling TF Team Introduction] • Members from various fields, including data planning, NLP modelers, data PM, and AI researchers, gather together to continuously research and develop AI trends and related technologies that evolve every day so that they can be applied to TEXTNET services. • We are a dynamic team that does not stop trying various things, such as designing and upgrading language resources optimized for the language model research/development model that reflects trends and customer needs mainly through market analysis, and applying services after analyzing the latest research papers. • As a result of these efforts, the paper submitted to HCLT 2022, the largest natural language processing society in Korea, was recently selected as an excellent paper, and the paper submitted to ICKL 2023, the International Korean Linguistic Society, was accepted. [Tasks you will be responsible for] # We will develop and upgrade language generation models that reflect industry trends and customer needs for artificial intelligence (AI) / NLP technology. The main tasks are as follows: • Design/develop and implement high-quality AI data-based language generation model • Enhancement work to optimize NLP model and improve performance • Data analysis and proposal to secure data suitable for NLP model • Apply research and services on the latest AI and NLP technology etc. [Working Conditions] • Employment Type: Full-time_3-month probationary period • Working Hours: 10:00 ~ 19:00, 5 days a week • Workplace: Headquarters (Fast Five Seoul Station Branch) • Salary: Negotiated after interview (final salary and desired salary) Presentation) [Selection Procedure] • Document screening ＞ Assignment performance + practical interview ＞ Management interview ＞ Final acceptance • After the document screening, only successful applicants will be individually notified of the schedule. • The procedure may vary depending on each position and candidate. [Documents to be submitted] • Resume, self-introduction, career description or portfolio[Organization Introduction] The data engineering team within Jobplanet's R&D organization establishes an efficient data collection/purification/storage/analysis environment to make the most of data, the core asset of the service, with a Data 1st strategy. To this end, we design and build a data platform and are responsible for stable operation. It handles both structured and unstructured data in various forms, such as DB Log Text Image, and plays a pivotal role in business success by providing a technical foundation for effectively extracting insights from large amounts of data. [Main tasks] • Establishment and operation of optimal data pipeline and data platform based on understanding of services • Establishment of data lake and DW based on understanding of cloud-based big data processing solutions • Consistent and highly usable integrated logging and distributed tracking System design and operation • Efficient purification of raw data and definition of data structure for data analysis and machine learning • Development and distribution of analysis environment using data analysis library • Introduction and application of various query and visualization tools suited to data characteristics[Introduction to the Data Business Team] • The Data Business Team performs work in all areas that require language expertise, from text data construction to management, including language resource construction services and AI dialogue design services. • I am in charge of analysis, design, and construction of how to build the data desired by the client, and manage all matters necessary to carry out the project. • Rather than just focusing on building large amounts of data, we are building customized data professionally, including data content/quality/data design and inspection. [ Job details you will be responsible for ] • Data design/construction project implementation and management • Analysis and design of how to build the data desired by the customer • Estimation of schedule and manpower required to carry out the project • Creation of work tools and work guides and training of workers • Project progress management (progress status and issue inspection, schedule management, worker management, etc.) • Output management (data quality, inspection, delivery follow-up management, etc.) • Project-related communication and collaboration (client company workers in other departments, etc.) [Working conditions] ㆍRecruitment Type: Full-time employee_3 months probation ㆍWorking hours: 10:00 ~ 19:00, 5 days a week ㆍWorking location: Headquarters (Fast Five Seoul Station Branch) / Depending on the project situation, permanent work may occur at the client company ㆍSalary: Negotiated after interview (final salary) and desired annual salary) [Selection Procedure] ㆍDocument screening ＞ Working interview ＞ Management interview ＞ Final acceptance ㆍSchedule after document screening will be individually informed only to successful applicants. ㆍThe procedure may vary depending on each position and candidate. [Documents to be submitted] ㆍResume, self-introduction, career description or portfolio• Deals with swing growth data. • We work on growth, including Performance Marketing CRM Data. • Establish marketing strategies such as retargeting to acquire new users to grow traffic. • Improve retention through various CRM channels such as app push KAKAO SMS/LMS. • Supervises various campaigns such as coupon savings.1. Operation of our Cloudera-based big data platform 2. DW/DM operation 3. Linux server management and operation 4. Source management and operation distribution management (GIT management distribution automation) 5. Communication and collaboration with related departments[Introducing the Data Service Team.] • The Data Service Team is part of the Data Division and collaborates with the Data Intelligence and Data Platform teams to create an environment where Data Driven is possible and contribute to the growth of the organization. • Data scientists belong to the Data Service team and Data Intelligence team and develop various models that can improve products and increase BM efficiency. • We each take ownership of a project, but we work together to solve problems that arise in each other's projects. • Collaborate with colleagues across the company, including the Product Owner, Product Engineer and Growth team, to achieve company-wide OKR. [This is what the Data Service team is doing.] • We develop a model for preventing and detecting financial fraud (FDS), such as voice phishing and short-term delinquency. • Develop a personal credit rating model (CSS) optimized for Finda users. • Contribute to product improvement and expansion by analyzing various feature data such as credit score, loan repayment limit inquiry, interest rate/limit, etc. • Develop and advance models that can improve key product indicators. [Technologies used by Finda Data Service Team] • Python SQL • Apache Spark Delta Lake Airflow • Lightgbm Spark-ML Scikit-learn Pytorch • Git Tableau• Development of Hana Tour’s recommendation service • Development of recommendation/personalization algorithm and optimization through A/B testing • ML/DL model learning and serving development (vision ranking recommendation, etc.) • Travel information and travel information that customers want to take using Hana Tour service Provide personalized travel product recommendations quickly and easily • Create customer-centered value through customer experience data based on various travel information/travel products• Data monitoring solution development (Java JSP) • Development of advanced solutions • Creation of Dashboard and Scenario (DB query)• Operation and management of cloud-based databases and platforms such as GCP AWS • Design and management of data architecture for near-real-time data processing • Establishment of own log data collection monitoring and analysis processing system • Development of automated dashboard and implementation of data analysis environment • Close to business Building a connected data platform• Verification of data collected from various sources every day • Development and operation of data pipeline (ETL) for stable data interconnection • Construction and management of data storage such as Data Lake, Data Warehouse, Data Mart, etc. • Monitoring and optimizing the performance of data pipelines and systems (data partitioning indexing, etc.) • Collaborate with data scientist analysts and other stakeholders to define and refine data requirements.• Extract and analyze data based on online commerce customer data to verify hypotheses about customer behavior and derive insights through the analyzed results • Analyze data using internal and external data and design a dashboard to analyze service status and performance Proposal of a plan for business growth Accumulation of data derived from product sales volume and production order cycle and design of new product planning and improvement suggestions and ordering system Planning KPI for sales status and performance analysisㆍDW/DM design and construction ㆍData analysisThe task you will be responsible for is ‘Development and operation of AntiGravity’s body size estimation algorithm’! • Data model and analysis to be provided to various stakeholders of AntiGravity • Design and construction of models and systems to collect, process and analyze various data • Develop body size estimation algorithm based on users’ data • Recommend products based on size Algorithm development [Recruitment process] • Document screening ＞＞ 1st interview ＞＞ Technical assignment ＞＞ 2nd interview ＞＞ Final result• Responsible for overall data infrastructure configuration and operation. • Collect all data from product/order/payment and company-wide for data-based decision making. • Design and optimize streaming and batch processing pipelines for data collection. • Communicate closely with other departments to define and solve important data analysis challenges that must be solved together.• ELT process development using Spark Trino • Data sourcing and integration using 3rd party API • SQL-based statistical analysis • Building a serverless backend using AWS Lambda API Gateway (REST API) • Building a serverful backend using Fast API• Data labeling and inspection • Data classification and organization • Data quality improvement • Manual writing • [Join journey] 1. Document review - Please tell us anything that can demonstrate your capabilities or strengths. - The application form is a free form. The file format is also free. - It would be better if you also send a link to your portfolio or Github Notion. 2. Job Interview - Talk in detail about the position you applied for with your co-workers. - It takes about an hour and you can check any questions you have about the job. - Depending on the job, you may be asked to give a presentation about your portfolio. 3. Culture Fit Interview - Ainex and the applicant share their thoughts on each other’s direction and values. - It takes approximately 1 hour and allows you to learn more about the company. 4. Treatment Agreement - We provide compensation packages based on individual capabilities. - Detailed information on compensation package and joining date will be discussed separately. 5. Final acceptance[ BD Team Introduction ] • The BD Team is responsible for the company's growth by identifying customer needs and discovering new opportunities based on expertise in designing and building text-based, user-centered interactive AI learning data. • The main tasks are sales activities through various channels and based on this, discovering new customers and winning project orders. [Introduction of duties you will be responsible for] • B2B sales activities (Outbound / Inbound) to win orders for data design/construction projects (NLP data, conversation data, etc.) • Sales deck (proposal content messaging, etc.) to be delivered to customers through collaboration with relevant departments Securing sales leads through production and contact • Continuously discovering sales opportunities through communication with existing customers • Continuously implementing and improving which actions are most efficient based on the results after contact [Working conditions] • Employment type: Full-time_ 3 months probationary period • Working hours: 10:00 ~ 19:00, 5 days a week • Workplace: Headquarters (Fast Five Seoul Station Branch) • Salary: Negotiated after interview (presentation of final and desired salary) [Selection procedure] • Document screening ＞ Practical work Interview ＞ Management interview ＞ Final acceptance • After the document screening, only successful applicants will be individually notified of the schedule. • The procedure may vary depending on each position and candidate. [Documents to be submitted] • Resume, self-introduction, career description or portfolio• Upgrading the data pipeline system and building a company-wide data analysis system • Architectural design and technical implementation related to building a data governance system • Building data products for Kyobo Bookstore’s internal/external services • AWS cloud-based infrastructure according to data governance policy Operation and advancement • Development of solutions through review/research/experimentation of cloud products and solutions suitable for problem solving Document screening Interview screening Health examination Final passing[If you join, we will do this together] 1. Nubi Lab is developing technology to scan food using a 3D scanner and analyze the type and amount of food. Through this, we aim to create new value in the healthcare market by collecting and analyzing personal eating habits data and innovate the food-related market and industry based on data-based eco-friendly kitchen solutions. 2. Data engineers design and build a platform optimized for NubiLab big data analysis. • Design and build a data platform to efficiently process various data such as food scanned images, AI analysis service information, etc. generated by Nubi Lab. • We operate real-time batch tasks according to the purpose of data analysis and create an optimal workflow. • Applications used in the product are operated in a Kubernetes environment and a DataOps environment is created along with CI/CD-based DevOps operations. • We are thinking about and creating ways to process large amounts of data quickly and efficiently.- Planning and management of residential real estate data - Discovery and construction of new data through analysis of residential real estate trends - Management of residential real estate solution data# This is what you will do when you join • Help each squad design OKRs well based on the North Star Metric NSM. • Contribute to the design and result analysis of experiments conducted in the squad. • Create and manage dashboards that can provide insights to the team. • Analyze data needed for business and share the discovered insights with colleagues. • Design marketing performance analysis indicators (ROAS CTR CVR…) and analyze by medium.•Design, build, and manage data infrastructure, including a data lake warehouse database, to utilize a wide range of medical data. •Develop complex ETL/ELT pipelines and data orchestration processes using Airflow. •Develop and optimize data analytics tools based on Streamlit to drive business insights and decision-making across the organization. •Lead in automating the design and deployment of data systems on Google Cloud Platform (GCP). •Work closely with data scientists and ML engineers to ensure seamless data flow to AI solutions.• Establishment of DW and big data based on our system • Development of statistical indicators and management of indicator system operation database • Construction of data set for analysis • Analysis of time series data • Implementation of time series machine learning model • Visualization of dashboardShareRound's data engineers design and build data platforms and credit review models for services. • Design and build a credit screening model for installment payments (credit evaluation limit, estimated income, fraudulent transaction estimation, etc.). • Design and develop internal data products such as data funnel analysis, segment analysis, and A/B testing for service indicator analysis and improvement.[What kind of team is the IP Big Data Analysis Team?] - Introducing the IP Big Data Analysis Team, comprised of pioneers in IP Big Data Analysis. The IP Big Data Analysis Team's core value is to derive business insights by deriving the unique value of patent data through various data analysis techniques. - In addition, through the publication of a book titled “Patent Big Data,” a secret note on the career of the top 1%, the importance of patent big data and its various potential uses are presented from the perspective of patent decision makers, sales, marketing, and policy makers. - I hope that many people who feel the limitations of existing patent trend analysis and IP R&D analysis or who want to discover the value of patent data from a new perspective will apply. [What does the IP Big Data Analysis Team do?] - Various analysis techniques from a data perspective (e.g. topic modeling, natural language processing, network analysis, document similarity AI model, etc.) are performed based on patent data from Keywort, a global patent search service. Our core task is to discover meaningful insights from patent data through application. - We provide services that meet the needs of various organizations, including discovering new industries with potential for future growth, technology classification and filtering of large-scale patent data, analyzing IP portfolios among companies, and discovering demand companies for technology commercialization. - In addition, we are conducting research on new items and reviewing their feasibility while developing various services and processing data based on patent data. [What are the vision and goals of the IP Big Data Analysis Team?] - We are the first in Korea to commercialize and provide patented big data analysis. - Through collaboration with patent experts and data experts, we are advancing data analysis by applying new analysis techniques and generating a variety of new data. - From the perspective of a pioneer in IP big data analysis, we prioritize the usability and service expansion of patent big data analysis by improving the value of patent data and deriving objective insights into the intended business strategy. - Our core goal is to enable team members to become differentiated from others through analysis of IP big data, which is rapidly growing every year.• Collect and utilize data generated by Wavve. • Responsible for developing Wavve data services.• Data Pipeline design and development• Manage and operate Wavve's database. (MSSQL/MySQL) • Responsible for monitoring Wavve database infrastructure and troubleshooting.Data expert • Google Analytics and Adobe Analytics data analysis work for luxury brand clients • Inquiry/collection/processing of client data within GA and AA • Establishment and operation of Dashboard using Data Studio, etc. • Digital advertising execution and result analysis insight delivery • Internal leadership and Communication with customers• Development and operation of data pipeline (ETL) • Construction and operation of data warehouse and data mart • Development of new data analysis and visualization platform (data part)✔ The task you will be responsible for is ‘Developing and operating AntiGravity’s data pipeline’! • Data model and analysis to be provided to AntiGravity's various stakeholders • Design and construction of a model and system to collect, process and analyze various data • Develop a size prediction algorithm based on users' data • Develop a product recommendation algorithm based on size• Search intent inference • Develop AI models applicable to automatic content creation and other services• Daycon Basic and Monthly Daycon planning • Daycon Basic and Monthly Daycon-based education planning and design (problem design evaluation)• Database Migration • Database Performance Tuning Monitoring• Logistics system (2 locations) • Settlement system • Management information system (MIS)• Data processing system design/implementation/maintenance using GCP (Google Cloud Platform) • Develop and build CI/CD Pipeline to ensure that tasks such as data workflow, data conversion, data visualization dashboard, etc. are performed stably. • Understand data-related requests from team members and internal/external customers and convert them into technical requirements to help resolve them• MS-SQL / MySQL DB operation and management - MS-SQL large-capacity DB operation - MS-SQL 2019 Always On construction and operation - MySQL DB operation - MySQL MHA construction and operation - Query inspection and tuning - DB Architecture design and construction - DB Modeling • Recruitment procedure - Document screening - Interview (1st and 2nd rounds) - Personality test- Development of data visualization platform - Development of platform related to big data processing• Game data platform data blockchain data processing • DL/DW design, development and operation • Batch and real-time data ETL pipeline construction and workflow development • Data analysis/experiment environment construction and operation automation • BI system operation and advancement- Establishment of data-based customer strategy and derivation of insights - Proposal of data analysis and advancement plan to establish customer strategy1. As the leader of the only data organization within Com2uS Holdings, he has ownership of most of the data and can take the lead in his work. You can contribute to the collection, indexing and analysis of various data, including in-game data as well as marketing and social data. 2. You can organically collaborate with various departments, including business development and marketing platforms, and define decisions and processes for new things created in the process. 3. You can contribute to improving services through data analysis. As a data-based decision-making culture is already in place, meaningful analysis results can be quickly reflected in services. 4. You can use the best infrastructure based on Google Bigquery. You can quickly process data according to your desired conditions, allowing you to focus solely on analysis.• Development of stock market data collection & processing pipeline • Development of service data processing & analysis pipeline- Analyzing data and deriving insights necessary for establishing and managing a fulfillment service business plan - Proposing ways to advance fulfillment service operations and improve processes through analysis of data (behavioral patterns within distribution centers, cost productivity indicators, etc.) - Definition of new indicators and systemization of indicator management and visualization - support decision-making based on data (regular forecasting/performance reporting)• Build and execute on-chain data-based threat analysis/tracking product roadmap • Define service policy and process design and detailed product functional specifications • Research on-chain data-related markets and products • Manage on-chain data projects and lead collaboration with relevant personnelThis is the job you will be responsible for when you join. - Lead/manage the DB area work of the performing company as an internal PL in relation to the financial system and company-wide data mart construction project. - Lead/manage the organization responsible for company-wide DB operation after project completion.- Research and development of real estate data algorithms - Big data analysis (structured and unstructured) - Research and development of ML (machine learning) algorithms - Research and development of algorithms based on spatial information and building information - Utilization of GIS tools - Document screening → 1st interview → 2nd interview → Final acceptance - This announcement may be closed early as recruitment is completed on a rolling basis - Your valuable application will be reviewed on a rolling basis, and only those who pass the document screening will be notified individually. - Video interviews, additional interviews, reference checks, etc. may be conducted depending on experience in the field of application and other circumstances. - If false information is discovered in the application documents, employment may be canceled even after the employment has been confirmed. - For inquiries regarding recruitment, please send to recruit@ytp.co.kr.• Create an on-chain tracking/analysis report • Discover research and analysis data related to on-chain data • Build an on-chain data visualization dashboard • Derive threat detection insights and action plans based on on-chain data analysis • Analyze cyber threat group activities and collect data• Hospital CRM data visualization and data analysis • Visualization optimization planning for C-Level reporting • Data analysis and result derivation for decision making • Dashboard and report automation for performance monitoring • Indicator structure setting and expansion planning/execution• Kakao Healthcare data lake and data warehouse modeling • Cleansing of clinical and claims data collected from multi-center data sources • Data Mart/Cohort curation for research in the medical field • Cloud-based batchable incremental loading configuration• Provide data optimized for research by effectively exploring, extracting, and processing data • Research ways to improve the performance of AI models and effectively evaluate them through data • Establish and operate a continuous model A/B testing and distribution system using MLOps • AI Team Build and operate a key indicator dashboard to achieve goalsData structuring and pipeline architecture construction for sLLM service (chatbot) development Data extraction transformation and loading infrastructure construction Data standardization quality analysis research1) Applied AI data scientist & platform design and development - Develop/build big data platform based on new technologies such as AI - Apply data analysis algorithm and establish system construction direction 2) Big data modeling & analysis 3) AI task promotion and consulting project manager (PM) )• Development of artificial intelligence application system related to computer vision speech recognition/synthetic natural language processingSpoqa's data analysts play an important role in using data to create service growth. Communicates with the PM designer and developer who directly create the service and plays a leading role in data-driven decision-making. We are looking for someone to grow our kitchen board service together. • We use data to solve various service problems. • Define, track, visualize, and share key service indicators with team members. • Design and manage data logs to analyze service usage patterns. • Through user analysis, we derive and share insights that can directly affect the service. • Define and manage company-wide and department-specific KPIs.• Building and operating a data warehouse analysis/modeling environment in a cloud environment • Building and operating a data pipeline at the production level• Clodera-based data governance (collection/purification/storage/processing/analysis/visualization) operation • Big data platform and analysis platform infrastructure operation and technical support inquiry response1. Analysis and modeling using statistics/ML, etc. 2. Data analysis and modeling based on advertising e-commerce customer behavior 3. Planning new data analysis solutions and upgrading existing solutions• Artificial intelligence learning data construction project plan - Consultation with customer regarding data specifications - Establishment of project execution plan - Project cost and cost management - Data quality management and final delivery • Artificial intelligence learning data construction project practice - Data construction worker training/production of screening materials - Data construction worker management (training/monitoring)[If you join, you will be able to perform the following tasks.] • Responsible for responding to SNS channel CS and operating overseas promotions. • Responsible for supporting the creation of promotion results reports. • Responsible for supporting external remittance list management. • Responsible for assisting in organizing sales data. [Itinerary to join Team Egongegong]: Document screening - 1st working-level interview - 2nd management meeting - Treatment discussion and joining *Because the recruitment process is structured in consideration of position and career, etc., the process may change in some cases and additional interviews will be suggested. [Probationary period] - **This position is a one-year contract and may be converted to a full-time position based on evaluation. - If you pass, there is a 3 month probationary period. (Payment of 100% of salary benefits) - During this period, the team leader and HR team exchange transparent feedback through regular and irregular meetings, and performance and growth potential are reviewed at the final evaluation meeting before the end of the probationary period.• Leading on-chain data analysis tasks • Analyzing on-chain data Clustering Rating Pattern Anomaly • Detecting and predicting threats based on on-chain data analysis • Developing and operating various algorithms to perform the above tasks • Experience in performing machine learning-based analysis tasks preferred [Technology Stack] • Creating various types of DB advanced queries - Mysql Postgresql MongoDB Neo4j, etc. • Creating large data processing jobs - Spark Hadoop Kafka Presto • DW - Redshift Snowflake Tableau, etc.• Healthcare data ETL • Workflow development[This is the job you will take on if you join us!] - Image and video data processing - Research and development of the latest technology in manufacturing AI machine learning and deep learning algorithms - Research and development of algorithms applicable to manufacturing defect classification detection segmentation measurement - Algorithm advancementThe Job Planet data analysis team performs optimization to improve product design and user experience, and contributes to product development by proactively conducting detailed and in-depth analysis. [Main tasks] • Set mid- to long-term goals and priorities for the data analysis team. • Set the direction of service and persuade related departments through data analysis. • Establish key KPIs, create hypotheses and verify them with data to achieve goals. • Collaborate closely with product teams to track, visualize and share KPIs with team members. • Design and manage data logs to understand service usage patterns. Monitors whether the business is growing from an individual product level as well as a company-wide perspective and participates in Jobplanet’s key decisions.- Establishment of financial and non-financial alternative data system required for global securities investment decision-making - Establishment of NER Texonomy to optimize LLM-based AI NLP task performance related to stock market data - Establishment and advancement of global corporate disclosure data analysis system based on IFRS GAAP XBRL - Event-driven hedge M&A Algorithm modeling and tuning for investment decisions such as buy-outThis is the main task. • Build a data pipeline so that the sound source and composition data collected by Poja Labs can be used to learn artificial intelligence models. • Define sound source and composition data with the composition team and establish a system to easily collect and use the data. • We create an environment where all Poja Labs members can easily understand data and actively utilize it in each sector (business system development, marketing, etc.).• AI endoscopy data inspection • Work schedule can be adjusted (weekly working days and daily working hours) • Contract worker (6 months/extension possible)•Building our own solutions and providing technical support •Consulting on big data solutions (Elasticsearch Kafka, etc.) •Big data analysis / solution construction / technical support / visualization* Data analysis and data visualization solution (Tableau Salesforce) proposal and sales/technical sales * Existing customer management and new customer discovery * Related project/task proposal and order solution delivery, etc.• Data Science Team Project General Management • Analysis Project & Data Consulting Pre-Sales • Review of issues related to analysis projects and support for strategy establishment • Research on data analysis technologies such as AI/ML• Real-time streaming data engineering to serve recommendation services • Design and build a highly available and scalable data architecture considering continuously growing services- Processing of various information required for real estate analysis (actual transaction price / surrounding environment / redevelopment / reconstruction / convenience facilities) - Data pipeline design and automation - Discovery of additional data required for analysis• Game DB development and maintenance • ASP ASP.NET configured API BackOffice • Web management and development• Ice Cream Home Run DB operation and development • DBMS (MariaDB/MySQL) operation tasks • DB monitoring/tuning/error response • DB schema design • SQL tuning and inspection • Research and BMT for new technology introduction • Data provision through data extraction1. Main tasks • Design and development of autonomous driving analysis service using JavaScript • Development of new service of standalone version of existing service using Electron • Maintenance and repair of Partridge Systems service 2. Recruitment procedure and guidance Step1_ Document screening (after viewing 3 Response within 10 days) Step2_ Technical interview (face-to-face) Step3_ Final acceptance and treatment discussion ㆍ If the information provided during the recruitment process is different from the facts, employment may be cancelled. ㆍ If you have any other questions, please contact the hiring manager.• Database change log extraction • Data storage and extraction • Database• On-chain data collection and warehouse design construction and operation • On-chain data pipeline automation design construction and operation • Application of on-chain data analysis tools and development and operation of related infrastructure • Establishment of on-chain data governance for large-scale data management/verification [Technology Stack] • Design and operate various DB Schemas Create advanced queries - Mysql Postgresql MongoDB Neo4j, etc. • Build and operate a large-capacity data processing platform - Spark Hadoop Kafka Presto • Experience in building and using DW - Redshift Snowflake Tableau, etc. • Build and operate data pipeline automation - Airflow Jenkins ArgoCD, etc.Vision part - AI model development using CT MRI medical images and data from specialized areas - Research on segmentation algorithms based on medical images or specialized area data• Leading the MyData business • MyData-based financial service B2B business discovery and planning, partnership sales work • MyData service planning/operation/advancement• Construct data pipelines from collection from various data sources to the databases used by the service • Perform DataOps MLOps in the cloud • Explore and develop metrics based on academic data • Collaborate with other developers Develop academic services • Implement in-house services to manage data • Develop natural language processing and other models used in academic servicesTechnical support for Nethru analysis solutions - Technical support request management - System failure/inspection/issue management - System maintenance management• Production and management of artificial intelligence learning data- Establishment of data governance system - Establishment and operation of data governance-related system• Meticulous management and collection of various data • Records, archiving, publication, etc.• Big data analysis practice • Preprocessing/verification of structured/unstructured data • Exploratory data analysis, model implementation using algorithms such as machine learning/deep learning, etc. • Visualization of analysis results and writing of analysis report • Conducting theoretical and practical education related to big data analysis• Perform projects such as installing analysis tags and identifying/analyzing issues using our solutions• Developing/maintaining other chapters and Miridi services to solve customer problems and provide value • Solving and communicating with various teams’ data issues that arise in Miridi’s projects and goals • Maintaining important indicators Organize, develop, analyze, and monitor experiments. • Track design in detail from the U-I level to the funnel through Google Tag Manager and Miridi integrated log service. • Access real-time raw data through Integrated LogDB GA4 (Big Query), individual service DB API, etc., purify and process it, and use it for analysis • Plan and produce various reports and dashboards through Tableau Google Data Studio Google Spreadsheet GA4 Share • Manage and develop automation tasks and related infrastructure for various data • Understand the functions of various online advertising platforms and related data APIs, and operate and set up automatic report creation and automation script creation • Book seminar videos for growth ( You can receive support for articles)- Development of big data-based data hub solution[This is the job you will take on if you join us!] - Image and video data processing - Research and development of the latest technology in manufacturing AI machine learning and deep learning algorithms - Research and development of algorithms applicable to manufacturing defect classification detection segmentation measurement - Algorithm advancement• AI-based product development • Development and optimization of supervised/unsupervised detection models using machine learning deep learning algorithms • MLOps for building AI services- Development of financial data collection and reprocessing - Development of API - Development of natural language processing analysis• Perform data analysis project • Design and build data analysis system • Automate data collection preprocessing analysis visualization, etc.This is a job that pursues value creation in the manufacturing domain by utilizing AI technology. This position is responsible for solving manufacturing process problems based on artificial intelligence and developing generalized solutions through solving these problems. This role focuses on applying innovative technologies in manufacturing to improve productivity and optimize business performance. • Image data preprocessing: We develop algorithms for preprocessing and feature extraction and classification of images and videos (X-ray LED Pellicle, etc.) generated in the manufacturing process. • Defect area detection: Develop and apply models to detect defective areas in images and videos. • Recognition of defect structure: Develop and apply a model that recognizes the structure of the defect and measures physical quantities (length angle, etc.) of the defect to determine whether it is defective. • Few Shot Instance Segmentation: Leverage pre-trained meta models and prompts to develop a model that can detect bad areas with only a small amount of images. • Generation of defective images: Due to the nature of manufacturing, the defect rate is very low, making it difficult to collect defective images. To overcome this, we develop and apply algorithms and models that generate bad images.We develop systems related to the data life cycle, including collection, storage, and analysis of data-based products such as AnalyticDID RiskView. We develop solutions that process large amounts of data and manage input and output data of deep learning systems. We develop big data systems and database systems and linked processing modules. - Development of a statistical-based anonymization system for combining large files with pseudonyms - Development of a training and serving system to be used for artificial intelligence-based analysis - Development of a system for collecting and storing log data from personal devices such as PCs and analyzing them - Big data systems such as Hadoop Hive Spark and various database systems and development of data linkage processing solutions• Develop large-capacity 3D Timelapse data visualization software (* Visualization engine uses OpenInventor) • Develop strategic analysis software using large-capacity 3D Timelapse data • Apply AI-based image processing and analysis functions to analysis software • Link with cloud-based data system • User interface Development • Report generation function development 3D cell microscope operation software development[Introducing the “Chatbot Business Team”] • We are in charge of building and operating knowledge/language resources necessary for the chatbot construction project. • I am in charge of analysis and design on how to build the data needed by the client, and manage all matters necessary to carry out the project. • Internally, the team also plays a role in research and development of natural language processing tools and AI, and supports other departments when necessary. [Job details you will be responsible for] • Implementation and management of chatbot data design/construction project • Analysis of source data, definition of customer requirements, communication for consultation with business departments • Dialogue analysis and scenario design/construction for chatbot construction • Project site management/operation (Progress status and issue management, field personnel management, schedule management, etc.) • Output management (built data quality and inspection / build-up / testing, etc.) [Working conditions] • Employment type: Full-time_3 months probation • Working hours: 5 days a week 10:00 ~ 19:00 • Workplace: Headquarters (Fast Five Seoul Station Branch) / Due to the nature of the project, permanent work may occur at the customer company • Salary: Negotiated after interview (presentation of final and desired salary) [Selection procedure] • Document screening ＞ Working interview ＞ Management Interview ＞ Final Success • After the document screening, only successful applicants will be individually notified of the schedule. • The procedure may vary depending on each position and candidate. [Documents to be submitted] • Resume, self-introduction, career description or portfolio• Cardiography image data labeling work • Participate in the labeling planning and design stage related to model development • Image data quality inspection and management • In-house image data labeling tool function screen configuration plan suggested1. Construction and operation of a big data platform based on Hadoop Spark Kafka Elasticsearch 2. Construction of a large data collection, storage and analysis service system using the big data platform and development of related applications 3. Design of ideas for efficient data processing and platform improvement• Aerial photo/satellite image data labeling work • Labeling planning and design • Data quality inspection and management• Develop integrated control solution • Develop data platform solution • Carry out solution construction project • Carry out research project[This is the job you will take on if you join us!] - Research and development of time series deep learning algorithm technology - Research and development of LLM model fine tuning techniques - Development of algorithms applicable to manufacturing defect prediction and Anomaly detection - Algorithm advancementrequirement• 2+ years of back-end development or modeling experience • University-level knowledge of mathematical statistics and algorithms• A person with understanding and experience in the cloud environment • A person with the ability to operate a serverless database in a cloud environment and write SQL queries • A person with good communication skills with data analysts, data engineers, and other departments • Able to propose a back-end architecture that meets business requirements Person with • 3 or more years of job-related experience• 2+ years of experience • Backend development experience using Java Spring Boot framework • Experience with RDBMS such as MySQLㆍEducation: Elementary school graduate or higher ㆍExperience: 2 years or more ㆍExperience in back-end or server program development ㆍExperience in server development and operation using Spring boot ㆍExperience in developing and operating a RESTful API server ㆍExperience in development and operation using RDB ㆍOverseas travel A person who has no grounds for disqualification• Education: Any • Experience: 2 years or more • Experience in Java development in a Linux environment • Experience in Java-based distributed processing system and big data system development • No disqualification for overseas travel• At least 2 years of experience • A person who can build databases and develop backends using the major technology stacks specified • A person who can quickly understand requirements and smoothly design data models and REST/Websocket APIs• More than 2 years of practical development experience based on Django (Python) • Django REST framework design ability • Experience with Docker Git-based CI/CD • PostgreSQL-based DB design ability • Experience using Gunicorn/nginx/celery/AWS (preferred stack)• Education: College graduate or higher (Master’s degree preferred) • Experience: Experience (more than 1 year)• Have a bachelor’s degree or higher in the relevant field or related major • Have practical experience or teaching experience in the relevant field or related field• Have a bachelor’s degree or higher in the relevant field or related major • Have practical experience or teaching experience in the relevant field or related field1. Qualification Requirements • Have worked on a project with Java 8 or higher or Springboot or have equivalent experience • Have experience understanding and implementing HTTP/REST API design • Have smooth communication skills and an active collaborative attitude • Be responsible for code work Anyone with experience documenting progress and results 2. Key technologies: Java Spring boot JPA QueryDSL MySQL Redis RabbitMQ Docker 3. Work tools: bitbucket Jira notion Junit Jenkins Grafana PrometheusI want to be with someone like this! - Those with more than 1 year of web development experience - Those skilled in using SPA frameworks such as React, Vue and Angular - Those with a basic understanding of HTML, CSS and Javascript - Those who can think about the value of UI/UX - Utilization of SQL People with experience - Collaborative ability to define problems and find solutions with various job groups such as planning/design - People who enjoy learning new technologies and sharing knowledge [Development environment] - Key skills: JavaScript TypeScript React. js Next.js Recoil react-query Tailwind CSS - Frontend: React(next.js) - Backend: Python / Django Node.js - Infrastructure: AWS - DB: BigQuery MySQL - Build Deployment Automation: Docker ECS Amplify - Conventions: ESLint and Convention guide provided [Work environment] - Slack - Notion - Github- Graduated with a major or bachelor's degree in a related field - Experienced in data analysis and modeling based on ML or statistics - Experienced in playing mobile/PC/console games for at least one year - Proficient in using Python and SQL- Ability to utilize languages ​​such as Python and SQL - Experience in data analysis planning and collaboration - Experience in various analysis modelingQualification Requirements Those with more than 4 years of related work experience Experience with SQL Those who write highly readable queries based on logical thinking Those with experience in one or more languages ​​such as Python or R for data processing• A person with understanding of the production-level ML model development process and project experience • A person with experience solving business problems through data modeling • A person with good communication skills with data analysts, data engineers, and other departments • Able to use Python SQL proficiently Person • A person who is proficient in using deep learning frameworks such as Tensorflow, Keras, PyTorch, etc.- Dashboard development ability using visualization tool (Tableau) - Data handling skills using SQL - Experience in data analysis planning and collaboration• Those familiar with SQL-based data analysis and query writing • Those skilled in Tableau data visualization • Those with experience handling data-related programming languages ​​(Python) • Analysis of sales product customer data through web/app services A person with experience in deriving service improvement insights and applying improvements (3 years or more) • A person who can communicate responsibly between engineers and field personnel based on soft skills in understanding the overall business and company-wide organizational goals • Data industry department A person who performs work independently with a passion for technology and is familiar with finding and developing data that requires action Document screening ＞ Interview screening ＞ Health checkup ＞ Final passing- Those who are skilled in SQL data extraction and Excel data analysis and have more than 3 years of related work experience - Those who have experience leading improvement projects through data analysis - Solve problems by deriving logical inferences and insights based on analysis results Anyone with experience solving• Those with a basic understanding of Database Data Warehouse • Those with experience designing data marts or equivalent knowledge▷ We are recruiting people with this kind of knowledge and experience. • Graduate/prospective graduate of a related major such as statistics, computer engineering, information security, or equivalent practical experience/knowledge related to cyber threat analysis (new recruits available) • Experience in automated analysis using languages ​​for data analysis (Python R, etc.) • Statistics and data Experience in deriving insights using analysis technology- Experience in implementing data strategic planning consulting technology roadmap (at least 2 years) - Experience in discovering business items based on AI/big data【We're looking for someone like this!】 • A person with work experience related to data analysis pipelines and data-based services • A person who understands data statistically and can organize and summarize data • A person who can provide insights and solutions through data • Those with mathematical understanding and statistical knowledge of data • Those who are proficient in statistical/analysis programs such as SQL, Python R • Understanding of indicators and analysis methods from a growth hacking perspective, such as AARRR model funnel analysis and cohort analysis Those who are there• We are looking for someone with an understanding of mobile app user behavior data analysis. • We are looking for someone with experience in raising practical service indicators or solving problems through data-based insights. • We are looking for someone who has experience using A/B Test and app service analysis tools (Amplitude Firebase Google Analytics, etc.) or who has no difficulty applying new tools. • Looking for someone who can extract and process data using SQL. • We are looking for someone with logical thinking and communication skills who can derive and communicate analysis results.• More than 3 years of related work experience in management/strategy/business planning, etc. • Skilled in diagramming/documenting planning/strategy plans • Ability to derive strategies through data analysis • Ability to smoothly work with documents such as Excel PPT• Those with a bachelor's degree or higher in computer data-related (computer engineering and science, etc.) statistics, mathematics, agriculture, or other related fields • Those with proficiency in SQL R and Python programming languages ​​• Those with basic statistical knowledge • Logical thinking A person with analytical thinking, problem-solving skills, and the ability to collaborate with various stakeholders• A person with more than 3 years of data analysis experience • A person with Python development experience • A person with experience using Linux RDBMS • A person with a strong interest and interest in handling data• No experience required • One or more of the development languages ​​python, golang, java• Those who can learn new things and take on challenges to produce results • Those who are proactive and can define and solve problems on their own • Those who are interested in financial investment, financial accounting, data science, cities, etc. and want to build related knowledge and experience• A person who can design the steps from business problem definition to hypothesis establishment and verification • A person who can extract data using SQL • A person who can present an action plan within the organization using analysis techniques (KDD CRISP-DM, etc.) • Analysis A person with visualization and documentation capabilities to communicate resultsChannel Talk is a B2B SaaS service that is experiencing unrivaled growth in Korea and continues to face problems for which there is no reference. For excellent problem solving, we are looking for people who can learn quickly and properly. I want to work with someone who has a tenacious tendency to dig in, solid basic knowledge, and a quick learning curve. Specifically, they are as follows: • Those with more than 5 years of data analysis experience or equivalent experience or capabilities • Those with high level of SQL proficiency • Can be used for visualization work using BI tools (Excel, Tableau Redash, etc.) Skilled person • A person with a high level of logical thinking ability to find the simple core in complex phenomena • A person with excellence in internal and external collaboration and communication • A person who considers creating self-directed impact in problem solving to be an important value• A person with relevant job experience who can query the desired data through SQL • A person with understanding and experience of analysis methodologies required for mobile and web services, such as cohort analysis, A/B testing, and funnel analysis • Data-based service/business problems A person who can design the steps from definition to hypothesis establishment and verification • A person who constantly considers data• Able to analyze data-based market trends • Able to use data analysis and visualization tools such as R, Python, Excel, POWER BI, etc.- More than 1 year of data analysis experience. Experience in improving actual products using data. - Experience in solving problems and developing various hypotheses for service growth through data analysis. - Ability to independently design experiments and interpret results. - Those with Python SQL capabilities or experience using data tools required to handle and visualize data - Those who can communicate easily to understand data-based insights, including experiment results• A person who can design the full cycle from business problem definition to hypothesis establishment and verification • A person with a basic understanding of machine learning • A person who can efficiently extract data using SQL (MySQL/MSSQL, etc.) • 1 Those who are familiar with using more than one type of analysis and modeling tool (Python R SQL Pandas TensorFlow Pytorch Spark, etc.)• Those with experience in data-based recommendation services and projects such as machine learning and deep learning • Those with experience in processing large amounts of data such as BigQuery Hadoop Elastic • Those with development project experience and programming knowledge such as Python• Experience in collecting/processing/loading large amounts of data such as SQL Spark Airflow Zeppelin Snowflake • Query skills and tuning skills such as SQL • Experience working in cloud environments such as AWS GCP • Proficient in one or more programming languages ​​such as Python ScalaA human like this can create synergy • People with more than 2 years of NodeJS experience • People with more than 1 year of Typescript experience • People with more than 1 year of React experience[Education] - Master's degree or higher from a 4-year university (Ph.D. level preferred) - Related major such as statistics, computer engineering, industrial engineering, etc. [Experience] - 4 or more years of data analysis work experience [Required Competencies] 1. Experience in carrying out at least 3 big data projects is required. - Deriving insights through business analysis - Large-scale data processing (data collection/processing/processing) - Model development and verification using analysis tools such as R/Python - Deriving business insights through data analysis 2. Technical skills in data analysis specialized areas Possess network and related work experience - Introduce/apply Machine Learning/Deep learning-based processes to data analysis 3. Possess understanding of statistical concepts and parallel/analytical processing (understanding big data architecture) [Required skills or certifications] - Large-capacity DBMS processing - Analysis support language/package (SQL/ SAS/ R/ Python/ Spark) user or certification holder - Hadoop Hive Hbase processing• Those with more than 2 years of modeling experience using TensorFlow, PyTorch, Scikit-learn, etc. • Those with a high understanding of data/statistics/machine learning/engineering • Those with logical thinking and communication skills• A person with more than 3 years of experience making an impact in the data domain • A person who can well define problems for complex problems • A person who can use statistics to remove bias that may occur in data or decision-making processes • Data A person who can explain the information obtained through A/B testing in a format that anyone can understand • A person with a deep statistical understanding of A/B testing and causal inference• Those with more than 3 years of data analysis experience • Those who are proficient in languages ​​required for data analysis, such as Python and SQL • Those with practical experience in A/B test design interpretation • Experience in service growth and problem solving through data analysis • A person with communication skills for smooth collaboration with colleagues in various fields • A person who can work together with governance managers and data engineers to improve infrastructure at the company level • A person who is not afraid of new tools/knowledge and is willing to learn ※Please include sources that can verify your coding skills, such as EDA sample blog URL and Github URL, on your resume. ※If you cannot include it on your resume, please submit it as a file.• Project experience in leading and verifying analysis hypotheses and drawing meaningful conclusions • Experience in feature engineering data in a variety of ways • Knowledge of technical statistics to interpret data • Skilled programming capabilities based on one or more languages • Self-motivation• Must have completed a master's degree or higher • Be proficient in the language for data extraction and analysis (SQL Python) • Have basic knowledge of data visualization tools (Tableau) • Knowledge and experience of data analysis methods such as Funnel AARRR Cohort Analysis • Those with experience designing and conducting A/B tests by creating hypotheses based on data • Those with a high understanding of marketing indicators (first open retention conversion rate)• More than 3 years of data analysis experience or equivalent experience • High understanding of data analysis methodology and related content (LTV AARRR Retention Cohort Funnel • etc.) • High understanding of data utilization tools such as R SQL Python • Data such as GA Amplitude High understanding of log analysis• Those with more than 1 year of product or business analysis experience • Those with practical experience in data analysis using SQL (BigQuery PostgreSQL) Python, etc. • Those familiar with configuring and visualizing dashboards through Redash • Designing mobile service logs or Individuals with analytical experience • Experience analyzing data and insights and presenting clear, actionable plans• Those with more than 3 years of related experience • Those with experience understanding and designing ML models • Those familiar with one or more languages, such as SQL, R, and Python • Those with more than 3 years of experience in visualization and reporting through creating dashboards • Logical based on data Thinking and problem-solving skills • Communication skills to smoothly communicate data analysis results • Ability to understand phenomena through rational/critical thinking • Flexibility and adaptability to change [Request] Please attach your resume. [Optional] Portfolio• Those who have experience using Apache Airflow • Those who can develop backend in one or more development languages ​​such as Python, Java, C, and C++ • Those who are interested in cloud technology stack • Those who have a strong sense of responsibility • Those who take initiative in carrying out their own work • Communication skills Who can explain this excellently and persuasively?• MS Office Utilization Award (Excel, PowerPoint) • More than 1 year of data analysis experience- A degree or practical experience in a field related to statistics and data mining is required. - You must be familiar with languages ​​and frameworks for data analysis/modeling (Python SQL Pandas TensorFlow Scikit-learn, etc.). - The ability to collaborate with members within the company and communicate smoothly with various departments is required.• A person capable of developing and operating a data lakehouse in a cloud environment • A person with experience in batch or near-real-time data processing- Those with at least 1 to 3 years of experience as a Data Analyst or Data Scientist - Those who have directly extracted and purified data using SQL - Those who have experience creating data-based insights and actions in collaboration with related departments such as marketing CS- Experience building and operating a data platform• People with more than 3 years of mobile data analysis experience and experience using data to improve actual products • People who have developed various hypotheses for problem solving and product growth through data analysis • Independently designed experiments and reported results A person who can interpret • A person with Python SQL capabilities or experience using data tools (Superset, etc.) required to handle and visualize data • A person who can easily communicate data-based insights, including experiment results, so that anyone can understand them• Those who have experience solving problems through hypothesis-based thinking • Those who have an understanding of machine learning and deep learning • Those who have experience building real-time recommendation systems and applying them to services or have equivalent knowledge • Those who use the TensorFlow PyTorch ML framework Someone who can handle it well- Those who have experience extracting and purifying data directly using SQL - Those who have the ability to use Python SQL BI Tools (Superset Data Studio (Looker Studio), Tableau, etc.) - Define problems based on logical thinking, create hypotheses, and A/ Those who have experience designing and executing B tests - Those who prefer and are skilled in quick thinking and strategic decision-making - Those who enjoy identifying problems, solving problems, and enjoying rapid growth• Those who can handle data and report results through Python or SQL languages ​​• Those who have modeling experience using frameworks such as Tensorflow or Pytorch • Those who have experience collecting/purifying raw data and deriving insights • Those who have experience in collecting/purifying raw data and deriving insights • Managing complex data through SQL, etc. A person who can extract and process desired data from a data set • A person who can verify hypotheses and derive insights based on statistical knowledge• Relational database and SQL proficiency • Python-based data analysis (Pandas Scipy Seaborn etc.) proficiency • In-depth statistics and ML knowledge • Excellent problem-solving and collaboration skills• A person with at least 5 years of experience in the field of data analysis or equivalent capabilities • A person who can directly define the problem they want to solve and solve it by setting and testing hypotheses using data • What kind of measures are needed to determine whether the business is growing? A person who can identify the cause of change by drilling down according to changes in indicators, whether an indicator is needed • A person with experience in understanding and practical application of analysis methodologies such as cohort analysis, A/B testing, and funnel analysis • GA BigQuery Amplitude Tableau QuickSight, etc. Anyone with experience using data analysis tools• A person with more than 5 years of relevant practical experience • A person majoring in statistics or possessing equivalent knowledge • A person with actual service operation experience from planning to improving a personalized recommendation model • Able to efficiently solve difficult and complex problems through SQL A person with advanced skills • A person with experience in collaborating with data engineers to solve various problems- A degree in statistics or data mining is required. - You must be familiar with languages ​​and frameworks for data analysis/modeling (Python SQL Pandas TensorFlow Scikit-learn, etc.). - The ability to collaborate with members within the company and communicate smoothly with various departments is required.- Those with research skills, such as collecting, organizing, and analyzing data on a specific topic *Must attach at least one analysis report of their own when applying (company/industry/market analysis or thesis/research document) - Smooth communication with developers, designers, and business personnel Anyone who can• A person with more than 3 years of data platform development or governance work experience or equivalent skills • A person who can process descriptive statistics and analyze data using SQL • A person who can program in Python• A person with experience processing large amounts of data in a distributed environment • A person with experience in building data pipelines A person with experience in data preprocessing • A person with proficient development capabilities in one or more of Java, Python, and Scala • A person with experience in processing large amounts of data • Clarify their work Someone who understands and can take responsibility for their actions5 to 10 years of related work experience (data commercialization modeling development capabilities, etc.) Able to analyze data using Machine Learnig Able to develop python JAVA Able to use various analysis tools (SQL R, etc.) Experience in analyzing customers/products/brands and deriving business-related insights• More than 2 years of data analysis experience • Ability to extract and analyze data using SQL • Ability to perform statistical analysis using Python (Pandas Sklearn) • Experience in or interested in corporate credit and financial analysis • Proficient in SQL • Those who can process and analyze raw data by using • Those who have mathematical/statistical knowledge and high-level logical thinking and problem-solving skills • Those who have experience in the entire data analysis process from data extraction, preprocessing, analysis, and derivation of insights• Those with experience processing data using Python • Those with experience developing and operating backend systems for data processing【We're looking for someone like this!】 • A person with work experience related to data analysis pipelines and data-based services • A person who understands data statistically and can organize and summarize data • A person who can provide insights and solutions through data • Those with mathematical understanding and statistical knowledge of data • Those who are proficient in statistical/analysis programs such as SQL, Python R • Understanding of indicators and analysis methods from a growth hacking perspective, such as AARRR model funnel analysis and cohort analysis Those who are there- A person who deeply sympathizes with the problem that connecting is trying to solve and can work together to solve it - A person who can extract and purify data through SQL - A person who can use mobile service data analysis methods (Customer Lifetime Value Retention Cohort Analysis, etc.) - Those who have directly improved services by setting up and experimenting with various hypotheses to grow indicators - Those who have a high level of understanding of user funnels and A/B testing and are able to design and analyze data for them• Those with more than 2 years of experience designing event logs • Those with advanced or intermediate Python skills • Those with experience in introducing/log designing/maintaining user behavior log analysis tools such as Amplitude GA4 • Actively seeking to improve/advance the current status A person who can define a problem • A person who goes beyond diagnosing the current state and designs considering expandability• A person with at least 4 years of DA experience • A person skilled in data processing/extraction/analysis/reporting using SQL Python, etc. • A person who can derive insights based on an understanding of data/statistics/machine learning • Logical thinking skills A person with communication skills- Those who can write SQL queries - Those who have completed related courses (computer engineering, big data statistics, etc.) or courses - Those who are interested in medical and healthcare data• A person with more than 2 years of practical experience in the field of statistical data analysis and machine learning • A person who can process data into the desired form using Python SQL, etc. • A person with a basic understanding of data placement pipelines• A person with more than 3 years of data analysis-related work experience or equivalent capabilities • A person who can design the steps from business problem definition to hypothesis establishment and verification • Intermediate or higher statistical analysis skills (regression/causal/survival analysis, etc.) • Those with extensive knowledge of engineering and mathematics, such as statistics, linear algebra, and calculus • Those with specialized knowledge in one or more areas of ML, such as User Modeling, NLP Recommendation System, etc. • Efficient data management using SQL (MySQL/MSSQL, etc.) Those who can extract • Those who are familiar with using analysis and modeling tools (Python R SQL Pandas TensorFlow Pytorch Spark, etc.) • Those who have visualization documentation capabilities (Tableau/PowerBI, etc.) to convey analysis results[Education and Experience] • Those with a degree from a 4-year university or higher (related majors such as statistics, computer engineering, industrial engineering, etc.) • Those with 4 or more years of data engineer experience or equivalent capabilities [Required Competencies] • Large-scale data collection/ Those with processing/processing capabilities • Those familiar with using analysis support languages/packages (SQL/R/Python/Spark) • Those with experience using Hadoop-based big data platforms • Those with experience performing data flow automation • Individuals with experience developing data pipelines using Python/SQL • Individuals with technical network and related work experience in the data analysis field• More than 3 years of Java-based web service development experience • Application development experience using SpringBoot • Sor Elasticsearch-based system design/development experience • Service experience using Oracle MySql NoSql • Linux environment operation and related scripting skills • AWS Possess development and operation experience using the JS framework • Possess administrative development capabilities using the JS framework• Majored in an IT-related department / Holds IT-related qualifications / Completed IT professional training • Has computer architecture knowledge • More than 3 years of related work experience - Development of IT reception work in the financial sector - Development of program language (JAVA C, etc.) - Framework environment (BXM Anyframe) UI Development tool (WebSquare Miflatform) development• More than 4 years of data-related practical experience • Statistical knowledge and ability to extract and process data using related tools (SQL Python, etc.) • Ability to document results derived based on data • Communication for smooth collaboration with multiple teams Someone with good skills• More than 3 years of programming experience such as Java, Scala, Python, etc. • People with understanding of Hadoop eco-system (Hadoop Spark Hive, etc.) • People who can design service-oriented architecture with scalability/reliability • Data structures and algorithms in OS/database A person with computer knowledge[Required Requirements/New Entry] • Education: Any • Proficient in Java SpringBoot mybatis or JPA SQL development • Individuals who perform their work with a sense of responsibility• Those with more than 3 years of experience in search planning and operation (required) • Those with experience in search data analysis and recommendation services • Those with smooth communication skills with relevant departments • Ability to analyze requirements through user perspective and business understanding minuteCode It always aims for world-class content. We conduct thorough verification, invite only those who are certain, and provide certain treatment. • A person with excellent capabilities and knowledge related to data science • A deep understanding of the basic knowledge of mathematics and statistics required for data analysis • Able to build and optimize various models based on an understanding of machine learning and deep learning algorithms • Python You can be good at data collection, pre-processing and visualization using tools such as SQL, Excel and Tableau • Actively learn the latest trends in the field of data science • Excellent content creation capabilities • Write concise and easy-to-understand articles • Create new content quickly and accurately You can do research • You are confident that you will never make a mistake in your explanation • You understand well what people find difficult and what they are curious about • You have excellent communication skills • You are confident in understanding and reflecting on feedback • On the other hand, you can provide good feedback • I am confident in efficient and clean communication• More than 2 years of relevant experience (or equivalent competency) • Skilled in data analysis tools such as SQL Python • Basic knowledge of growth strategy • Interested in quantitative analysis • Smooth communication with people involved in planning, marketing and development, etc. ability• At least 2 years of experience in data analysis or business intelligence • Proficient in data querying and manipulation, including SQL • Experience using data visualization and dashboard tools • Able to solve problems and establish strategies through data-based decision making Person • A person with excellent communication and collaboration skills-More than 2 years of development experience in related fields -Experience in collecting/processing/loading large amounts of data such as SQL Spark Airflow Zepplein Snowflake -Query skills and tuning skills such as SQL -Experience working in cloud environments such as AWS Azure -Python Java Kotlin C++ Go lang Ability to solve problems using more than one language - Deep understanding of Elasticsearch - Deep understanding of Tokenizer* Those with experience in developing and operating large-scale website scraping services (Python) * Those with experience in data scraping using various methods * Those who are capable of developing to avoid various scraping defense techniques * Those with experience in handling data such as Python SQL Those who are proficient in the language * Those who have experience operating development services in a Linux environment * Those who have experience collaborating using Git- Those with more than 3 years of product data analysis experience - Those with experience in driving product growth with statistics-based analysis/experiment insights - Those with data preprocessing analysis modeling skills using SQL, Python, R, etc. - Colleagues in various fields People with extensive experience in collaboration and data storytelling[Experience] • At least 5 years of data modeling experience [Required Competencies] • A person who can design experiments and verify statistical hypotheses to enable data-driven decision making • A person who can create machine learning models and optimization models through Python • Collection of data A person who can use SQL for processing and• Those with more than 2 years of DB-related experience • Those with a skilled understanding of DB or SQL • Those who are proficient in using Python or similar programming languages• A person with experience in data solution or platform construction projects • A person with knowledge and ability to use ETL process, cloud-based Data Pipeline Data Lake • A person who is proficient in using Python SQL in relation to data processing • New data-related technology A person who enjoys prototyping based on curiosity • A person who can communicate clearly between data analyst service planners and data scientists• More than 3 years of experience developing and operating large-scale data processing systems or equivalent capabilities are required. • Must be proficient in one or more backend programming languages ​​(e.g. Go Python Scala Java). • Requires expertise in Database Data Warehouse concepts and language (SQL NoSQL). • Experience in building and operating a cloud-based data lake mart is required. • Problem-solving skills, team collaboration, and smooth communication with other departments are required.• Those with development capabilities for data application development (one or more of Kotlin, Java, Python, and Go Scala) • Experience in utilizing large-capacity distributed systems (Hadoop Kafka Spark, etc.) • Experience in processing large amounts of data and quasi-real-time data processing in an MSA environment(1) Those who aim to build a career related to the following: • Using Spark and other big data frameworks • Designing and operating data infrastructure in cloud environments such as AWS GCP • Scalable to match Lemon Base’s improved and growing products and services Data architecture configuration • A-Z experience and design of data infrastructure and operation • Data mart design construction and operation • Data product/service operation (2) Individuals whose personal tendencies resonate with the following • Challenge new technologies and methods to overcome existing problems People who enjoy improving methods • People who aim to find and improve better methods rather than following conventional methods and rituals • People who are interested in soft skills such as collaboration and efficient communication with colleagues outside of engineering • Company A person who has experienced deep interest and concern for people in Esau• We are looking for someone with more than 3 years of data analysis-related experience or equivalent skills. • We are looking for someone with domain experience related to payment, finance, commerce, and fintech. • We are looking for someone who can extract/process desired data using SQL. • We are looking for someone with experience leading A/B testing and experiment design. • We are looking for someone with excellent communication skills and experience providing data analysis-related advice to various teams/job groups.• Have more than 5 years of data analysis and engineering experience • Have high level of SQL or Python proficiency • Experience with data visualization using BI tools such as Tableau Radash• Those with more than 3 years of experience operating a cloud-based data platform • Those with experience operating cloud infrastructure such as AWS/GCP • Those with experience operating distributed data tools such as Hadoop Kafka and cloud data services such as S3 Athena Redshift • Using SQL Python Individuals with service development experience and experience in developing and operating workflow tools such as Airflow • Individuals with the practical experience necessary to stably operate and improve data pipelines and able to make appropriate decisions and lead team members • Collaborate amicably with data-related departments A person who can communicate, interpret the needs of non-data experts, and derive the best operational direction• A person with 2-5 years of data analysis experience • A person who can directly extract and clean data using SQL • A person who can freely process and visualize extracted data using Excel Python R • A person who can analyze key service indicators People who have directly improved services by creating and experimenting with various hypotheses to set and grow indicators • People who have experience designing A/B tests to verify hypotheses, analyzing the results, and applying them to products • Experience in multi-device (mobile / app) A person with experience in integrating and analyzing data • A person with a high understanding of product data analysis methods such as funnel analysis, retention analysis, and cohort analysis • A person with experience designing and managing data logs using GA Amplitude, etc. • Knowledge of basic statistics Someone who has- Those with more than 3 years of large-scale data ETL experience - Those who are able to work on basic SQL/Stored Procedures - Those who have an understanding of RDBMS - Those who have experience handling AWS or Cloud Infra - Those who can develop scripts to automate DB operations- Those with work experience in customer data/customer experience (CX)/digital marketing/CRM/digital media, etc. - Those with experience in mar-tech and ad-tech operations are preferred - Those with a sincere and proactive work attitude - Any major with a bachelor's degree or higher - Those who can speak Business English are preferred.• Those with at least 3 years of data analysis experience • Those with proficient SQL usage capabilities (SQL coding test conducted separately) • Those with experience in business intelligence reporting dashboards such as Redash Tableau Power BI • Rapidly changing business environment A person who is capable of multitasking with the ability to judge work priorities that meet the• A person with more than 2 years of commerce-related practical experience • A person who is proficient in data analysis and machine learning-related languages ​​(Python SQL R, etc.) • A person with a deep statistical understanding of A/B testing and causal inference minute• A person who has a general understanding of MySQL and can operate from a service perspective • A person who is proactive and proactive in communicating with developers • A person who enjoys challenging new technologies • A person with more than 3 years of MySQL experience• Education: Graduated from a 4-year university or higher • Experience: 7 or more years of related experience • Experience developing or operating text analysis services • Ability to use Java Spring programming language • Excellent collaboration and communication capabilities in related departments • Cloud-based Those with service development experience • Those with an understanding of RESTful APIs and networks • Those with an understanding of NLP• A person with more than 5 years of mobile data analysis experience and experience improving actual products based on data-based insights • A person who can three-dimensionally think about various hypotheses for problem solving and product growth through data analysis. • A person who can independently design and interpret experiments with a deep statistical understanding of A/B testing and causal inference • A person who is skilled in querying and processing data using Python SQL (Bigquery), etc. • A person who can A person who can effectively convey and communicate insights based on data, including* Those who are able to build and operate a data platform in a cloud environment * Those who are proficient in one or more back-end programming languages ​​(Python / Scala / Java, etc.) * Those who have an understanding of databases such as RDBMS Nosql * Develop and operate large data processing systems * Those with experience or equivalent capabilities * Those with experience developing APIs related to model serving * Those who can collaborate and communicate clearly between related departments * Portfolio submission is required.- Those with more than 2 years of data analysis-related practical experience - Those with general knowledge of data analysis, machine learning, or data-based marketing - Those with experience in the entire data analysis process from data extraction, preprocessing, and derivation of analysis insights - Free use of SQL - Those who can process and analyze raw data - Those who can derive results and suggest logical improvements through analysis results - Those who have experience in data visualization (Tableau Amplitude, etc.) - Those who can work in various fields such as planning, marketing, and development Those who are capable of smooth communication and collaboration• IT major / IT professional training completion / IT certification holder • Knowledge of computer architecture • More than 3 years of related work experience - Website mobile front/back-end development and operation - Native Android/iOS developmentWe are looking for someone like this 1. Language - Someone who is proficient in at least one language among Python/Scala/Rust 2. Realtime Process Engineering - Someone who has experience processing large-scale data by building real-time pipelines with various use cases 3. Analytical Engineering - Those who have experience modeling data in a data warehouse and serving it as a valuable data mart. 4. Backend Engineering - Those who have experience using real-time and batch data and building and serving Data API Server.- More than 3 years of data engineering development/operation experience or equivalent capabilities - Proficient in development using Python, SQL, Airflow, and Scala• Bachelor's degree or higher in a department related to computer engineering or software development • At least 2 years of data-related work experience • Experience developing data pipelines using Airflow • Proficient in Python and JavaScript • Experience collaborating using Git • Those with experience in DB construction and linkage development * Required documents/free form 1. Resume 2. Self-introduction (do not describe previous university transfer) 3. Portfolio materials related to the job- Those with experience in data analysis and insight discovery - Those with an understanding of the overall web and app environment, including HTML/Javascript - Those with an understanding of the AdTech Eco-System (DSP SSP DMP Ad Exchanges RTB etc..) - Google Tag Manager Google Those with an understanding of the digital data ecosystem, including Analytics Campaign Manager 360 and App Tracker* At least one of the requirements of 1) or 2) is required 1) A person with basic understanding and operating experience of RDBMS • Understanding of normalization and denormalization • Understanding of transaction level and characteristics • Understanding of JOIN (NL Merge Hash join) • MySQL or PostgreSQL or SQL Server MS-SQL operation experience and knowledge • Understanding of Index 2) Basic understanding and operation experience of NoSQL • Redis Sentinel Cluster configuration and operation experience • (Optional) MongoDB Replica Set configuration and operation Someone with experience• Ability to actively define problems and set goals • Experience in planning multiple services/products • Communication and collaboration skills • Understanding and experience in UX/UI • Ability to write project documents- 3+ years of experience - Those who can build Airflow through Python - Those who can quickly become familiar with new technologies - Those who have basic knowledge of databases - Those who can work patiently to improve data• New recruits • Those majoring in computer engineering/statistics/industrial engineering, etc. • Those capable of performing a series of data processing processes such as data collection, extraction, conversion, loading, etc. • Those capable of various manipulations of required data by writing SQL• Ability and experience in self-serving Facebook, Instagram, and TikTok advertisements • In-house or agency experience (at least 1 year) • Experience in project execution based on understanding of service planning/design/development processes • Active collaboration with relevant departments A person who can communicate and collaborate• A person with more than 3 years of NLP or language model-related development experience • A person with a high understanding of data/statistics/machine learning/engineering • A person who can freely use TensorFlow, PyTorch, Scikit-learn, etc. • A person with logical thinking and communication skills• A person with more than 5 years of relevant practical experience • A person majoring in statistics or possessing equivalent knowledge • A person with actual service operation experience from planning to improving a personalized recommendation model • Able to efficiently solve difficult and complex problems through SQL A person with advanced skills • A person with experience in collaborating with data engineers to solve various problems• At least 2 years of web/app service data analysis experience or equivalent work skills • Intermediate or higher SQL skills • Ability to perform data analysis visualization and machine learning using R or Python[We need someone like this!] - People with experience in machine learning/deep learning projects (Prediction Anomaly detection Classification) using Python - People with experience using Tensorflow or Pytorch - Data purification preprocessing analysis using packages such as Pandas Matplotlib Those with modeling and visualization experience - Those who are interested in the latest technology (LLM, etc.) - Those who take the initiative to create work and lead responsibly - Those who can define problems and find solutions together with other job groups [Essential for work] This is the technology stack] - Tensorflow - Pytorch• Experience as a data engineer with more than 2 years of experience • Able to extract and process raw data using SQL to analyze data • Experience designing and building a data platform in a cloud environment (AWS) • Ability to use ETL solutions • Familiar with various tools or analysis methodologies For those who can learn and apply new things, the technologies/tools used are as follows. • ETL: Hevo Data • Language: SQL Python • DB: AWS Redshift MongoDB Mysql • Dashboard: Tableau Datastudio • A/B Test: Firebase Hackle• Experience: 2 years or more • Gender: Any • Understanding of financial investment and asset allocation models • Introductory understanding of AI algorithm structure and operation • Python-based data search and analysis • Analysis using Pandas Numpy matplotlib seaborn, etc.▷ We are recruiting people with this kind of knowledge and experience. • Container environment management for collection/analysis/data pipeline • Development/management of core library for collection/analysis/data pipeline • Securing and analyzing confidential leaked data • Surface Web Deep/Dark Web SNS collection and collector operation • Bot detection /Development and application of blocking bypass technology • NoSQL self-operation, storage and management of collected data using Datalake • Development and operation of API for collection module• Experience: 3 or more years of operational management experience with MySQL, MariaDB or SQL Server • Education: Any (related majors are preferred)• Education: Any academic background • Experience: 5 years or more • Knowledge and understanding of DBMS SQL data processing • Experience in writing and performing data combination detection analysis/verification test cases • Positive attitude and sense of responsibility Willing to solve problems until the end • Commitment to work Ability to understand and explain problems without difficulty communicating with othersㆍExperience of more than 3 years in data pipeline development and operation ㆍExperience in Kafka Spark stream processing processing and operation ㆍExperience in Hadoop Eco System operation ㆍExperience in languages ​​such as Python SQL• Recommended: Those with more than 3 years of modeling experience • Those with a high understanding of data/statistics/machine learning/engineering • Those who can freely use TensorFlow, PyTorch, Scikit-learn, etc. • Those with logical thinking and communication skills• Minimum of a bachelor's degree in data science, statistics, computer engineering, or a related field • Experience with statistics such as distribution analysis, classification, and clustering • Excellent coding skills using data frames ※Required conditions (e.g. Panda R Matlab Apache Spark ) • Demonstrated experience emphasizing innovation creativity and intuition (e.g. ability to identify useful data laterally and think outside the box) • Experience related to the digital asset industry or Those who have a passion to learn and build a career in the asset industry• Those skilled in using Python 3.7 or higher • Those with more than 3 years of related experience • Those who value solving fundamental problems • Those who communicate actively • Experience in operating BigQuery • Experience in building Airflow and operating DAG• Minimum of a bachelor's degree in data science, statistics, computer engineering, or a related field • Experience with statistics such as distribution analysis, classification, and clustering • Excellent coding skills using data frames ※Required conditions (e.g. Panda R Matlab Apache Spark ) • Demonstrated experience emphasizing innovation creativity and intuition (e.g. ability to identify useful data laterally and think outside the box) • Experience related to the digital asset industry or Those who have a passion to learn and build a career in the asset industryㆍA bachelor's or master's degree in statistics or computer engineering ㆍA person with an understanding of big data/machine learning ㆍA person with certification or experience in SAS/R/Python/SQL ㆍAble to communicate in English• Education: College graduate or higher • Experience: More than 5 years of experience • Able to develop C/C++ • Able to develop in Unix/Windows environments- Those with more than 3 years of relevant work experience - Those with experience in forecasting by analyzing time series data - Those with experience in analyzing and implementing algorithms such as papers - Master's/Ph.D. degree in statistical analysis artificial intelligence/machine learning or equivalent Experienced person - able to use Python language• Graduated from a related department • BI / R- Those with more than 3 years of vision project experience or equivalent skills - Those with experience in data collection fusion and model development with various sensors (RGB-D LiDAR IMU) - Those with experience in development/operation of commercialized products and services - A person who can develop products through smooth communication with each member - Sympathize with and join Iocrops' mission and vision to contribute to the high-tech industrialization of modern agriculture and realize an Autonomous Greenhouse equipped with overwhelming production technology. ※ For foreign applicants, TOPIK (Test of Proficiency in Korean) level 5 or higher or equivalent Korean communication skills[We want to work with people like this!] • Those who are proficient in one or more programming languages ​​such as python and scala • Those who have experience processing large amounts of data using distributed processing systems and have at least 2 years of related experience • Those who have experience in processing large amounts of data using distributed processing systems Anyone familiar with data engineering work• App service data analysis practical experience with at least 2 years of experience as a data analyst • A person who can analyze data by extracting and processing raw data using SQL • A person with experience analyzing app data and improving products based on it • Funnel analysis cohort Those who have experience deriving insights by applying methodologies such as analysis A/B testing to practical work • Those who have no difficulty learning and applying new tools or analysis methodologies• Experience: Regardless (new graduates can also apply) • Statistics/computer/mathematics majors are preferred • Persons subject to military service exemption• Graduated from a computer engineering department or at least 3 years of ETL development experience • Experience designing and developing ETL processes • Experience in data analysis using numpy/pandas • Experience in Agile and Scrum culture• A person with more than 5 years of related experience • A person with experience in Install Backup Recovery for Oracle Mariadb, MSSQL, etc. • A person with basic Shell Script skills, such as implementing Backup Script Index rebuild • A person with experience in database performance management using Open Source • Basic A person with tuning and index management skills through query analysis- Those who graduated from university (4 years) - Those who can extract and analyze data through Adobe analyitcs - Those who can use MS Office and other statistical programs (R/Python/SQL/SPSS) - Those who have experience deriving insights based on web log data - Those who can communicate smoothly with related departments and clients - Those with at least 3 to 7 years of relevant experience• Advanced Java • Spring Boot (Spring Framework) • Understanding JVM / GC • Linux server status analysis / management • Understanding TCP/IP HTTP Web environment • Understanding data structure algorithms • Proactivity in analyzing and solving problems • Collaborative communication skills· At least 3 years of data scientist experience · Master's degree or higher in mathematics, statistics, or data science-related major · Excellent use of R Python · Possess mathematical understanding and statistical knowledge of ML/DL · Portfolio submission required[We are looking for people like this] • More than 3 years of development experience related to NLP-based deep learning/machine learning • Proficient in using AI frameworks such as Python and Tensorflow / Pytorch • Pre-training and fine tuning of transformer-based language models (BERT GPT, etc.) -Experience in tuning • Experience in development and deployment using cloud-based AI frameworks such as AWS GCP • Experience in improving AI model performance based on data • Enjoy collaboration through smooth communication with colleagues in various roles Who is• A person with more than 3 years of data engineering practical experience or equivalent skills • A person with experience in large-scale data batch processing or asynchronous data processing in a distributed environment • A person who is proficient in one or more programming languages ​​and data analysis libraries • AWS architecture Anyone with design or operation experience[We are looking for people like this] • 3+ years of experience related to artificial intelligence data design/construction • Project PM/PL experience required (NLP / NLU / language resource construction, conversation design/construction, data analysis/design/construction, data voucher, etc.) • AI A person with general knowledge and experience of basic technologies and the latest trends • A person who can communicate smoothly between clients and teams• A person with practical understanding and insight, such as funnel analysis, cohort analysis, and retention • A person who can extract and analyze data such as SQL BigQuery • A person who can think comprehensively about AARRR’s User Flow • More than 3 years in relation to app services Those with customer development experience[Education and Experience] - Those with a bachelor's degree or higher in a computer engineering/industrial engineering field - Those with more than 5 years of data analysis platform work experience or equivalent experience [Required Competencies] 1. Hadoop-based big data platform A person with experience using it (required) 2. A person with an understanding of and configuration experience in big data platform architecture 3. A person with technical network and related work experience in a specialized field of data analysis 4. Experience in operating a Linux server (Redhat CentOS, etc.) 5. Those who have an understanding of DB (MySQL, Oracle, etc.) 6. Those who have good communication skills for coordination with related departments and outsourcing companies, etc.• Those with more than 7 years of data analysis and modeling experience • Those with experience using big data platforms such as Hadoop Spark • Those with experience solving real business problems using major ML frameworks • Experience with data analysis model implementation and operation Anyone who can perform the course• More than 3 years of experience developing Python development services • Experience developing and serving ML/DL models • Experience developing services using one or more frameworks of Django Flask FastAPI • A person who understands and can implement recommended related papers • Linux Possess the ability to operate the environment and related scripts • Have experience using RDBMS such as Oracle MySql and noSql • Ability to logically communicate with people collaborating with you • Have an understanding of the Pytorch framework• High level of understanding of JSP Java Spring Mybatis Jquery • More than 5 years of web-based solution development experience or equivalent experience developing results• Senior with more than 5 years of experience as a data engineer and developer • Experienced in designing and improving cloud and open source-based data architecture • Experienced in operating cloud-based databases and platforms such as RDB (MySQL) NoSQL (MongoDB) Kafka • Python Java Scala Individuals with experience processing data in one of the following languages ​​• Experience designing and building DataWarehouse and DataLake in a cloud environment • Individuals capable of smooth communication with business personnel• Those with experience designing and operating large-capacity data processing systems • Those with experience developing and operating in AWS cloud environments • Those with experience building and using Airflow • Those with experience building services in container-based environments • Various visualization tools and Anyone with experience in improving data verification and operational efficiency by linking analysis tools• Experience improving services through customer behavior (DB LOG) data analysis based on commerce web for more than 1 year • Ability to use two or more languages ​​for data processing (SQL Python, etc.) • Gain insights and improve areas through user behavior data analysis A person with experience in finding and applying services • A person who leads work proactively with a passion for service in a rapidly changing environmentㆍEducation: Any ㆍ2 or more years of related experience ㆍExperience in ETL construction or operation ㆍExperience in data modeling DW/DM design and construction ㆍSQL skilled Experience in developing large-capacity queriesIs there anyone like this somewhere? • A person with more than 1 year of machine learning-related work experience • A person with experience in E-Commerce and platform business • A person familiar with one or more programming languages ​​among Scala, Java, and Python • A person with an understanding of the use and operation of ML basic algorithms (regression) decision tree neural network recommendation clustering) • Build and operate a data pipeline using SQL Spark Hadoop BigQuery Airflow, etc. • A person who can understand and implement machine learning papers and apply them to business models• We are looking for someone with at least 3 years of relevant experience or equivalent data engineer work experience. • We are looking for someone who is proficient in at least one Python or SQL programming language, which is essential for handling data. • We are looking for someone with experience developing data pipelines (real-time/batch) for processing large amounts of data. • We are looking for someone with experience using data analysis and visualization tools (Redash Pandas, etc.).• Seed ~ Series A round startup experience • Data engineer experience (data pipeline development experience) • Real service operation experience (game app, etc.) • Understanding of K8S • Major related to quantitative analytical field at a leading domestic and international university (e.g. Those who have earned a bachelor's, master's or doctoral degree in mathematics, computer engineering, physics, industrial engineering, financial engineering, etc. • Those who have majored in computer engineering or have equivalent major knowledgeWe hope that if you join us, you will be able to help us in this regard. • Data labeling and inspection • Data classification and organization • Data quality improvement • Manual creation[ We are looking for people like this ] • Less than 5 years of B2B Sales experience (Junior) • People with a strong interest in artificial intelligence and data services • People who enjoy social media and community activities (LinkedIn Blog Facebook Instar, etc.) • People for proposal writing Those who are skilled in using PPT • Those who enjoy the process of acquiring new knowledge or skills • Those who enjoy collaboration through smooth communication with stakeholders• A person with experience in performing data solution or platform construction projects (more than 8 years) • A person with knowledge and ability to utilize ETL process cloud-based data pipeline • A person who is proficient in using Python SQL in relation to data processing • New A person who enjoys prototyping based on curiosity about data-related technologies • A person who can communicate clearly between data analyst service planners and data scientists[Looking for such colleagues] • A person with more than 2 years of related work experience • A person with capabilities in developing and operating a data platform based on open source solutions • A person with experience in understanding and building system requirements for large-scale data processing and analysis • Those with experience processing big data using Spark • Those with experience operating Airflow Jenkins • Those with technical capabilities in SQL Python • Those with project management and problem solving capabilities • Those with high creativity, innovativeness, and result-oriented thinking capabilities minute- Those with a good understanding of the real estate market - Intermediate or higher in OA utilization# Join us with Moyo ＜Looking for this kind of person＞ • A person with more than 3 years of product data analysis experience • A person with the SQL capabilities necessary to preprocess and visualize data • A person with experience creating and analyzing various hypotheses based on data • A person who can proactively look at data and derive insights rather than performing a given analysis • A person who has explained data insights, including experiment results, to colleagues•Bachelor's degree or higher in computer science/engineering or related field •5+ years of experience in data/ML/software engineering •Experience with ETL/ELT pipelines and data orchestration using tools such as Airflow Prefect Dagster •AWS Azure GCP Experience with cloud-based data solutions such as Snowflake Databricks • Proficiency in Python and SQL • Proficiency in Git/Github for version control and collaboration • Proficiency in Docker/Docker Compose for software containerization and orchestration • Excellent problem-solving skills and collaboration skills• Proficient in writing SQL queries and procedures • Time series analysis, statistics, deep learning, machine learning • More than 2 years of development experience for data preprocessing and modeling • Knowledge of mathematics/statistics• More than 5 years of related experience • Those with back-end server development capabilities in at least one of Java, Kotlin, Scala, and Python • Those with advanced SQL and intermediate capabilities in Python • Data mart based on understanding of credit rating model development environment and institutional regulations Individuals with experience in leading design, construction, and operation • Experience in improving operational efficiency by linking various visualization and analysis tools • Individuals capable of operating Airflow and troubleshooting• More than 3 years of experience in patent research analysis or big data analysis• A person with more than 5 years of experience in collecting and visualizing user logs • A person with development experience using the Python Node.js go language • A person with experience developing message queue-based data processing such as Kafka • A person with experience developing Restful API• Education: Bachelor's degree or higher • Experience: 3 years or more • A person with more than 3 years of experience in data collection and pre-processing, or someone with equivalent skills • A person with skilled SQL skills and experience in Python data processing• A person with more than 3 years of experience operating a database in a cloud environment • A person with experience in development (monitoring automation, etc.) for database operation • A person with experience in deployment automation in an operational environment• Those with an understanding of the data structure of Google Analytics or Adobe Analytics • Those with a general understanding of performance marketing such as digital advertising/campaigns • Those with basic business English • Those who can freely use Excel and PowerPoint • Active • Those with a proactive mindset • Those who can communicate smoothly with others • Foreigners can apply• Those with more than 5 years of data engineering experience • Those with experience in real-time platforms (Kafka etc) and distributed processing (Spark etc) • Those who are able to develop REST API for data processing✔ Is there anyone like this anywhere? • A person with more than 2 years of machine learning-related work experience • A person with experience in E-Commerce and platform business • A person familiar with one or more programming languages ​​among Scala, Java, and Python • A person with an understanding of the use and operation of basic ML algorithms (regression) decision tree neural network recommendation clustering) • Build and operate a data pipeline using SQL Spark Hadoop BigQuery Airflow, etc. • A person who can understand and implement machine learning papers and apply them to business models• Those who majored in machine learning-related fields such as computer engineering, statistics, artificial intelligence, etc. • Those who have at least 5 years of experience in data analysis and machine learning • Experience in processing unstructured data such as images and text and developing deep learning models • Implementation of thesis/technical research and those who can test • Those who can use one or more DL Frameworks such as TensorFlow Pytorch＜Qualification Requirements＞ • Those who have a bachelor's degree or higher in data or other fields and plan to obtain it before March 24 • Those who have relevant knowledge of ML, DL statistics, etc. - Submit GitHub or other links • Those who are proficient in Python R or PyTorch languages Those with experience in at least one competition (Daycon Kaggle, etc.) • For men, those who have completed military service or are exempt from military service and are not disqualified from overseas business trips ＜Required information (submitted when applying)＞ • Share one code from the Daycon competitions Submit to tab - Code evaluation items 1. Problem analysis and strategy 2. Readability 3. Delivery 4. Technical skills 5. Resume• Education: Any academic background • Experience: 2 to 6 years • Have at least 2 years of database-related experience • Have at least 1 knowledge of DBMS (My SQL MariaDB MS-SQL Oracle PostgreSQL)• Education: College graduate or higher • Experience: More than 5 years • More than 5 years of Python JAVA Spring boot development experience • Use of PostgreSQL • Experience in operating settlement system • Experience in operating logistics system • Experience in operating internet shopping mall and e-commerce• Ability to use SQL Python or Java • Ability to use Looker Looker Studio and LookML • Experience with Google Cloud, such as BigQuery Dataflow Pub/Sub and Cloud Functions • A strong understanding of database structure theory principles and best practices • Data modeling and data warehousing and Experience building ETL pipelines• MS-SQL 2008~2019/MySQL 8.X technical knowledge • Linux/Windows technical knowledge • Public Cloud management and operation experience- More than 5 years of Java development experience - Experience in web application development - Person with service implementation capabilities• Those with more than 10 years of data engineer experience or equivalent skills • Those skilled in processing various source data (SQL NoSQL log third-party, etc.) • Those with experience in public cloud (Azure GCP, etc.) environments • Linux and open-source Individuals with application (airflow superset, etc.) operating experience • Positive and proactive about collaboration • Individuals with team management experience• A person with experience in deriving business insight through data analysis project planning/discovery • A person who can design the steps from business problem definition to hypothesis establishment and verification • A person who can explore and analyze data for service advancement based on CRM data • Those who can efficiently extract data using SQL (MySQL/MSSQL, etc.) • Those who are familiar with using analysis and modeling tools (Python R SQL Pandas TensorFlow Pytorch Spark, etc.) • Data collection in big data environments (MySQL MariaDB NoSQL MongoDB, etc.) and handling experience • Those with project leading (PM or PL) experience • Those with more than 10 years of data analysis-related work experience or equivalent capabilities • For effective consensus building and persuasion during the decision-making process A person capable of quantitative analysis/insight visualization/reporting1. A person with more than 7 years of data analysis-related work experience is required. Sufficient experience is required because you must manage not only BI but also most tasks related to data and numbers. 2. I hope you are someone who enjoys playing games. The best data analysis results can be obtained when not only analysis theory but also experience as an actual game user is added. As a user, I would like someone who enjoys playing the game and as an analyst, I would like someone who can sharply break down the game. 3. Experience in forming hypotheses, processing data, and drawing conclusions is required. The tool can be anything, including Excel, Python R, etc. What is important is the experience of forming a hypothesis and finding “meaning” in a large amount of data. 4. Log design, which is the basis for BI, requires ETL and data processing capabilities through SQL. Because SQL is the basic competency for both indicator development and data analysis, you must be able to freely process the data you want.• More than 3 years of Python development • People with interest and experience in building DW Data Lake Data Mart • People with interest in and experience in distributed data processing and building data pipelines • People with understanding of RDB such as PostgreSQL • Knowledge of data structures and algorithms Those with an understanding of • Those with experience using cloud services such as AWS • Those with the ability to easily use collaboration tools such as Airflow, git jira, etc.- Those with more than 6 years of practical experience related to data analysis - Those with experience solving problems by deriving logical inferences and insights based on numbers and data - Those skilled in using data analysis tools (Excel SQL, etc.) - Bae Min B Anyone with interest or understanding of Mart Baemin Store and Logistics Center• Have more than 5 years of product planning experience related to blockchain data analysis and security forensics or have equivalent capabilities • Able to understand and model the structure of various mainnets such as Block TX • Have relevant knowledge of data-related products such as data analysis and data visualization • Understanding of on-chain data analysis/tracking tools (Dune Messari Chainalysis Qlue MetaSleuth, etc.) • Ability to utilize various tools to lead service planning and collaboration (Slack Figma Notion GA SQL, etc.) • Preferably someone with a strong understanding of data modeling • Practical experience in on-chain data visualization preferredWe are looking for someone with these capabilities and experience. - Those with 7 to 15 years of work experience - Those with MySQL system operation experience - Those with monitoring and performance analysis experience - Those capable of DB Performance Tuning /SQL Tuning - Possess smooth troubleshooting capabilities The person who did it- At least 2 years of DBA-related experience - A person with a variety of problem-solving skills - A person with experience in DB design data models based on an understanding of service structure• More than 3 years of data analysis-related practical experience or equivalent capabilities • Ability to understand and model the structure of Block TX, etc. of various mainnets • Ability to utilize SQL/HQL and data visualization tools (Tableau Maltego Gephi, etc.) • On-chain data analysis /Experience in using tracking tools (Dune Messari Chainalysis Qlue MetaSleuth, etc.) • Competency in writing advanced queries for various types of DB (Mysql Postgresql MongoDB Neo4j, etc.) • Experience in developing work automation tools • Preferably someone who can perform work in a self-directed manner • Planning and operation Preferred is someone who can communicate and collaborate smoothly with a variety of positions, including development.• 3 to 4 years of related experience • Experience in visualizing various BI such as Tableau, Google Data studio, Google Sheets, etc. • Intermediate level or higher in SQL • Experience in deriving insights through data and applying improvements• Those with the ability to utilize SQL of an enterprise-level RDBMS • Those with the ability to utilize PySpark or Spark SQL • Those with program development (Python Java, etc.) skills • Those with experience in data preprocessing for structured/unstructured data Person • Experienced in processing large amounts of data• Those who have earned a master's degree or higher in science or engineering: Those who are expected to graduate can also apply, but they can apply for transfer to the Military Manpower Administration after obtaining a diploma. Since the Military Manpower Administration limits the number of transferees on a first-come, first-served basis, graduates are given priority in applying. Therefore, prospective students can apply for transfer after obtaining a diploma, but if the deadline is on a first-come, first-served basis, transfer in 2023 may not be possible. However, in such case, please note that you can apply for transfer by receiving the 2024 T/O in December of this year. • Extract and process desired data using SQL • Competency in using Python R • Visualization experience using BI tools such as Tableau PowerBI • Knowledge of statistics and ML algorithms • Experience in writing a thesis patent report proposalA person with at least 2 years of DE experience who is capable of logical/physical design of DE Data Lake A person with experience using data collection and processing automation (ETL) A person skilled in writing basic DB queries and using Java or Python1) AI/statistics-based data analysis capabilities 2) Experience in carrying out actual projects using big data ML/DL 3) Project Managing & Leading experience - Understanding manufacturing data analysis structure and concepts - Basic capabilities in process/strategy consulting 4) Proposal writing and Communication & Presentation skills 5) Experience/Education: More than 10 years / Bachelor's degree or higher 6) Workplace: Headquarters (Seolleung) 7) Department: Applied Ai Team• Able to develop C/C++ and Python-based software and analyze data • Have experience using Python data analysis packages • Have basic knowledge of machine learning/deep learning • Have knowledge and development experience in frameworks such as PyTorch and Tensorflow • Those with experience developing computer vision, voice recognition, and natural language processing systems using deep learning models ● Required requirements • Those with a passion for keeping up with rapidly changing technologies and environments • Those who can communicate and collaborate smoothly • SW Engineering People with skills• A person with more than 5 years of product or business analysis experience • A person with experience leading a data analysis team or overall service • A person capable of providing technical feedback such as SQL (BigQuery PostgreSQL) Python • A/B testing to verify hypotheses Those with experience designing, analyzing results, and applying them to services • Those familiar with configuring and visualizing dashboards through Redash • Those with experience analyzing data and insights and presenting clear and actionable plans• At least 5 years of relevant experience or equivalent work skills • Experience in designing, building, and operating data pipelines using Spark Hive, Presto, Elastic Stack, etc. • Experience programming using one or more of Python, Scala, and Java • Intermediate or higher SQL writing skills• Experience: 5+ years of experience • Able to build a large data pipeline service environment, monitor and respond to failures • Able to understand and utilize technologies such as Hadoop Hive impala Spark Jupyterhub- Those with experience in operating and planning data analysis solutions - Those with experience developing data analysis models using SQL, Python, R, etc. - Those with more than 5 years of related experience - (Required) Submission of portfolio, Git Blog, etc.• A person who understands the AI ​​learning data collection and production process • A person with excellent negotiation and negotiation skills related to AI learning data construction • A person who can use Python for data management - File system management (worked at os shut and il Google) Modules such as Colab) - Structured data management (modules such as csv json xml Pandas) - Calling and utilizing Git API Improve project efficiency by utilizing various frameworks • Those who sympathize with the importance of data quality management • Data collection/processing methods/data production A person who can set up a status monitoring method/data quality verification method • A person who has writing skills to provide clear work instructions based on logical thinking• Those who approach work with a proactive and responsible attitude • Those who are comfortable with a work style that tries new things and responds flexibly to changes• More than 7 years of data science-related practical experience or equivalent capabilities • Ability to utilize large-scale data analysis and machine learning-related languages ​​(SQL/HQL, Python R, etc.) • Ability to define and solve problems through large-scale data analysis • Related algorithms Experience in analysis and implementation, including writing a thesis on the subject • Preferred to have a business technical understanding of blockchain • Preferred to have practical experience related to Complex Network Modeling• Experience: More than 2 years of relevant experience • SQL Linux (bash) Docker user[We need someone like this!] - People with basic knowledge of image data - People with experience in machine learning/deep learning projects using Python - People with experience using OpenCV Numpy Tensorflow and Pytorch - Experience using CNN and transformer - Those who are interested in the latest technology - Those who proactively create work and lead responsibly - Those who can define problems and find solutions together with other job groups [This is a technology stack essential for work] - Python - OpenCV Numpy - Pytorch or Tensorflow• A person with more than 7 years of data analysis experience • A person with experience leading a data analysis team or overall service • A person capable of providing technical feedback such as SQL, Excel, Python R, etc. • A person who sets key service indicators and develops various hypotheses for indicator growth Those who have directly improved services by creating and experimenting • Those with experience designing A/B tests to verify hypotheses, analyzing the results and applying them to products • Those who are capable of causal inference and data analysis based on statistics • Multi-device (mobile / • Those with experience in integrating and analyzing data from apps • Those with experience designing and managing data logs using GA Amplitude, etc. • Those with a high understanding of product data analysis methods such as funnel analysis, retention analysis, and cohort analysis- Expected to be in charge of financial investment or fintech industry. More than 5 years of combined work-related experience - Interested in AI technology, including natural language processing - Experience in thinking about stock investment and data-based investment decisions - Communicating with various job groups and Experience solving problems - Business English level communication skills - Understanding country-specific differences between IFRS and GAAPThis is an essential competency. • More than 4 years of data engineering work experience • Data engineering capabilities that deeply understand the domain and reflect the characteristics of the domain • Ability to understand the characteristics of artificial intelligence learning data and construct an effective artificial intelligence data pipeline • One or more programming languages • Ability to design and operate infrastructure in a public cloud environment • Ability to achieve business goals through appropriate technology • Communication ability to collaborate smoothly with various team members• More than 5 to 10 years of experience working as an endoscopy nurse.ㆍEducation: Any academic background ㆍExperience: 3 to 8 years of experience ㆍDocument screening ＞ 20-minute practical test ＞ Practical interview ＞ Executive interview ＞ Final acceptance ㆍInterview schedule will be notified individually.* College graduate or higher * 3 or more years of experience * English proficient• College graduate or higher • Data Scientist experience of more than 10 years • Model development or project experience using Python / R, etc. • High understanding of the analysis process, such as data collection preprocessing, analysis, visualization automation, etc.• Those skilled in real-time data processing using Flink Kafka, etc. • Those with experience building data pipelines and data engineering in a production environment • Those with more than 5 years of above-mentioned practical experience or equivalent capabilitiesWho do you need? The most important thing is the ability to collaborate! Our crew has built a culture of respect for each other while transparently sharing work details, so the ability to collaborate based on respect and trust is the most important. Less important, but quantitative qualifications are as follows: - Those who are capable of data collection/processing/loading (Python / Node.js) - Those who have experience developing and operating data pipelines (Airflow) - Those who have experience designing data marts / warehouses - MongoDB S3 - Jupyter / VSCode / Databricks• Understanding of MS-SQL • MS-SQL performance optimization modeling SP development query tuning ability • Good communication skills• Experience: More than 10 years • Those with MariaDB/MySQL operation capabilities • Those with knowledge of overall MariaDB/MySQL functions • Those with experience in MariaDB/MySQL performance tuning and SQL tuning • Active communication skills to solve problems • Those with passion and interest in new technologies1. Qualifications • Experience in SPA development using react (2-5 years) • Proficient in using react recoil typescript javascript emotion (styled-components) • Experience developing personal projects or services using electron • Web application service Deployment and operation experience • Ability to skillfully handle HTML CSS tailwind • Experience in web development using REST API • A person with a general understanding of code configuration management using git 2. Key skills: electron nextjs react typescript javascript 3 .Work tools: docker bitbucket jira notion jenkins figma storybook aws registry verdaccio• Fresher or Experienced • C / C++• 7+ years of data engineer-related practical experience or equivalent capabilities • Experience in designing, building, and operating data pipelines • Work experience processing large amounts of data using distributed processing systems • Practical experience in on-chain data engineering preferred- Those with knowledge of building artificial neural networks based on image data - Those with experience in data analysis and deep learning algorithm research using Python - Those with the ability to implement learning layers and fine tune using deep learning frameworks such as Pytorch Keras Tensorflow - Experience in 2D or 3D image segmentation research using deep learning• More than 5 years of financial services experience • Experience in business/service planning, development, launch, and partnerships by seeking data-based business opportunities • Ability to consider and resolve various constraints in service implementation (legal and technical constraints, etc.) • A person who has insight into a specific financial area and is able to find pain points and present and implement ideas to improve them • A person who is good at communicating smoothly with various job groups to solve problems• Those familiar with computer engineering knowledge • Those with experience in distributed processing systems such as Hadoop Spark • Those with experience in cloud services (AWS GCP Azure…) • Those with experience in statistical analysis • SQL Python JVM Those who are familiar with the environment- New employee/experienced (related work) with less than 3 years of experience - Basic knowledge of computer engineering/statistics/data analysis - Experienced in Java Script development• Experience using Photoshop • Good communication skills- Experience in establishing a data governance system - Experience in performing metadata management and quality management• Architecture or design • Data management (Excel)• Experience: 5 to 10 years of experience (manager level)• Have at least 1 year of relevant work experience • Experience using GA (Google Analytics) and GTM (Google Tag Manager) required• More than 6 years of experience in data analysis • Experience leading a team • Highly skilled in data processing using SQL • Experience in creating reports and dashboards, such as Tableau Google Data Studio • Proficient in Google Sheets, MS Excel, etc. • Experience writing data analysis-related code such as Python R- At least 8 years of relevant bachelor's experience - Ability to build a Linux development/operation environment - Proficient in Java SpringBoot mybatis SQL development - Able to interpret/analyze English technical documents - A person who performs his/her work with a sense of responsibility[We need someone like this!] - People with basic knowledge of image data - People with experience in machine learning/deep learning projects using Python - People with experience using OpenCV Numpy Tensorflow and Pytorch - Experience using CNN and transformer - Those who are interested in the latest technology - Those who proactively create work and lead responsibly - Those who can define problems and find solutions together with other job groups [This is a technology stack essential for work] - Python - OpenCV Numpy - Pytorch or TensorflowㆍEducation: College graduate ㆍExperience: 5 years or more ㆍAbility to use Python Linux ㆍAbility to use Docker Kubernetes ㆍA person who is not disqualified from traveling abroad- Those with more than 2-5 years of experience - Those with proficient programming skills in Python or Java - Those with basic knowledge of large-scale data design and development experience related to distributed processing technology - Take the lead in finding and solving problems and recording them and who shares• Experience of less than 10 years • Majored in a related field such as computer engineering/statistics/business administration/management informatics/industrial engineering • Able to perform a series of data processing processes such as data collection, extraction, conversion, loading, etc. • Various manipulation of required data by writing SQL ( A person capable of preprocessing data, building an analysis mart, etc.)• Master's or doctorate degree in a related field such as computer vision, image processing, deep learning, etc. • Practical experience in image and video processing (more than 2 years) • Experience building models using deep learning frameworks such as Tensorflow, Keras, Pytorch, etc. • Project and product management and leadership skills • Communication and collaboration skills with other teams- Experience requirements: 3+ years of big data processing development - Python (Flask Numpy/Pandas, etc.) development experience - Big data-related development experience (Hadoop MR Hive Spark NoSQL, etc.) - High-performance, large-capacity processing experience• C++ development experience • Qt development experience or quick learning ability • Passionate about software design and improvement • Age/gender/education/major irrelevant[We are looking for people like this] • 3+ years of experience in chatbot data design/construction • PM/PL experience in chatbot development/construction required (chatbot conversation design/construction, chatbot knowledge resource construction, chatbot data analysis/design/construction, etc.) • AI A person with general knowledge and experience of basic technologies and the latest trends • A person who can communicate smoothly between clients and teams• Experienced or new employee in cardiology or angiography imaging • Graduated from a college or higher - majoring in radiology (department) • Holder of a radiologist's license • Ability to use a computer to the extent of being able to work with labeling tools provided by the company1. 1 to 5 years of experience 2. Java development experience (required) 3. Familiarity with Linux environment• Computer skills sufficient to work with labeling tools provided by the company • Those who are able to travel to Daejeon • Those who are not disqualified from traveling abroad[Required Requirements/Experience] • Bachelor's degree or more than 3 years of related experience • Proficient in Java SpringBoot mybatis or JPA SQL development • Ability to build a Linux development/operation environment • A person who performs his/her work with a sense of responsibility[We need someone like this!] - People with experience in machine learning/deep learning projects (Prediction Anomaly detection Classification) using Python - People with experience using Tensorflow or Pytorch - Data purification preprocessing analysis using packages such as Pandas Matplotlib Those with modeling and visualization experience - Those who are interested in the latest technology (LLM, etc.) - Those who take the initiative to create work and lead responsibly - Those who can define problems and find solutions together with other job groups [Essential for work] This is the technology stack] - Tensorflow - Pytorchpreferred• Experience in time series analysis • Experience in configuring ETL pipelines • Knowledge of mathematical statistics at major level• Those with experience building large-scale backend systems • Those with experience building backend systems using real-time data • Those with experience operating various types of databases • Those with experience developing GraphQL APIs• Experience in using Oracle • Experience in Spark development • Experience in front-end development • Experience in using Linux • Experience in using Git Jenkins • Experience in processing large amounts of data using ETL tools • Experience in using DockerㆍBasic knowledge of Windows and Linux preferred ㆍPython development experience preferred• Experience in Hadoop spark Kafka development preferred • Experience in Lucene Elasticsearch development preferred• Those who are skilled in collaborating and communicating with other teams • Those who are passionate about voluntary learning in their field • Those with experience developing services that require high reliability and security • Those with development experience using Web3 or blockchain RPC • Person with an understanding of cloud services or virtualization containers such as Docker • Submitter of portfolio Github profile or personal blog• Experience using Python-based ROS • Experience processing big data • Experience designing data management platforms• More than 1 year of development experience in a related field preferred • Master's degree preferred • Understanding and development experience in DBMS preferred • Related major (computer engineering, computer science, etc.) and information processing engineer certification preferred• Preference given to those with prior government curriculum experience • NCS registered training instructor • Preference given to those capable of teaching MSA Hadoop MLOps• Preference given to those with prior government curriculum experience • NCS registered training instructor • Preference given to those capable of teaching MSA Hadoop MLOps• Those who have a passion for selecting the appropriate technology needed for a service and wanting to quickly develop and improve it • Those who have experience building tests and continuously improving structures • Those who are very interested in their own growth- Experience with API development and maintenance - Experience with self-directed development (library and open source contribution experience) - Experience with server-side (SSR) development or basic understanding of backend - Experience with Agile Scrum - Those with experience in UI development considering responsive design, web accessibility, and web standards - Those with interest or experience in data engineering, such as building data pipelines - Those with experience in data flow definition, collection, analysis, and visualization- Master's degree in a data science-related major or at least 1 year of data analyst/data scientist experience - Experience in a public cloud environment (GCP AWS Azure)- Experience in data analysis in the securities and financial sector - Data analysis utilization skills (advanced) - Experience in analytical modeling using a big data analysis platform - Experience in data visualization using a visualization tool (Tableau)Preferences: Experience with data visualization using BI tools such as Tableau. Experience with code management using Git and experience building a data warehouse using Airflow. Experience with commerce logistics or log analysis projects. Computer engineering, statistics, industrial engineering. Those with a related degree, etc.• Those with experience in data analysis solution or platform construction projects • Those with experience in GNN (Graph Neural Network) • Those with experience in the entire data processing/analysis/modeling process • Those with experience in natural language processing projects • Project portfolio Git Anyone who can attach a blog- Experience in data analysis in securities and financial sectors - Ability to utilize languages ​​such as Python and SQL - Ability to design DB logical models and develop data marts - Experience in operating and building/upgrading company-wide BI-Platform• Those with experience participating in projects to build data analysis solutions or platforms • Those with experience in online/offline logistics distribution data-based analysis projects • Those with freedom of numerical and data-based communication and experience in configuring and operating business dashboards • Those who understand the value of data A person who understands and can bring about real change in services and users- Those who have completed a major in data analysis - Those who have experience using data visualization tools (Tableau Redash) - Those who have work experience and business understanding in related industries such as IT platform distribution/logistics commerce - Those who have practical experience related to business operation/management Those with more than 2 years of experience• Those with experience handling large amounts of data • Those with interest in constructing scalable data architecture • Those with experience building datasets for ML learning • Those with experience with advertising domain data • Those with a high understanding of mobile games▷ If you also have these capabilities, it’s the icing on the cake! • Experience using it for actual analysis based on a high understanding of big data processing • High understanding of NoSQL and related experience (ES MongoDB) • Understanding and analysis experience of open source intelligence (OSINT) • Major dark web and deep web related experiences Understanding of incidents/accidents • Experience in analyzing various unstructured data (network analysis, text analysis, etc.) • Experience in winning data analysis contests ▷ The joining process goes like this. ① Upon receipt of application, field leaders/executive staff and HR will review the submitted information together. We are working hard to provide feedback as quickly as possible! (It takes up to 1 week) ② Pre-assignment + job interview You will perform a pre-assignment before the job interview, and have a job interview for approximately 1 hour with the leaders and practitioners of the team you applied for based on the pre-assignment performance details and application form. Proceed. ※ The pre-assignment period is 1 to 2 weeks, and there is a possibility of winning or losing. ③ Culture fit interview This is a culture fit interview conducted with the HR leader. We have honest conversations about the vision and values ​​pursued by S2W and the applicant. ④ Treatment consultation and joining We will coordinate your treatment consultation and joining date through a formal offer email, and once all processes are completed, you will join our team.Experience in data utilization services and business planning【This kind of person is better!】 • A person with experience processing large amounts of data or building a data pipeline • A person who can facilitate collaboration and communication with related job groups such as developers/marketers • A person who can improve and automate existing processes People with experience in increasing work efficiency • People who send answers to PaytaLab’s preliminary questions when applying• It is good if you have a data-related major (statistics/computer engineering/industrial engineering, etc.) or have completed data-related education. • It is recommended if you have an understanding of fashion e-commerce platform services and advertising platforms. • It would be good if you have experience in other fields such as service planning, marketing, and development. • It would be great if you can communicate smoothly using data and share data-based thinking methods throughout the company. • It would be good if you have knowledge or experience building a data warehouse. [Recruitment procedure and submitted documents] • Recruitment procedure: Document screening - Telephone interview - Pre-assignment - Job interview - Final interview - Final acceptance • Required documents to be submitted: Free format resume and career description (Please avoid hwp files.) • Optional Documents to be submitted: If you have a portfolio self-introduction, please submit it additionally :) • Please do not include salary information in the documents. We plan to make a separate request later. [Notes on joining] • All positions except interns undergo a 3-month probationary period. • Salary and benefits are paid without discrimination during the probationary period. • If false information is found in your application documents, your employment may be cancelled.• Those with an understanding of the online platform business and the IT industry • Those with basic financial accounting knowledge or experience • Those with smooth interpersonal and communication skills• Experience in handling data extraction requests and analysis while communicating directly with business units • Experience in analyzing data using BigQuery on Google Cloud Platform (GCP) • Experience using generative artificial intelligence chatbots such as ChatGPT or Bard Experience with data analysis or processing or prompt engineering, that is, experience guiding generative artificial intelligence (Generative AI) solutions to produce desired results • Ability to use BI tools (e.g. Looker Studio Tableau Power BI) • Those with experience and knowledge in agriculture and rural economy or related fields • Those with experience and knowledge in statistical analysis of A/B test results• Those with experience using a GCP-based data environment • Those with experience in data management and policy establishment • Those with experience in data analysis, such as User-Segment Cohort analysis • Those with experience in data visualization and dashboard construction, such as Tableau / Looker / quicksight People • People who have little resistance to learning and applying new technologies• Big data solution experience preferred.• Those majoring in statistics, data science, financial accounting, and urban engineering • Those who can use Python, SQL, GIS, MS Excel • Those with experience planning web/app services • Those with an understanding of key concepts related to financial investment• Those with intermediate or higher level knowledge of statistics and mathematics • Those with work experience in the media industry and content business• Those with experience in B2B SaaS service analysis • Those with experience in making substantive changes to services or work processes with colleagues in various fields • Those with knowledge or experience in building a Data Warehouse• A person who is analytical, strong in detail, and has a business sense • A person with the ability to visualize analysis results using various tools so that other teams can easily understand • A person who can quickly learn and solve even unknown problems • Product Analysis Business Analysis A person with experience in creating impact in product function strategies, etc. • A person with experience diagnosing and solving problems directly to maximize business impact • Service by setting key service indicators and creating and experimenting with various hypotheses to grow the indicators A person with direct experience in improving• Those who are interested in the music industry and music-related data and have practical analysis experience • Those from the music industry (music service companies, entertainment companies, etc.) are preferred- Those who have extensive experience with Bigquery - Those who are interested in mobility services - Those who have a statistical understanding of A/B testing and causal inference• Those with intermediate or higher statistical analysis skills (regression/causality/survival analysis, etc.) • Those with experience in data analysis and ML projects for services currently in operation • Those with a good understanding of Nexon games• Those with experience in cloud platforms such as GCP AWS • Those with experience in data visualization tools such as Tableau Data Studio • Those with experience in operating Google Analytics GTM projects • Those with experience in designing, building and operating real-time data pipelines• Experience in DW/DM development and work in the gaming domain • Interest in the latest technological trends • Not afraid of change to solve problems • Knowledge of gaming and gaming market trends• Those with experience in MongoDB • Those with experience in NestJS • Those with experience in using react-query • Those with experience in using AWS[Required Competencies] - Top conference presenter experience related to data mining or journal paper author preferred - Experience in S/W development and service analysis related to big data in the manufacturing/service/logistics field preferred [Language Competency] - OPIc IM2 or higher or equivalent• Those with experience performing modeling/engineering work in the commerce domain • Those with experience applying and operating ML models to actual services • Those with development experience in cloud environments such as AWS• A person who can turn unstructured data into meaningful information using a variety of technologies • A person who can efficiently analyze large amounts of data using tools such as Airflow or DBT • Create valuable information that people can use through code People with experience in creating products • Please refer to those who have directly participated in the product creation process and have a good understanding of how products are created • In case of full-time employment, there is a 3-month probationary period • Depending on the evaluation results, the probationary period may be extended or Recruitment may be canceled. Join like this 1. Document screening 2. Video interview 3. Job interview 4. Culture fit interview 5. Final acceptance• Those with experience contributing to the spread of a company-wide data utilization culture • Those with experience considering and applying various analysis methodologies • Those with proficiency in using Git Airflow • Those with experience in user behavior log design and log data QA • Experience with BI tools such as Tableau • Those who have experience using analysis tools such as BigQuery, Google Analytics Amplitude, and Google Tag Manager • Those who majored in statistics, mathematics, computer engineering, industrial engineering, and economics• Experience and interest in the field of anomaly detection • Experience developing and improving services directly through analysis and models • Expert knowledge or equivalent experience in related fields such as statistics/machine learning• Experience using AWS (DynamoDB EC2 RDS) GCP (BigQuery) Google Analytics• Those with experience in data analysis at a commerce online content education platform company • Those with experience improving actual services through A/B testing • Those with a high level of understanding of the entire pipeline related to DB • Data such as Tableau Looker Power BI Anyone who has used visualization tools• Those with experience in mobile/web log design and analysis • Those with smooth internal and external collaboration and communication • Those with interest and passion in solving management problems in restaurant stores • Those with knowledge or experience in building a data warehouse• Data-based thinking ability • Ability to accurately understand basic statistical knowledge • Experience with data analysis in the e-commerce industry • Experience using visualization tools such as SUPERSET REDASH • Experience with A/B test operation and design analysis • Those who have experience with data mart creation tools such as DBT • Those who are familiar with the on-premise environment • Those who have experience applying ML models to business • Those who have knowledge and experience of data analysis methods such as Funnel AARRR Cohort Analysis• Understanding of advertising business • Experience in operating Python-based services • Experience in configuring and operating AWS infrastructure • Holder of cloud-related certifications• Experienced in pharmaceutical industry- Those who sympathize with the necessity and importance of data processing - Those with excellent problem-solving skills - Those who want to experience the process of connecting their knowledge and skills to actual services rather than just research - Statistical data analysis computer engineering Those who majored in the field or have more than 2 years of data analysis experience - Those who work in the veterinary domain or are loving companion cats - Those who love nature• A master's/doctoral degree in a related field such as machine learning/data mining/statistics/mathematics, or someone who has performed related work for more than 3 years • A person with direct or indirect experience in building a large-capacity real-time data processing architecture • Optimizing machine learning/deep learning model serving and distribution experience • Experience performing analysis tasks using deep learning or machine learning or winning contest prizes • Experience developing and operating large-capacity traffic services- Those who majored in related fields such as big data statistics, applied mathematics, and computer science - Those who have experience providing insights and creating business growth through user behavior data analysis - Those who can manage data efficiently by automating monitoring - Those with experience in data visualization (Tableau, etc.) - Those with experience in modeling through ML- Experience in performing work related to securities and financial sectors• Those interested in O2O service marketplace commerce services • Those with deep statistical understanding of A/B testing and causal inference • Those with extensive experience handling Bigquery • Understand the vision and value of Carrot Market and the direction of the service Please note that anyone who may be concerned • In the case of full-time employment, there is a 3-month probationary period • Depending on the evaluation results, the probationary period may be extended or employment may be canceled. This is how you join 1. Document screening 2. Video interview 3. Job duties Interview 4. Culture fit interview 5. Final acceptance• Those who have experience reading papers and applying the content to actual work • Those who have experience building real-time recommendation systems and applying them to services • Those who are interested in the latest technology and are interested in acquiring new knowledge- Those who have experience extracting and purifying data directly using SQL - Those who have the ability to use Python SQL BI Tools (Superset Data Studio (Looker Studio), Tableau, etc.) - Define problems based on logical thinking, create hypotheses, and A/ Those who have experience designing and conducting B tests - Those who prefer and are skilled in quick thinking and strategic decision-making - Those who enjoy identifying problems, solving problems, and enjoying rapid growth - Working with mobile game data Anyone with analysis experience - Anyone interested in improving the infrastructure and environment to create a data-based organization * If you have experience in deeply understanding service users through data analysis and presenting impactful actions through this, please attach it to your portfolio!• Those with experience in deriving insights using BI tools • Graduates of related majors such as statistics or computer engineering • Those with experience in the entire data analysis process from log-level data processing to analysis/modeling • Those who work independently • New A person with a strong spirit of challenge in technology or work • A person who can present opinions derived through analysis logically and systematically • A person who can communicate with practitioners and management in various related departments • A person who considers any task important and takes responsibility Anyone who can proceed•Experience with cloud-based data platforms such as AWS Azure GCP Snowflake Databricks •Experience developing data analysis/visualization tools using Tableau Power BI Grafana Streamlit, etc. •Experience with Git/Github for version management and collaboration •Docker/for software containerization and orchestration Docker Compose experience • Experience with medical/healthcare data • Paper publication records or academic achievements in data/AI-related fields• Those with experience designing company-wide common indicators • Those with experience in contribution analysis according to user behavior journey • Those with basic knowledge of data engineering • Those with deep understanding of e-commerce and experience analyzing related data • Quickly Experience working in a growing business environment• Those who have experience in data analysis or AI model distributed learning on PB or TB level data • Those who have practical experience in AI using ML DL • Those who have practical experience in data engineering- Those who sympathize with the necessity and importance of data processing - Those with excellent problem-solving skills - Those who want to experience the process of connecting their knowledge and skills to actual services rather than just research - Statistical data analysis computer engineering - Those who major in the field of veterinary medicine or those who love companion cats - Those who love nature- Those who have an understanding of e-commerce business marketing advertising performance measurement - Those who can program for data processing and analysis (Python SQL, etc.)• Those with experience in data preprocessing and visualization • Those with experience in data processing or data platform configuration using Apache Spark • Those with experience in developing tools (web applications, etc.) for data management or utilization Used by Finda Data Platform Team Technologies being used • Python Scala SQL • Apache Spark Delta Lake Airflow Kafka FastAPI • Git ArgoCD Grafana Recruitment journey • Document screening ＞ 1st interview ＞ 2nd interview ＞ 3rd interview ＞ Reference check ＞ Treatment consultation ＞ Final acceptance • Applicant experience and recruitment Some procedures may change depending on the situation.• Those with experience using GCP-based data environments • Those with experience in data management and policy establishment • Those with experience in building and operating large-capacity data pipeline systems • Those with experience in processing Kafka Hive Spark, etc.• Preference given to those majoring in related departments (mathematics, statistics, computerized data analysis) • Preference given to those with experience developing and operating ML-related services • Preference given to those with experience in agile work environments (Scrum Sprint Squad, etc.) [Other information] ※ Selection information • Document screening - 1st interview - Online Personality test - 2nd interview - Reference check - Treatment screening/health checkup - Pass - Interview method and process may change depending on the job. - If you pass the recruitment, a 3 month probationary period will apply. (Applies to document applications starting in November 2023.) - If false information is found in the application, employment may be cancelled. - Document submission is only possible for one announcement. (Duplicate support not possible)• Experience in developing corporate credit analysis and credit risk prediction models• Those who are capable of structuring data using GIS • Those who are interested in or have experience in analyzing data related to regional commercial districts and sales • Those who have experience in data preprocessing and visualization • Those who are interested in system optimization and performance improvement • People with experience using data libraries such as Pandas • People with experience processing data using Apache Spark Technologies used by OpenUp PT data engineers • Python Pandas Streamlit • MongoDB SQL AWS DynamoDB S3 • JS GO AWS Lambda SQS FastAPI • Git GitLab CI Airflow Grafana Recruitment Journey • Document screening ＞ 1st interview ＞ 2nd interview ＞ 3rd interview ＞ Reference check ＞ Treatment consultation ＞ Final acceptance • Some procedures may change depending on the applicant's experience and recruitment status. • Industrial technical personnel support available. (New transfer to supplementary station)【This kind of person is better!】 • A person with experience processing large amounts of data or building a data pipeline • A person who can facilitate collaboration and communication with related job groups such as developers/marketers • A person who can improve and automate existing processes People with experience in increasing work efficiency • People who send answers to PaytaLab’s preliminary questions when applying- Those who have a thirst for top-level decision-making related to products - Those who want to test their hypotheses to their heart's content to solve problems - Those who have experience with Agile methodology or Lean Process - It is good if you have knowledge or experience building a Data Warehouse . - Experience with data visualization (Redash Tableau Superset, etc.) is preferred. - English proficiency at business level or higher (Verbal/Written)• Those with experience using/introducing Amplitude GTM Tableau BigQuery Airflow • Those with experience in crawl maintenance • Those with experience working in user-oriented tech companies • Those with experience improving organizational productivity through automation• Those who have performed service analysis in the commerce domain • Those who can persistently delve into data • Those who have direct experience or equivalent understanding of ML model development and related engineering • Experience in improving models and driving business outcomes through experiments A person with this- Those who can speak English/Japanese - Those who have majored in medical IT - Experience in developing/operating hospital medical information systems or CDWs• Experience in the entire cycle from user experiment design to result analysis • Experience in playing a variety of games or a similar understanding of games and game users • Experience in operating services for users or analyzing user behavior data Anyone who has• Master's degree or higher in ML-related major, such as statistics, industrial engineering, computer engineering, etc. • Experience in data analysis and ML projects in actual operating services • Experience in Kaggle algorithm competition contests, academic presentations, and publishing papers• Those with experience tuning large-capacity DB (SQL) performance • Those with experience developing big data-related services • Those with experience performing three or more big data projects• Experience in building, operating and upgrading systems that handle large amounts of traffic • Experience in building data pipelines or developing search personalization and recommendation services based on user data • Experience in developing search models/rankings • Experience in developing morphological analysis and pre-/post-processing • Search service API Development experience • Experience in large-scale data collection/indexing • Experience in designing/building/upgrading Elasticsearch Cluster • Experience in developing Elasticsearch plugin • Experience in developing personalization of search • Experience in developing natural language processing using HMM CRF model, etc.• Experienced in developing next-generation account system (including receipt) • Settlement (billing/deposit) expert• Statistics and programming-related degrees preferred (computational mathematics, etc.) • Experience in processing and analyzing game-related data • Experience in collecting and linking external data (Google Play, Apple Appstore, Appsflyer, etc.) • Ability to analyze data required for business analysis and decision-making• Those with experience in the Hadoop eco-system (Hadoop Spark Hive, etc.) • Those with experience processing large amounts of data in a distributed environment • Those with experience in backend data processing and management • Those with experience analyzing real-time data transmitted from Message Queue [Recruitment Process] • Document screening ＞Practical competency interview ＞ Final interview ＞ Treatment consultation ＞ Confirmation of employment • During the process, the order of each screening may be changed or omitted and a reference check may be performed. • Contract type: Full-time employee (3 months trial period)• Majored in computer engineering-related department • Full stack developer • Capable of configuring Spring Security • Capable of MSA design, development, construction and operation • Proficient in DB Schema design SQL and Index Tuning • Capable of interpreting/analyzing English technical documents • Experience in developing open source projects • Developing and launching solutions Experience • Proficient in developing/operating cloud and container environments such as AWS Azure N Cloud • Active in taking on new challenges• Ability and experience in data analysis through the use of tracking tools (GA/amplitute) • Experience in using collaboration tools such as jira confluence • Experience in planning and operating services in the travel domain• Those who are interested in the field of education • Those with practical experience in the data science field • Those with a sense of design or video planning • Those who can confidently approach filming • Those who have taken Code It • Those who speak English at a native level • Those who can confidently create educational content on the following topics • Data analysis, data science, machine learning, deep learning, computer vision, natural language processing, mathematics, statistics, etc. • Python, SQL, Excel, Tableau, PyTorch, TensorFlow, Hugging Face, etc. • In addition, we plan to produce content on various topics Please feel free to apply 【 This is how you join 】 1. Document submission 2. Mini project 3. Interview 4. Reference check 5. Joining • We are working on a mini project when the documents pass. • The mini project is a process where the applicant and the Code It team can match each other. We will provide you with tasks to verify your content creation capabilities, and we will also work with you in the improvement process based on feedback on the tasks. • The results will not be used for any purpose other than employment verification, and assignment fees will be paid. • Interviews related to job and culture fit are conducted, and all interviews are conducted over the course of one day. • The interview is conducted at the Code It office. • Reference checks are conducted after the interview. • Reference checks are carried out after obtaining consent from the applicant and providing detailed information in advance about the process and deadlines. Depending on the applicant, this procedure may be omitted. 【 Submission Documents 】 (Required) Resume (Required) Please tell us what motivated you to apply to Code It. (Required) Please tell us what topics you can currently create and what topics you would like to create in the future. (Optional) Portfolio that can confirm data science-related capabilities (Optional) Portfolio that can confirm competency in writing or educational content 【 Asked to a current content producer 】 Q. What is the best part of doing this job? A. I enjoy doing work that has an impact. There is no limit to the reach of online content, right? So, if you create good content, many people can continue to use it. Even when we are sleeping. I think it's great that so many people learn programming and become excellent developers through the education we created. It's also fun to read reviews that come up every day. Q. You went from working in the field to becoming a content producer. How do you think the two roles are different? A. I think I discover things I couldn't see while working in the field. In practical work, the focus is on creating, while content PD focuses on understanding and grasping the principles. In the past, there were a lot of things that were overlooked because we were so busy making things on schedule. When I looked at the content again to teach it to someone, I realized many new things. There are many times when you have to study more than ten things to teach one thing. I think the advantage of content PD is that you are given the opportunity to study properly. 【 Please note 】 • In the case of full-time employment, there is a 3-month probationary period. • Depending on the results of the probationary period evaluation, the probationary period may be extended or employment may be cancelled. • You can apply as a military service exception (former industrial technical personnel newly transferred to supplementary service).• Experience in service improvement through analysis of customer behavior data • Major in statistics, mathematics, data analysis, etc. • Ability to interpret A/B testing and statistical results • Understanding algorithms such as recommended prediction models for data analysis • Experience in data analysis projects • Experience with decision-making processes based on hypothesis testing• Experience in operating Salesforce or Tableau • Experience in processing and analyzing large amounts of data- A person with the ability to freely modify and utilize open source code - Deep understanding of Unit Test Docker Kubernetes - Deep understanding of asynchronous concurrent parallel distributed processing - Experience using Kafka Redis - Experience developing DevOps Backend* Those with a high understanding of e-commerce data * Those with experience in back-end development using Python * Those with experience working with data in the AWS cloud environment * Those with modeling experience using ML/DL algorithms * Flask FastAPI, etc. * Those with experience developing Python web frameworks * Those with experience collaborating using Slack Jira Confluence Preferred talent * Those who consider any task important and can proceed responsibly * For smooth collaboration with colleagues in various fields A person with communication skills * A person who is not afraid of new tools/knowledge and can learn them quickly * A person who thinks deeply about writing efficient code- Those with experience building data marts or designing data pipelines - Those who have proposed and actually applied methods for efficient data analysis - Those who have their own strengths/know-how in analysis work - Those who have experience expanding data organizations - Those who have a clear sense of values ​​about what a healthy product is - Those who have modeling capabilities using machine learning algorithms - Those who are interested or curious about search and curation recommendations• Ability to use data visualization tools (Tableau, etc.) • Ability to use ML frameworks (Pytorch Tensorflow, Keras, etc.) • Experience using clouds such as AWS GCP• Experienced in Data Warehouse Data Mart modeling • Experienced in data pipeline development and ETL process • Experienced in DB design, construction, and operation for large-capacity transaction processing • Respond to and resolve DB failures and performance issues• More than 5 years of relevant job experience • Experience leading a project to build a data analysis solution or platform • Experience in building a real-time data analysis pipeline• Experience building data pipelines in a snowflake environment is preferred. • Preference will be given to those who have experience building infrastructure with Terraform. • Experience operating Kubernetes-based services is desirable. • Experience with CDC (Change Data Capture) data pipelines is preferred.• ETL data lake operation and engineering experience through Hadoop / Spark, etc. • Kubernetes construction/operation experience • Work experience in cloud environment [Technology used by Hackcle] • AWS Kinesis AWS Glue AWS Lamda AWS S3 AWS EKS AWS DynamoDB etc • Kubernetes • Apache Spark • Presto/Trino • Terraform • Prometheus • Elastic Search [Recruitment Procedure] Document screening - 1st interview - 2nd interview - Treatment consultation - Final acceptance • Only those who pass the document screening will be held in the 1st round (telephone interview) . • If there are false information in your application, your application may be cancelled. • If you are accepted to this position, a 3 month probationary period will apply.• Those with experience configuring and applying AWS network and security policies • Those with experience operating ML Ops • Those with experience in discovering needs for data marts and building pipelines and marts to resolve them • Targeting customers • Those with experience in operating data products and platforms • Those who are familiar with SQL Python and can help members learn • Those with experience building Spark (PySpark)-based data pipelines [Need to join the Lemonbase Data team] Reason] • You can experience the process of collaborating with various domains and presenting data-based solutions. - Experience in solving problems that require data with all teams that make up Lemon Base, including the team that helps customers succeed, the team that is in charge of product sales, the team that accumulates knowledge of overall performance management, and the team that monitors the team members who create the product. You can. - So-called traditional data engineering, which designs/builds and operates data pipelines and marts, as well as a problem-solving process and solutions to problems that structure problems experienced by customers and Lemon Base members to find the true cause, not the reason that appears on the surface. As part of this, you can gain experience using data engineering. - In addition to technical growth, you can also build domain knowledge that encompasses all human resources (HR) in your organization through collaboration with the Business and People Science teams. • You can handle personnel data that must be strictly managed in a security manner. - Most of the customer data that Lemon Base handles is often clear personal information or data equivalent to personal information. You can experience handling data while considering security and safety in all data handling processes. - De-identification measures can be designed and applied depending on whether or not personal information is included. - In addition, you can find out who checked what data and when through data access rights management and audit logs. • You can experience new attempts and challenges. - Instead of working according to the established stack and working method, you can experience directly researching, introducing, and disseminating new things. For example, to ensure data usability for our members, we are reviewing and introducing Databricks, which is somewhat unfamiliar in Korea and which none of our members have experience with, to manage the cluster authority pipeline. Additionally, we believe that Databricks' real-time data collection system is not suitable and are pursuing new challenges and attempts rather than based on existing frameworks and experiences, such as building a Kafka-based infrastructure. - We have a culture of boldly using and sharing anything that is new to us as long as there is a purpose and reason. • Experience combining multiple tools in different cycles for different purposes. - We utilize and link tools to improve business and customer experience such as Pipedrive / Channel Talk, tools to understand customer behavior such as Hotjar / Amplitude, and tools to collect and process data such as Databricks / Kafka (MSK). . - We are performing data work in various cycles, including real-time, daily, weekly, and monthly batch work. [This is what the Data Team is like] Lemon Base's Data Team is an organization in charge of research and development activities that discover insights and develop products that create higher value based on the various data held by Lemon Base. We are continuing to put a lot of effort and thought into creating higher-level value by refining and combining various data such as business marketing service user behavior logs. In particular, objective and highly acceptable performance management and organizational and We are creating research and data products that help our customers so that our members can grow healthily together. This is a team that challenges all data-related tasks within Lemonbase, including business and marketing indicators and dashboards that measure Lemonbase's growth and enable effective decision-making. Lastly, we build an efficient data experiment/analysis environment. [What kind of colleagues will you join?] People who have experience analyzing people in various organizations and have designed and developed national projects as machine learning engineers in the R&D department have come together to form Lemon Base's Data Team. We are working to explore issues about ‘people’ and ‘growth’ that we have been concerned about through various experiences and to present data-based solutions and products.• It is better if you have experience making decisions based on data analysis. • Experience with data-related programming, such as Python or R, is preferred. • It is better if you have experience using data visualization-related services or BI (Business Intelligence) tools. • It is better if you have a master's or doctoral degree in statistics or financial engineering.• Those with extensive experience in handling Bigquery • Those with knowledge or experience building a data warehouse • Those with experience in data projects related to commerce logistics preferred• Those with experience operating real-time data tools such as Spark Streaming Flink • Those with experience leading data-related departments [Be sure to check!] *Detailed procedures will be provided individually to those who pass the document. *The process may change depending on the schedule and circumstances after prior notice, and the results of each selection will be notified individually. [Other Matters] • Those eligible for employment protection, such as people of national merit or the disabled, are given preferential treatment in accordance with relevant laws and regulations. • There must be no disqualifications for overseas travel, and men must have completed military service or been exempted. • If any of the information in the application is found to be false, employment may be cancelled.• Experience in establishing new business strategies, KPI setting management and organization management • Experience in using PA MMP tools such as Amplitude Braze Appsflyer • Experience in designing and operating GTM events • Experience in cultivating data literacy of company members [Recruitment procedure] - Document screening -> 1st working interview -> 2nd interview with related department -> 3rd interview with management * Selection stages may be reduced or added during the selection process if necessary.- Experience in building and operating data workflows (Airflow) - Experience in developing distributed processing technologies such as Hadoop MR Hive Spark - Experience in operating cloud data services (RDS/Athena/Redshift, etc.) - Experienced in SQL (MySQL/PostgreSQL) ) - Those with experience in systems based on Microservices Architecture - Those with experience building data infrastructure in container environments such as ECS Kubernates- Breakfast provided / Self-development fee paid / Points provided for coffee drinks purchased at the in-house cafe - Leaving work at 3 p.m. twice a month on Fridays - Birthday gift card providedThose with experience working in the e-commerce or internet service industry Those with experience using other programming languages ​​such as Python R• A person who can turn unstructured data into meaningful information for team members using a variety of technologies • A person who can explain the information obtained through data in a form that anyone can understand • Take the initiative to identify issues or improvements and gain new knowledge A person who learns quickly and is passionate about solving problems • A person who can share and discuss ideas with team members and actively communicate with related departments• Those with experience in public cloud environments • Those with experience in task automation using development languages ​​(Bash shell / Perl / Python, etc.) • Those with experience in DBMS other than MySQL, such as ORALCE / PostgreSQL / Redis / MongoDB• Those with experience developing or managing DB and data pipelines • Those with knowledge and experience of Oracle MySQL • Those with Front-End development experience • Those with experience developing NLP projects using Transformer (BERT GPT, etc. )• Those who have experience improving user discovery experiences using related data, such as search feeds • Those who have experience making decisions that consider both user experience and business • Can understand the vision and value of Carrot Market and consider the direction of the service Someone who has* Have experience managing workflows using Airflow, etc. * Have experience developing distributed processing technologies such as Hive Spark * Have experience building data infrastructure in container environments, such as ECS Kubernates * Have experience using visualization tools using data (looker studio tableau, etc.) * Anyone with experience or understanding of MLOps * It is better if you have financial market domain knowledge or relevant industry experience- Those who have experience in raising service indicators or solving problems through data-based insights - Those who like to make detailed calculations on a daily basis - Those who can easily communicate data-based insights so that anyone can understand - Those with a data-related major (statistics/ Those who have completed data-related education (computer engineering/industrial engineering, etc.) - Those who have analysis-related qualifications, such as Data Analysis Associate Professional (ADsP) or big data analysis engineer - Those who have a high understanding of marketing indicators (first open retention conversion rate)• Proficient in Javascript jquery or capable of full-stack development • Experienced in Web Square • Experienced in DBMS Oracle • Experienced in Spring frameworkThis type of person is better - Those with experience handling financial or blockchain-related data - Those with experience using big data warehouses or frameworks such as BigQuery Snowflake Spark Trino - Those with DBT experience - Various OLTP/OLAP Those who understand databases and have experience operating them in production - Those who have experience developing and operating Airflow-based ETL pipelines - Those who have experience developing and operating real-time pipelines based on Kafka/Flink - Experience operating container orchestration systems such as Kubernetes - Those with experience in CI/CD automation - Those with experience designing and operating data infrastructure using IaC in cloud environments such as AWS GCP- Experience using Google BigQuery - Experience processing marketing data (Appsflyer Singular Adjust)• Those with development experience with Mesh or CT data • Those with development experience with MLOps or DataOps • Those with an understanding of graphics pipelines- A person with GA (Universal Analytics) GA4 experience - A person with interest in and understanding of all basic technologies used in digital marketing (Web App Machine Learning etc..) - Ability to use Programmatic data analysis language (Python R etc..) Anyone interested - Statistics/Mathematics graduate• Experience in operating an AWS-based DB • Experience in operating an online service DB for an unspecified number of people - Experience performing backup and recovery - Experience in configuring redundancy - Experience in monitoring and troubleshooting• More than 3 years of service planning experience • Experience in field service planning and development through teamwork • General understanding of development work and communication skills • High understanding of various platforms (web and mobile)- Experience handling large amounts of data through pipelines - Experience handling search engines - Experience building and operating a data lake data mart - Experience understanding and using various databases - Experience building and operating an environment using Kafka• Computer engineering major • CRM-related work experience • Smooth PPT and communication skills• Experience in setting up and using the Appsflyer GA4 Firebase tool • Experience in search result optimization (SEO) and app store optimization (ASO) • Experience in English copy writing or native-level English writing skills • Digital targeting the US, European and Southeast Asian markets Preference given to those with marketing experience and success stories • Experience in operating apps targeting overseas markets (video & image advertising viral email push, etc.) • Experience in marketing digital content related to in-app purchases, such as game videos, webtoons, e-books, etc.• Those with experience in the entire ML process from data processing analysis modeling service implementation • Those with experience in improving models and driving business performance through experiments • Those with experience developing/operating search models in the commerce domain • AWS databricks environment and Spark Anyone familiar with• Those who have experience in data analysis or AI model distributed learning on PB or TB level data • Those who have practical experience in AI using ML DL • Those who have practical experience in data engineering• Those with knowledge and practical experience in statistical analysis and ML algorithms • Those with experience in planning A/B tests, analyzing results, and presenting opinions[This type of person is even better!] - Those with experience writing papers and patents - Those with experience in image and video data projects - Those who can implement the contents of the latest papers - Those with experience writing the latest time series deep learning / machine learning algorithms - Those with a degree in computer engineering, industrial engineering, electrical or electronic engineering• Interest in and experience with platform O2O business is more important than experience and understanding of the dating industry. • Those with experience in building data pipeline infrastructure at startups • Those with experience working at startups and those looking to work there • Those with interest and experience in platform O2O business • Those who have taken the lead in creating data platforms • Analyze app data and use it as a basis Anyone with experience improving products with• Experienced in the financial sector (formerly from a securities company or asset management company) • Mathematics major/Master’s degree or higher in macroeconomics • Master’s degree in econometrics/Master’s degree or higher in finance (finance) • Graduation thesis or research results in the field of corporate finance or asset pricing▷ If you also have these capabilities, it’s the icing on the cake! • Web collection experience using Playwright Selenium • Collection experience through Tor Network • Telegram collection experience • Bot detection/blocking technology Analysis bypass technology development experience (e.g. Captcha Cloudflare) • Development experience using NoSQL DB ▷ The joining process goes like this . ① Upon receipt of application, field leaders/executive staff and HR will review the submitted information together. We are working hard to provide feedback as quickly as possible! (It takes up to 1 week) ② Pre-assignment + job interview You will perform a pre-assignment before the job interview, and have a job interview for approximately 1 hour with the leaders and practitioners of the team you applied for based on the pre-assignment performance details and application form. Proceed. ③ Culture fit interview This is a culture fit interview conducted with the HR leader. We have honest conversations about the vision and values ​​pursued by S2W and the applicant. ④ Treatment consultation and joining We will coordinate your treatment consultation and joining date through a formal offer email, and once all processes are completed, you will join our team.• Those majoring in computer engineering, etc. • Those eligible for national veterans and the disabled are given preferential treatment in accordance with relevant laws • Non-smokers • Those with experience in the same industry • Those with excellent performance in the relevant field• Preference given to those majoring in computer engineering/business management • Those with financial SI SM experience • Those with experience using SQL and scripting/Linux • Those with knowledge and experience in development languages ​​widely used in project work, such as Python and Java, are preferred • Those with SAP Douzone ERP experienceㆍExperience in building and operating cloud-based pipelines ㆍExperience in developing and operating large-scale distributed processing systems• Those with experience in the entire ML process from data processing analysis modeling service implementation • Those with experience developing/operating recommendation models in the commerce domain • Those with experience in improving models and driving business results through experiments• Master's degree or doctoral degree (doctoral program applicants) preferred • Experience in data analysis preferred• Experience with data catalog management • Experience with server development • Experience with open source operations • Experience with MLOps processes • Experience with agile processes • Experience with multi-cloud and cross-account• Master's degree or doctoral degree (doctoral program applicants) preferred • Experience in data analysis or similar work preferredㆍ Experience in programming and modeling using SAS R or Python is preferred ㆍ Experience in machine learning, data mining and statistical analysis is preferred ㆍ Those with an understanding of data science workflow and methodology are preferred ㆍ Those majoring in mathematics, statistics, computer science, machine learning or related fields are preferred Preference given to those with 3 to 5 years of analytical work experience. Preferred to those with business experience in the financial services industry.• Experienced in DBMS (Oracle MS-SQL, etc.) • Experienced in CC certification and GS certification • Experienced in security software development- Experience with weather-related data - Experience with machine learning-related modeling tools (Tensorflow, PyTorch, etc.) - Experience with data analysis (R Tableau, etc.)• Have more than 3 years of large-scale system experience • Have excellent communication skills (orally and written) • Have experience leading a team successfully- Those who can use the Python language proficiently - Those who have experience in the entire process of robot development from planning to production - Those who have experience in projects related to self-driving robots - Those who have experience in path planning and development of robot manipulators - Those who have experience in developing path planning for robot manipulators - Those who have experience in developing path planning for robot manipulators - Those who have experience in the entire process of robot development from planning to production Anyone with experience - Anyone with a basic understanding or interest in Ag-Tech (agricultural knowledge is not required!)[This kind of person is better!] • A person with an understanding of the Hadoop Eco system and relevant practical experience • A person with an understanding of data governance and catalogs and experience in introducing a data discovery platform based on this • A reliable data pipeline Anyone who has thought about• Interest in and experience with platform O2O business is more important than experience and understanding of the dating industry. • Those who are interested in services that utilize matching logic between users • Those who have experience in classifying user segments and analyzing app data based on them • Those who have experience in visualization using BI tools such as Tableau Data Studio• Experience using AWS (DynamoDB EC2 RDS) GCP (BigQuery) Google Analytics• Practical skills in MLOps (ML + DevOps)• Those with experience operating and cloning Oracle EBS 11i • Those with experience operating and managing Maxscale • Those with experience in operating other open source-based databases or building infrastructure- Those who have a good understanding of apps/web - Those who can analyze using Google analytics - Those who can communicate in foreign languages ​​- Those who have experience creating dashboards using PPT/Excel• Experience in building data pipelines • Experience in developing big data analysis systems • Experience in building semiconductor display factory automation MES and analysis systems • Experience in Open API design/development • Experience in MSA design/construction • Experience in build/deployment automation· Those with experience building data pipelines and databases in an AWS environment · Those with extensive experience in time series data analysis · Those with research experience related to analysis prediction classification algorithm models[This type of person is even better] • A person with a master's degree or higher in natural language processing or has experience participating in papers in a related field • A person with experience developing and fine-tuning services utilizing LLM (Large Language Model) • Machine translation conversation model ( People with experience developing and implementing Conversational AI) • People with understanding and experience in data preprocessing, such as data planning/design required for model learning and evaluation • People who enjoy the process of acquiring new knowledge or skills in the AI ​​field, including the NLP field • Those who have a strong drive and sense of challenge to build-up in a startup• Experience designing and operating a high-availability data platform in a rapidly growing service • Experience with and understanding of data analytics and machine learning practices • Experience building data infrastructure for container-based development environments and MSA • Clouds such as BigQuery Athena Databricks Snowflake Those who have experience using basic data pipeline solutions • Those who have experience building and operating big data platforms using various tools in the Hadoop Ecosystem, such as Spark Kafka Hive • Those who have experience in improving DB performance or tuning queries • Those who are interested in DevOps or SRE and infrastructure Those who are active in automation • Those who have experience collaborating with data analysts [Selection process] • Document screening -> 1st working interview -> 2nd collaboration department interview -> 3rd CDO interview *Selection stage may be reduced if necessary during the process or may be added.[A person like this is even better] • A person with a strong drive and sense of challenge to build up a startup • A person who enjoys the process of acquiring new knowledge or skills • A person who enjoys communication activities with customers and stakeholders • Preference given to those with related majors (Korean literature, linguistics, linguistics, cognitive science, computational language, multilingualism, computer engineering, etc.)• Those with experience working in the horizontal culture of a startup • Those with marketing analysis experience in app platform mobility, etc.1. Those with experience linking AWS services (experience building and deploying infrastructure using AWS docker) 2. Those with experience analyzing/extracting data using SQL queries 3. Those with experience diagnosing failures and analyzing causes 4. Those with experience using configuration management tools such as Git SVN• Those with experience in data analysis and modeling for fintech services • Those with an understanding of distributed processing of large amounts of data • Those with experience using cloud services such as AWS GCP • Experience in identifying problems within existing products and defining and analyzing indicators • Those with experience operating an ETL pipeline using Spark Airflow, etc. Recruitment journey • Document screening ＞ 1st interview ＞ 2nd interview ＞ 3rd interview ＞ Reference check ＞ Treatment consultation ＞ Final acceptance • Applicant’s experience and Some procedures may change depending on recruitment status. • Industrial technical personnel support available. (new transfer or transfer to active duty or former supplementary service)• Experience in developing models related to natural language processing • Experience in development and operation using AWS • Experience in applying Multi Armed Bandit • Experience in building, operating and upgrading systems that handle large traffic • Experience in the commerce domain• Experience developing data-based solutions • Experience developing visualization and monitoring solutions • React experience preferred• Those with experience building and operating k8s-based data pipelines • Those interested in Data LakeHouse development/operation • Those interested in DataOps / MLOps • Those with experience in large-scale data batch processing and those interested in data streaming processing • BigQuery Athena Those with experience using large-scale analysis engines such as Trino Hive • Those with experience using table formats using Apache Iceberg Hudi, etc. • Those with experience in real-time data synchronization using Apache Spark Kafka, etc.• Experience with DB tuning and SQL query optimization • Experience operating services in container orchestration systems such as Docker Swarm Kubernetes • Experience with data visualization using dashboards and BI tools such as Tableau Redash Grafana • Building or utilizing MLOps Those with experience • Those with an understanding of basic computer knowledge such as algorithms, data structures, OS databases, etc. • Those with web crawling experience• Those who have experience in commerce or have a general understanding • Those who can collaborate and communicate smoothly with colleagues • Those who can take initiative • Those who have a high understanding of data processing using the SQL Python language • Data visualization People with extensive experience • Master’s degree or higher in a data analysis-related department (statistics, industrial engineering, mathematics, etc.)ㆍExperience in BI systems preferred ㆍExperience in BI tools (Tableau Qlikview BI-Matrix, etc.) preferred ㆍHolders of data processing analysis modeling-related qualifications preferredIf you have this experience, we will roll out a red carpet. • Those with experience building machine learning data/service pipelines • Those with a master’s degree or higher in statistics/computer engineering • Those who have presented papers at academic conferences in related fields• We are looking for someone with understanding and experience with the AWS GCP cloud environment. • Experience developing large data ETL pipelines is preferred. • It is better if you have experience using technologies such as Spark Airflow BigQuery. • It is better if you have financial market domain knowledge or relevant industry experience.• Experience in operating B2B SaaS services • Experience in analyzing e-commerce financial investment data • Experience in open source project activities • Experience in OLAP analysis • Experience in implementing data streaming such as Kafka • Experience in time series DBs such as InfluxDB • Experience in ML DL-related projects • Experience in VC Growth hacking Experience • Able to speak EnglishI think it would be better if there was someone with this kind of experience or inclination/capability. • Experienced in data labeling/labeling work • Experienced in data inspection • Person performing work related to medical data[It would be even better if someone like this] • Those with sales development experience in the same/similar industry are preferred • Those with experience using CRM/automation tools such as LinkedIn Appollo Zapier • Those with understanding or experience in marketing branding who can facilitate smooth communication • A person with a strong drive and sense of challenge to build up a startup • A person with experience creating meaningful growth through data-driven implementation• Those with experience leading projects to build data analysis solutions or platforms • Those with experience in building real-time data analysis pipelines[It is better if you have this kind of experience] • Experience in Data Warehouse environment Experience in designing and building Data Mart • Experience in solving data-related problems • Person who can design the right architecture to fit the problem you are trying to solve • Good communication with colleagues and Anyone with experience collaborating through feedback [Technology stack] • AWS (EKS Glue Redshift EMR Athena S3 RDS) • Spark Kafka Hadoop Ecosystem Ceph • Kubernetes Trerraform ArgoCD Github Actions Istio • FastAPI MySQL Mongodb MinIO Airflow Jenkins • Collaboration tool: Slack Notion Github Google Docs [I want to work with people who have this type of personality] • I prefer people who have a strong desire for excellence. We want to work with people who set high goals and persistently dig in to solve problems, and who grow faster than anyone else in the process. • We prefer people who can understand multiple perspectives and contexts. We want to work with someone who can define the goals and problems they want to solve from an organizational team service perspective. • We prefer people who can communicate well while respecting the other person and focusing on the essence. [Working conditions are as follows] • Work type: Full-time employee (promotion period: 3 months / 100% salary paid) • Work location: 6th floor, Deokmyeong Building, 625 Teheran-ro, Gangnam-gu, Seoul (3-minute walk from Samsung Station) • Work days/hours: Weeks 5 days (Mon-Fri) / 40 hours a week (8:00 - 11:00 am) • Treatment and compensation: Negotiated after interview based on individual capabilities and experience [Recruitment process] 1. Recruitment process: Document screening ＞ 1 1st phone interview ＞ 2nd working interview ＞ Reference check & 3rd C Level interview • If you pass the 2nd working interview, the results of the reference check and 3rd interview will be comprehensively considered to decide whether to proceed with treatment consultation. Please note that the reference check process does not affect the decision to proceed with a third interview. 2. Document instructions • Resume (required) • Portfolio GitHub address (optional) • Instructions: Please submit all documents in PDF format or link. 3. Interview Guide • 1st phone interview: A 1:1 phone interview with the HR Lead that takes up to 30 minutes. During the phone interview, you will be asked questions about your motivation for applying, your career history, and your expectations for the future. • 2nd working interview: This is a face-to-face interview with a working staff member and takes approximately 1 hour. We will review your work experience and job performance abilities based on the documents you submit. • Reference check: This is conducted using a platform called Specter to review the candidate’s capabilities from various angles. • 3rd C Level Interview: This is a face-to-face interview with the CEO. It will take about an hour. You will have time to talk about Nubi Lab's organizational culture, work style, and work experience. ※ The selection process may change depending on internal circumstances. ※ If there are false information in the application form, your acceptance may be cancelled.- Those who can use SQL - Those with more than 3 years of experience working in the real estate industry - Those with real estate department and real estate-related qualifications - Those with experience in data construction and processing logic design＜This type of person is better＞ • A person who has experience analyzing data in a cloud environment (AWS GCP…) • A person with A/B testing experience in a product organization • A person with work experience using analysis tools (Amplitude Mixpanel) • Dashboard People who have experience using tools (Redash Tableau QuickSight…) • People who are thinking about how the entire team can see data better•AI/ML-related project experience •Statistics/data analysis-related knowledge •Image processing/computer vision-related experience •Medical/healthcare data-related experience• English proficiency • Artificial intelligence-related experience preferred • Master’s degree or higher Journal paper author■ Full-time employee: There is a 6-month probationary period to check if it fits Share Round’s culture and vision. ■ Recruitment process documents - Online interview - Project performance - Culture fit interview 1. Documents • Resume and career description or portfolio: How to define the user’s problem 2. Online interview: Time to increase your understanding of the company and the job in question 3. Project performance: Carry out an actual project for the job you are applying for (appropriate compensation regardless of the result) 4. Culture fit interview• Those who are willing to grow together with interest and passion for new IP big data analysis • Experience with data visualization tools (preferred) -> Experience with data visualization tools such as Tableau and POWER BI (preferred)• Those with experience developing container-based microservices • Those with experience developing API servers using Swagger • Those with experience analyzing and processing large amounts of data using Spark Hive Redshift Bigquery, etc. • Constructing data visualization and dashboards such as Tableau Looker Quicksight Working conditions for those with this experience • Employment type: Full-time employee (100% salary paid for 3 months of probation) • Work location: Post Tower, 60 Yeouinaru-ro, Yeongdeungpo-gu, Seoul • Salary conditions: Company regulations and separate agreement Joining itinerary Document selection ＞ Coding test ＞ 1 Primary interview ＞ Second interview ＞ Joining • The first interview will be conducted in the form of a separate assignment and code review. • Details will be provided only to those who pass the documents. Note • The selection process may change depending on the situation. • Only those who pass the document screening can take the SKCT (personality test). • This announcement may be closed early when recruitment is completed. • Recruitment may be canceled if false information is provided.• Those with experience in designing/building on-premises-based server networks are preferred • Those with experience leading data lake construction projects are preferred • Those with a high understanding of the Hadoop eco system are preferred • Those with the ability to design and build high-availability systems are preferred• A person with NOSQL operation experience (elasticsearch mongo cassandra redis) • A person with experience migrating to a cloud environment database • A person who can use or understand development languages ​​such as Python golang • A person with an active and positive mind in collaboration and communication • Those who are interested in the database New Feature Working conditions • Employment type: Full-time employee (100% salary paid for 3 months on probation) • Work location: Post Tower, 60 Yeouinaru-ro, Yeongdeungpo-gu, Seoul • Salary conditions: Company regulations and separate consultation Joining itinerary Document selection ＞ 1st interview ＞ 2nd interview ＞ Joining • The 1st interview is conducted in the form of a test and review. • Details will be provided only to successful applicants. Note • The selection process may change depending on the situation. • Only those who pass the document screening can take the SKCT (personality test). • This announcement may be closed early when recruitment is completed. • Recruitment may be canceled if false information is provided.• Experience with in-depth/ad-hoc analysis of Google Analytics or Adobe Analytics data • Experience with an advertising agency or GA/AA consulting firm (with experience in executing global advertisements)• Experience in data engineering based on cloud platform (GCP AWS) • Experience in leading a data engineer team✔ It would be even better if you have this kind of experience. • Those with experience building machine learning data/service pipelines • Those with a master’s degree or higher in statistics/computer engineering • Those who have presented papers at academic conferences in related fields• Those with a master's degree or higher in the field of deep learning or equivalent experience • Those with experience in applying and operating models to actual services • Experience in analyzing and implementing the latest artificial intelligence models • High interest in new technologies and willing to apply them to actual services People with experience • People with experience working in startups• Those with a degree in mathematics, physics, statistics, or industrial engineering • Those who can perform work in a foreign language (English) • Those with high-ranking experience in one or more competitions in the ML DL AI field (Dacon Kaggle, etc.) - Submit the final ranking link• Experience with PaaS (Azure SQL / AWS Aurora RDS, etc.) in a cloud environment • Knowledge of overall infrastructure such as OS networks • Experience with Data Warehouse Big data NoSQL• Possess smooth communication skills • Extensive understanding and operating experience of commerce platforms • High interest and interest in fashion platforms• Holder of GCP Professional Data Engineer certification • Have more than 2 years of data engineering experience, such as data analysis and reporting • Have designed, built, and operated large enterprise data solutions and applications using GCP • Have knowledge of Machine Learning and AI concepts • A person with excellent problem-solving and analytical skills• Experience working at a portal/online shopping mall (more than 2 years) preferred • Possession of IT certification (Information Processing Technician MCDBA MCSE CCIE AWS Certified Solutions Architect Azure Solutions Archtect Expert Linux Information Security Technician certification, etc.) • Ability to write shell python scripts- Person majoring in computer engineering - Person with experience working as a data visualization user researcher - Person capable of front-end development• A person majoring in computer science, mathematics, and statistics • A person who proactively finds and solves problems • A person who always thinks about growth • A person with experience in MongoDB MySQL Azure Superset• Those majoring in data statistical analysis (statistics, mathematics, industrial engineering, computer engineering, etc.) • Those with experience in carrying out projects using data and big data • Those who can perform analysis tasks, specify them, and report them • Those who are good at systematizing analysis tasks and communicating using visualization1. It is better if you have previous experience leading a team or part. 2. It is better if you have experience designing or operating a BI system. 3. It is better if you major in one of AI/Big Data Engineering, Applied Mathematics, Statistics, or Computer Engineering.• Those with experience in stock market-related domains • Those with interest in AI technology, including natural language processing • Those with growth experience in early startup work • Those with interest and experience in stock investment- Those who majored in a department related to statistical data analysis - Those who have experience using data visualization tools (Tableau Redash) - Those who have an understanding of commerce/distribution/logistics or related industries - Those who have experience dealing with indicators related to human resource productivity and cost efficiency• Experience working as an on-chain detective or analyst • Experience working at a security/forensic company • Majored in the security/forensic field (BoB, etc.) • Service planner from White Hacker • Experience in planning products related to SaaS regtech fintech • Experience in successfully growing an initial service • English Communication and documentation skills.It would be better if you have these capabilities and experience. - Those with experience in operating cloud-based DBs such as AWS/GCP - Those with experience in project leading and organizational management - Those with DBA experience in platform services - Those who can develop automation scripts (languages) for DB operations- Experienced in RDBMS (Prostgresql MYSql MSSql) - Experienced in Gis (Qgis Geoserver) - Experienced in R data analysis - Experienced in building high availability (HA) systems - Person with relevant certification - Person with a positive and proactive mindset toward collaboration• Experience working as an on-chain detective or analyst • Experience working at a security/forensic company • Majored in the security/forensic field (BoB, etc.) • Expertise in blockchain and cryptocurrency • Experience developing or using automated vulnerability detection tools • Experience winning a major hacking competition • Proficient in English communication and document writing• Those with experience designing A/B tests to verify hypotheses, analyzing the results, and applying them to products • Those with experience with customer business indicators (Retention LTV CPI), etc. • Those with experience using ML • Learn, Those who have a passion and fellowship• Those with experience operating databases related to medical systems (EMR HIS, etc.) • Those with knowledge of medical IT data standards such as CDM HL7 FHIR • Hadoop-Ecosystem and Apache Airflow • Those with experience using Google Cloud Platform • Those with experience using Kubernetes • Those with experience in data de-identification • Those with experience in Change Data Capture • Real-time streaming data Anyone with experience in loading• Those with research results related to big data in financial statistics • Those with experience working in the data fintech industry • Those who like to identify problems on their own, raise hypotheses, and verify them through experiments • Those who have no difficulty communicating with various job groupsThose who have experience building LLM services Those who have more than 2 years of backend development experience Those who are interested in AI education, including conducting research sessions Those who are interested in AI technology and education Those who are interested in new technologies and strive for self-development1) Those who have earned a master’s/doctoral degree in statistics or computer science 2) Those who have multiple Project Manager (PM) experiences• Those with experience in data processing and artificial intelligence model development in the medical field • Those with experience in machine learning challenges such as Kaggle • Those with the ability to work using the latest artificial intelligence technology and open source • Those with experience in services utilizing machine learning• Those with experience collaborating with engineers to build and manage data pipelines • Those with more than 2 years of data engineering and analysis experience each • Those with experience planning/analysis of mobile service logs • Familiar with various analysis methodologies such as statistical modeling • Those with experience in ML/DL product services • Those with smooth internal and external collaboration and communication • Those with interest and passion in solving management problems in restaurant stores• Experience developing products based on Apache Spark • Experience building and operating AWS or cloud-based data infrastructure • Experience using data visualization or BI tools (Tableau Apache Superset Power BI, etc.) • A person who can derive results from data processing to analysis/modeling• Experienced in real-time data processing using Kafka Nifi Spark, etc. • Those who can use Kubernetes Docker • Those with extensive Linux system experience • Those with a good understanding of web services • Those with high ability to use shell scripts- Those with master's/doctoral degrees in related fields such as data statistics and industrial engineering - Those with experience in recommending services and projects using machine learning and deep learning based on big data - Those with knowledge related to the advertising industry and e-commerce - Real-time data pipes People with experience in line design, construction, and operation• Those with more than 2 years of AI learning data construction or similar work experience or equivalent experience • Those with AI-related knowledge and understanding of the learning pipeline data labeling market • Interested in changing technology trends such as Prompt Engineering Those who want to learn • Those who have experience achieving results in a free work environment[Additional experiences and strengths expected] • Those with practical experience in related work • Those who can use English for business purposes• Experience working at a company related to blockchain and cryptocurrency • Experience working at a big data-related company • Experience of winning prizes in related competitions • Holder of a master’s/doctoral degree in a related field such as statistical analysis and machine learning • High-level English communication and document writing skills• ETL or migration experience (3+ years) • Medical domain experience • EMR/OCS experience • Interested in Open Source • SQL tuning ability • SQLD SQLP preferred[This type of person is even better!] - Those with experience in writing papers and patents - Those who can implement the contents of the latest papers - Those who can write the latest time series images and video deep learning / machine learning algorithms - Computer engineering, industrial engineering, electrical and electronics related Those with a degree• Those with experience establishing new business strategies, managing KPI settings, and managing organizations • Those with experience collaborating with engineers to build and manage data pipelines • Those who have led the configuration of data analysis environments or have a high understanding of various analysis environments (data Integrated Data Mart) • Those with experience using PA MMP tools such as Amplitude Braze Appsflyer • Those with experience designing and operating GTM events • Those with experience influencing the development of data literacy among company members [Recruitment Procedure] - Document screening -＞ 1st working interview -＞ 2nd collaboration department interview -＞ 3rd CDO interview * Selection stages may be reduced or added during the selection process if necessary.- Experience performing work related to investment accounting disclosure based on understanding ofThis is preferential treatment. • MLOps work experience • Experience building data pipelines • Back-end development experience or high-level understanding of back-end development • Unstructured data modeling experience • Experience in designing and operating large-scale data processing systems• Experience working in an endoscopy unit • Nurse with career breakㆍExcellent presentation skills ㆍExcellent business operation/management skills ㆍRelated certification holder ㆍRelated work experience (more than 2 years) ㆍCertificate: Linux Master Information Processing Engineer Elastic/Kafka/Redis/AWS/MongoDB Certified ㆍMajor: Computer Science/Computer Engineering Applied Software Engineering Information and Communication Engineering- Incentive payment (based on company and individual performance at least twice a year) - Lunch support in addition to annual salary (KRW 1.2 million per year) - Provision of a work laptop - Provision of a corporate credit card for each individual - Support for business communication expenses - Support for business transportation expenses (taxi available) - Support for birthday celebrations - Support for vacation expenses - Support for comprehensive health check-ups - Operation of congratulatory leave and support fund system - Support for returning home expenses for holidays - Support for certification and self-development expenses - Support for activity expenses for in-house club activities - Support for books - Support for snacks• Master's/Ph.D. degree holders • Majored in related fields (computer/systems engineering/industrial engineering/mathematics/statistics/business administration, etc.) • Experience in analysis projects in various domains • Leadership holder• Those with experience using Java Scala • Those with a high understanding of recommender systems • Those with playing experience and equivalent understanding of various games • Those with experience using various work collaboration tools and collaborating with colleagues from various job groups and departments A person skilled in: • A person with experience serving ML servicesIt is even better if you have startup experience. Additionally, preference will be given to those with the following experience. - Those who have extensive experience processing various source data to suit their needs and automating the process - Those who are interested in the real estate domain• Those with experience developing game DB • Those with experience opening commercial games • Those with experience developing MongoDB • Those with experience using Redis • Those majoring in science or engineering are preferred• Those who are interested in online learning services • Those who have a good understanding of online services and various experiences in using them • Have development experience and communication skills with developers• Computer engineering major or similar major • Development experience using visualization library [ex) AmChart Chart.js apexCharts .. ) • Development experience in Docker environment • Experience configuring project environment using webpack babel, etc. • Experience building and developing UI tests • Experience in modifying and utilizing open source • Experience in flexible work response ability • Experience in proposing ideas and reflecting them in services • Experience in implementing or using internal UI libraries• A person with a high overall understanding of databases • A person with extensive database-based programming experience • A person with a high overall understanding of the Linux system• Experience building an on-chain data DB • High understanding of blockchain and cryptocurrency • Experience using on-chain data analysis tools (Dune Analytics Messari, etc.) • No difficulty communicating in English and writing documents- Experience in deep learning projects related to hospital collaboration or medical imaging - Experience writing papers using developed AI models - Master's degree or higher in deep learning - Proficient in the Pytorch framework• Experience working in the fintech/financial industry or planning/operating relevant services • Experience in planning/operating DT (Digital Transformation) along with understanding of the financial (insurance, loan, investment, etc.) industry• Perform academic data-related tasks • Experience in writing papers and research • Experience in operating databases (RDBMS NoSQL) • Experience in operating scalable distributed processing systems • Experience in developing web services • Experience in using GCP • Experience in developing and training machine learning models • Linux environment • Experience using various programming languages ​​• Experience working in Agile / Scrum Team • Experience in Pair Programming / Code Review-Computer engineering/statistics/mathematics/major -Possess smooth PPT and communication skills -Work experience or strong interest in digital marketing• Experience in data labeling • OA-related qualifications- Has experience working in securities and financial sectors• 5-day work week • 4 major insurances and overtime pay, etc.• Engineering math/statistics, industrial engineering • Those with a master's/doctoral degree in a related major • Those with experience winning data analysis-related competitions • Those who have completed data analysis-related certifications and specialized training • Those who collaborate well with team mates • A variety of people A person with experience in data-based problem solving in the field • A person with a high understanding of machine learning/deep learning/statistics/data science • Experience in handling large amounts of data • Experience in processing and processing structured/unstructured data • Data mining/machine learning/deep Experience implementing models using learning• Possess understanding of HTML/css/Javascript • Capable of analyzing digital campaigns • Capable of directly operating major marketing channels and media settings, including execution analysis and optimization • Possess the ability to design marketing funnels for each campaign and establish key KPIs• Those who majored in statistics or have statistics-related knowledge • Those who have team leading experience (practical management and human management of team members of 4 or more people) • Those who have experience solving problems and failing with the rise and fall of actual indicators based on data-based insights • Experience in setting up and operating an online advertising platform • Experience in statistics/deep learning/machine learning modeling for data analysis- Majored in a computer engineering-related department - Full stack developer - Capable of configuring Spring Security - Capable of MSA design, development, construction and operation - Proficient in DB Schema design SQL and Index Tuning - Experience in developing open source projects - Experience in developing and launching solutions - Clouds such as AWS Azure N Cloud and Proficient in container environment development/operation - Active in taking on new challenges[This type of person is even better!] - Those with experience in writing papers and patents - Those who can implement the contents of the latest papers - Those who can write the latest time series images and video deep learning / machine learning algorithms - Computer engineering, industrial engineering, electrical and electronics related Those with a degreeㆍExperience in using Kubeflow Airflow MLflow ㆍExperience in LLM fine tune ㆍExperience in on-premise based service products- Those with the ability to collect and process financial data - Those with experience using the Trading View API - Those with NoSQL construction and development experience (MongoDB ElasticSearch, etc.) - Those with experience developing Rest APIs and services - The latest technology trends A person who solves problems without missing a beat and is not afraid of change• Computer engineering/statistics/mathematics/ major • Experienced in DW/CRM related work • Possess smooth PPT and communication skills • Experienced in log analysis campaign personalized recommendation solutions/projects• Communication skills to convey concepts and considerations • Ability to write materials to effectively communicate information and analysis results.- Willingness to learn Deep Learning Pipeline and Map Reduce - Experience in system programming such as Linux C/C++ Rust - Experience in statistical processing or encryption processing - Experience in network programming• Experience in package software development • Experience in commercial software development • Experience in software architecture design • Experience in application framework design • Experience in building a CI/CD environment • Experience in building an environment to collect information for usability evaluation[This type of person is even better] • Those with development-related knowledge preferred (able to collaborate and communicate with the development department) • Those with a strong drive and sense of challenge to build-up in a startup • Those who enjoy the process of acquiring new knowledge or skills • Those who can enjoy communication activities with customers and stakeholders • Those with related majors are preferred (Korean literature, linguistics, linguistics, cognitive science, computational language, computer engineering, etc.)• Those with work experience related to medical image labeling • Those with experience or understanding of the startup environment • Those with an understanding of artificial intelligence research • Those who can communicate with engineers based on their understanding of the product1. Experience in developing and operating Elasticsearch in the Production Hadoop ecosystem 2. Experience in developing and operating based on an open source platform/framework in an on-premise environment 3. Experience in developing and operating in a Cloud (AWS GCP, etc.) environment• Those with experience working in video labeling • Those with experience or understanding of the startup environment• Majored in computer engineering-related department • Full stack developer • Capable of configuring Spring Security • Capable of MSA design, development, construction and operation • Proficient in DB Schema design SQL and Index Tuning • Capable of interpreting/analyzing English technical documents • Experience in developing open source projects • Developing and launching solutions Experience • Proficient in developing/operating cloud and container environments such as AWS Azure N Cloud • Active in taking on new challenges[This type of person is even better!] - Those with experience writing papers and patents - Those with experience in image and video data projects - Those who can implement the contents of the latest papers - Those with experience writing the latest time series deep learning / machine learning algorithms - Those with a degree in computer engineering, industrial engineering, electrical or electronic engineering